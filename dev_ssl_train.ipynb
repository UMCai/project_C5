{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49070e05",
   "metadata": {},
   "source": [
    "## Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf64bf41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "MONAI version: 1.4.0\n",
      "Numpy version: 1.26.4\n",
      "Pytorch version: 2.2.1+cu121\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: 46a5272196a6c2590ca2589029eed8e4d56ff008\n",
      "MONAI __file__: /home/<username>/miniconda3/envs/cloudspace/lib/python3.10/site-packages/monai/__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: 0.4.11\n",
      "ITK version: 5.4.0\n",
      "Nibabel version: 5.3.2\n",
      "scikit-image version: 0.24.0\n",
      "scipy version: 1.15.1\n",
      "Pillow version: 10.4.0\n",
      "Tensorboard version: 2.15.1\n",
      "gdown version: 5.2.0\n",
      "TorchVision version: 0.17.1+cu121\n",
      "tqdm version: 4.67.1\n",
      "lmdb version: 1.6.2\n",
      "psutil version: 6.1.1\n",
      "pandas version: 2.1.4\n",
      "einops version: 0.8.0\n",
      "transformers version: 4.40.2\n",
      "mlflow version: 2.19.0\n",
      "pynrrd version: 1.1.1\n",
      "clearml version: 1.17.0\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n",
      "Folder created: ./logs\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time, random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.nn import L1Loss\n",
    "from monai.utils import set_determinism, first\n",
    "from monai.networks.nets import ViTAutoEnc, densenet121, AutoEncoder\n",
    "from monai.losses import ContrastiveLoss\n",
    "from monai.data import DataLoader, Dataset, CacheDataset\n",
    "from monai.config import print_config\n",
    "from monai.transforms import (\n",
    "    LoadImaged,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    CopyItemsd,\n",
    "    SpatialPadd,\n",
    "    EnsureChannelFirstd,\n",
    "    Spacingd,\n",
    "    OneOf,\n",
    "    ScaleIntensityRanged,\n",
    "    RandSpatialCropSamplesd,\n",
    "    RandCoarseDropoutd,\n",
    "    RandCoarseShuffled,\n",
    "    RandAffined,\n",
    "    RandFlipd,\n",
    "    Resized,\n",
    ")\n",
    "from src.utils import data_split, ensure_folder_exists\n",
    "print_config()\n",
    "# Set Determinism\n",
    "set_determinism(seed=42)\n",
    "pretrain_data_path = './data/MedNIST/pretrain_data_dicts.json'\n",
    "assert os.path.exists(pretrain_data_path), 'Pretrain data not found'\n",
    "logdir_path = './logs'\n",
    "ensure_folder_exists(logdir_path)\n",
    "assert os.path.exists(logdir_path), 'Log directory not found'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e2e12c",
   "metadata": {},
   "source": [
    "##### Define file paths & output directory path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "657217e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size:  18864\n",
      "Validation data size:  4717\n",
      "Test data size:  0\n",
      "Total Number of Training Data Samples: 18864\n",
      "{'image': 'data/MedNIST/CXR/006621.jpeg', 'label': 'CXR'}\n",
      "##########\n",
      "Total Number of Validation Data Samples: 4717\n",
      "{'image': 'data/MedNIST/ChestCT/001226.jpeg', 'label': 'ChestCT'}\n",
      "##########\n"
     ]
    }
   ],
   "source": [
    "with open(pretrain_data_path, \"r\") as json_f:\n",
    "    json_data = json.load(json_f)\n",
    "\n",
    "splited_data = data_split(json_data, [0.8, 0.2])\n",
    "\n",
    "train_data = splited_data[\"train\"]\n",
    "val_data = splited_data[\"val\"]\n",
    "\n",
    "print(\"Total Number of Training Data Samples: {}\".format(len(train_data)))\n",
    "print(train_data[1])\n",
    "print(\"#\" * 10)\n",
    "print(\"Total Number of Validation Data Samples: {}\".format(len(val_data)))\n",
    "print(val_data[-1])\n",
    "print(\"#\" * 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d106d4ea",
   "metadata": {},
   "source": [
    "##### Define MONAI Transforms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8ebbdd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape: torch.Size([64, 64])\n",
      "image2 shape: torch.Size([64, 64])\n",
      "gt_image shape: torch.Size([64, 64])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABaIAAAHcCAYAAAAk+t1cAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAACIwUlEQVR4nO3deZRV5ZX//13zPDBWAVUgKII4ixOiiY1EtBPjQEeTtm00AmmCtmMnsVcM6jeKnbSJ0SAqGjQmBttEjYlxREFRwYADKoogIMU81TwP5/eHPysWPJ9Hz+Wemu77tVatlexT+znz2fc8XGsnBUEQGAAAAAAAAAAAEUnu6g0AAAAAAAAAAPRuTEQDAAAAAAAAACLFRDQAAAAAAAAAIFJMRAMAAAAAAAAAIsVENAAAAAAAAAAgUkxEAwAAAAAAAAAixUQ0AAAAAAAAACBSTEQDAAAAAAAAACLFRDQAAAAAAAAAIFJMRAOd4IEHHrCkpCTbsGFDV28KAAAwajMAAIkmKSnJbrjhhq7eDCChMRENAAAA9GCPPfaYXXDBBTZixAjLzs62UaNG2TXXXGMVFRVdvWkAAMTVLbfcYk888URXbwaAGCUFQRB09UYAvV1ra6s1NzdbRkaGJSUldfXmAACQ8HpTbe7fv78NHjzYzjnnHBs6dKi9++67dvfdd9uIESPszTfftKysrK7eRAAA4iI3N9f+5V/+xR544IHQuQ0NDZaammqpqanx3zAAXwp3H9AJUlJSLCUlpas3AwAA/P96U23+4x//aKeeemqH2NixY23KlCn2+9//3qZOndo1GwYAQDeSmZnZ1ZsAJDz+NAfQCfb+O5QHHHCAfeMb37BFixbZsccea1lZWXb44YfbokWLzOzT/8T28MMPt8zMTBs7dqy99dZbHcZbuXKlXXzxxTZixAjLzMy04uJi++53v2u7d+/eZ92frSMzM9MOPPBAu+eee+yGG25wfvvrd7/7nY0dO9aysrKsb9++9u1vf9vKysrifjwAAOhqvak27z0JbWZ27rnnmpnZBx98EMPRAQCg831RfUxKSrLa2lp78MEHLSkpyZKSkuziiy/+0uPv/TeiPxv7o48+sn/7t3+zgoICGzBggF1//fUWBIGVlZXZ2Wefbfn5+VZcXGy33XZbh/GamprsJz/5iY0dO9YKCgosJyfHTjnlFHvppZf2Wffu3bvtoosusvz8fCssLLQpU6bYO++8Y0lJSft8u/vDDz+0f/mXf7G+fftaZmamHXvssfbkk09+6f0EujO+EQ10kbVr19q//uu/2ve+9z37t3/7N/vf//1fO+uss+zuu++2//7v/7bvf//7ZmY2e/ZsO//882316tWWnPzpvx09//zztm7dOrvkkkusuLjY3n//fbv33nvt/ffft6VLl7YX6rfeesvOOOMMGzRokN14443W2tpqN910kw0YMGCf7bn55pvt+uuvt/PPP9+mTp1qO3futDvvvNO+8pWv2FtvvWWFhYWddmwAAOgKvak2b9u2zcw+/bMdAAB0d1+mPj700EM2depUO/7442369OlmZnbggQfu97ovuOACO+SQQ+zWW2+1p556yn76059a37597Z577rEJEybY//zP/9jvf/97u/baa+24446zr3zlK2ZmVlVVZffdd5995zvfsWnTpll1dbXdf//9NmnSJHvjjTfsqKOOMjOztrY2O+uss+yNN96wGTNm2OjRo+3Pf/6zTZkyZZ9tef/99238+PE2ZMgQ+9GPfmQ5OTn2f//3f3bOOefYn/70p/Z/aAZ6rABA5ObPnx+YWbB+/fogCIJg2LBhgZkFr732WvvvPPvss4GZBVlZWcEnn3zSHr/nnnsCMwteeuml9lhdXd0+6/jDH/4QmFnw8ssvt8fOOuusIDs7O9i8eXN7bM2aNUFqamrw+dt/w4YNQUpKSnDzzTd3GPPdd98NUlNT94kDANDT9fbafOmllwYpKSnBRx995D8QAAB0A1+2Pubk5ARTpkyJaR1mFsyaNav9/8+aNSsws2D69OntsZaWlqCkpCRISkoKbr311vZ4eXl5kJWV1WHdLS0tQWNjY4d1lJeXB0VFRcF3v/vd9tif/vSnwMyC22+/vT3W2toaTJgwITCzYP78+e3x0047LTj88MODhoaG9lhbW1tw0kknBSNHjoxpv4HuhD/NAXSRMWPG2Lhx49r//wknnGBmZhMmTLChQ4fuE1+3bl177PNNhxoaGmzXrl124oknmpnZm2++aWafNmF64YUX7JxzzrHBgwe3//5BBx1kZ555Zodteeyxx6ytrc3OP/9827VrV/tPcXGxjRw50vmfFgEA0Nv0ltr88MMP2/3332/XXHONjRw5MvRxAACgM4Wpj1H4fC+FlJQUO/bYYy0IArv00kvb44WFhTZq1KgOtT8lJcXS09PN7NNvPe/Zs8daWlrs2GOPba/9ZmbPPPOMpaWl2bRp09pjycnJNnPmzA7bsWfPHnvxxRft/PPPt+rq6vbav3v3bps0aZKtWbPGNm/eHPf9BzoTf5oD6CKff6E1MysoKDAzs9LSUme8vLy8PbZnzx678cYbbcGCBbZjx44Ov19ZWWlmZjt27LD6+no76KCD9ln33rE1a9ZYEATyZTUtLe3L7BIAAD1ab6jNr7zyil166aU2adIku/nmm52/AwBAdxKmPkbBVf8zMzP3+fNWBQUF+/R+ePDBB+22226zDz/80Jqbm9vjw4cPb//fn3zyiQ0aNMiys7M75O69b2vXrrUgCOz666+366+/3rmtO3bssCFDhnz5nQO6GSaigS6SkpISKh4EQfv/Pv/88+21116z//qv/7KjjjrKcnNzra2tzc444wxra2sLvS1tbW2WlJRkTz/9tHP9ubm5occEAKCn6em1+Z133rFvfvObdthhh9kf//hHS03loz4AAF/EVWe/TO3/3e9+ZxdffLGdc8459l//9V82cOBAS0lJsdmzZ9vHH38cejs++7xw7bXX2qRJk5y/0xkT80CU+HQK9DDl5eW2cOFCu/HGG+0nP/lJe3zNmjUdfm/gwIGWmZlpa9eu3WeMvWMHHnigBUFgw4cPt4MPPjiaDQcAoJfqDrX5448/tjPOOMMGDhxof/vb3/hHZABAjxGmPn7W/Lc7+OMf/2gjRoywxx57rMN2zZo1q8PvDRs2zF566SWrq6vr8K3ovfdtxIgRZvbpf/U0ceLECLcc6Dr8jWigh/nsX2Y//y+xZma33377Pr83ceJEe+KJJ2zLli3t8bVr19rTTz/d4XfPO+88S0lJsRtvvHGfcYMg2Oc/PwIAAP/Q1bV527Ztdvrpp1tycrI9++yzNmDAgHjsFgAAnSJMfczJybGKiopO3kI3V/1ftmyZvf766x1+b9KkSdbc3Gzz5s1rj7W1tdmcOXM6/N7AgQPt1FNPtXvuuce2bt26z/p27twZz80HugTfiAZ6mPz8fPvKV75iP/vZz6y5udmGDBlizz33nK1fv36f373hhhvsueees/Hjx9uMGTOstbXVfv3rX9thhx1mb7/9dvvvHXjggfbTn/7UrrvuOtuwYYOdc845lpeXZ+vXr7fHH3/cpk+fbtdee20n7iUAAD1HV9fmM844w9atW2c/+MEPbMmSJbZkyZL2cYqKiuxrX/ta5McAAID98WXr49ixY+2FF16wX/ziFzZ48GAbPnx4exPhzvaNb3zDHnvsMTv33HPt61//uq1fv97uvvtuGzNmjNXU1LT/3jnnnGPHH3+8XXPNNbZ27VobPXq0Pfnkk7Znzx4z6/gt7zlz5tjJJ59shx9+uE2bNs1GjBhh27dvt9dff902bdpk77zzTqfvJxBPTEQDPdDDDz9sl19+uc2ZM8eCILDTTz/dnn766Q4dhs0+LdJPP/20XXvttXb99ddbaWmp3XTTTfbBBx/Yhx9+2OF3f/SjH9nBBx9sv/zlL+3GG280s0+bM51++un2zW9+s9P2DQCAnqgra/NnL6U/+9nP9tmur371q0xEAwC6vS9bH3/xi1/Y9OnT7cc//rHV19fblClTumwi+uKLL7Zt27bZPffcY88++6yNGTPGfve739mjjz5qixYtav+9lJQUe+qpp+yKK66wBx980JKTk+3cc8+1WbNm2fjx4y0zM7P9d8eMGWPLly+3G2+80R544AHbvXu3DRw40I4++ugOf/4L6KmSgr3/Wz8Avd4555xj77///j5/uxIAAHQNajMAAPvqzfXxiSeesHPPPdeWLFli48eP7+rNAToFfyMa6OXq6+s7/P81a9bY3/72Nzv11FO7ZoMAAEhw1GYAAPbVm+vj3vvW2tpqd955p+Xn59sxxxzTRVsFdD6+EQ30coMGDbKLL77YRowYYZ988onNnTvXGhsb7a233rKRI0d29eYBAJBwqM0AAOwrlvrY2tr6hU38cnNzLTc3N4pN/tKmTp1q9fX1Nm7cOGtsbLTHHnvMXnvtNbvlllvsuuuu69JtAzoTfyMa6OXOOOMM+8Mf/mDbtm2zjIwMGzdunN1yyy286AIA0EWozQAA7CuW+lhWVmbDhw/3jjtr1iy74YYb4ry14UyYMMFuu+02++tf/2oNDQ120EEH2Z133mmXXXZZl24X0Nn4RjQAAAAAAAB6nIaGBluyZIn3d0aMGGEjRozopC0C4MNENAAAAAAAAAAgUjQrBAAAAAAAAABEKrK/ET1nzhz7+c9/btu2bbMjjzzS7rzzTjv++OO/MK+trc22bNlieXl5lpSUFNXmAQASWBAEVl1dbYMHD7bk5MT+N9lY67UZNRsAEC3q9T9QrwEA3VWoeh1EYMGCBUF6enrwm9/8Jnj//feDadOmBYWFhcH27du/MLesrCwwM3744YcffviJ/KesrCyKMthj7E+9DgJqNj/88MMPP53zQ72mXvPDDz/88NP9f75MvY7kb0SfcMIJdtxxx9mvf/1rM/v0X2BLS0vt8ssvtx/96Efe3MrKSissLIz3JkUuNzfXGT/vvPNkjjr0vn+l7up/wW5paXHGU1JSQo+l9r+trU3mqPWkpaXJnIyMjFDrN/u04UGYHN+/+DQ2Njrjra2tMicnJyfUetT2mpmlp6c7475zprZNnX8zs82bNzvjL7zwgswBulpFRYUVFBR09WZ0mf2p12bUbDNqNjW7d9Xsbdu2OeObNm2SY1VUVMhlQLxQr6nXn0e91qjX1GuFeo3O8GXqddz/NEdTU5OtWLHCrrvuuvZYcnKyTZw40V5//fV9fr+xsbHDQ6S6ujrem9QpVPFSDyiznlkk1UO6txVJtQ2xFEmV4yuS6rpR6/Eds3gWSd9++s4B0F119TO1K4Wt12bU7DBjfdGyzkDNpmYr6hyo9XT1tQwk8jVIvd4X9VqjXlOvga70Za7BuP+hrV27dllra6sVFRV1iBcVFTn/1Wb27NlWUFDQ/lNaWhrvTQIAAHsJW6/NqNkAAHQ26jUAoDfp8o4P1113nVVWVrb/lJWVdfUmAQAAB2o2AADdH/UaANBdxf1Pc/Tv399SUlJs+/btHeLbt2+34uLifX4/IyND/mcdPUln/Stz2P/UIt5/Alz9JyCx/Cco6j918f0nQFlZWaHiZmaZmZnOeH19vcxRfw9KxdU6zMyys7Odcd9/6hP2Pw/y/edpqanu2zyWa8O3zYMHDw49HoCuE7Zem1Gzw6JmU7NdunPNHjZsmDN+0kknybF27tzpjPsmvtTfsFR/CxNIZNTr6FGvqdcu1GvqNaIR929Ep6en29ixY23hwoXtsba2Nlu4cKGNGzcu3qsDAAAxoF4DAND9Ua8BAL1J3L8RbWZ29dVX25QpU+zYY4+1448/3m6//Xarra21Sy65JIrVAQCAGFCvAQDo/qjXAIDeIpKJ6AsuuMB27txpP/nJT2zbtm121FFH2TPPPLNPgwUAANB1qNcAAHR/1GsAQG8RyUS0mdlll11ml112WVTDAwCAOKBeAwDQ/VGvAQC9Qdz/RjQAAAAAAAAAAJ/HRDQAAAAAAAAAIFKR/WmORFNSUuKMJyUlyRzfsu4qOTn8v12o/UxNdV9+2dnZcqycnJzQ629sbHTGKyoqZE5tba0znpWV5Yzn5eXJsdT+t7W1yZy0tDRnvKWlxRmvrq6WY9XV1TnjKSkpMic9Pd0ZV/tvZtbU1CSXAUB3Qs3WqNmJXbObm5udcd/+FxQUOON9+vSROYcffnio9ZuZbdq0KS5xM33NAOheqNca9Zp67UK9Rk/AN6IBAAAAAAAAAJFiIhoAAAAAAAAAECkmogEAAAAAAAAAkWIiGgAAAAAAAAAQKSaiAQAAAAAAAACRcrdUhaQ62paWljrjsXTtDYJALvN1QQ1LbZtv/Yqva2rYzr2+rrFKVVWVXFZfX++Mq665ZmaDBg1yxlVH4fz8fDlWRkaGM6464Jrpc6M67e7evVuOtWPHDme8pqYm9Pp9xyyWbs+J4JhjjpHLNm/e7Ixv3749qs0BEgo1242aTc1W94a6ntR14cvxUdusjr+Z2YgRI5zx4cOHh94udZxVXfYto2YD+4967Ua9pl5Tr6nXvREzRwAAAAAAAACASDERDQAAAAAAAACIFBPRAAAAAAAAAIBIMRENAAAAAAAAAIgUE9EAAAAAAAAAgEgxEQ0AAAAAAAAAiFRqV29AT1NSUuKMp6SkOONJSUlyrCAIQsXNzFpbW0OtPxZqHWZmmZmZznhLS4vMUccgKysr3IaZWXV1tTNeU1Mjc9SxKSwslDmDBw92xrOzs53xtrY2OZY6nxkZGTJHUcdywIABMqehocEZb2pqCr1+H9+1nsjGjh0bell9fb0zXlZWJsfatGlT6Jx4XwNAd0PNpma7ULP1tqn99x2zWMRzvORk9/dqUlP1a466ZoqLi2UONRuIDvWaeu1CvaZeU697J74RDQAAAAAAAACIFBPRAAAAAAAAAIBIMRENAAAAAAAAAIgUE9EAAAAAAAAAgEgxEQ0AAAAAAAAAiJRuTwmn0tLSUL/v687rW6aojqKqm+q9994beh29zaxZs5xx1YHVTHfuVR1QVQdYM32efd1UGxsbnXHVnVd1gPXl+Drgqg7NvhzfMUA4qtv1wQcfLHN8y5QtW7Y446o7sJnuELxnz57Q6wei1tNqdizrUF3rfeP5ctLT00PFKysr5Vhqma+j/aBBg5xxanbn1OxYarlaj7rOfct894Bvf8KK52cWajaw/6jX1GsX6jX1mnrdO+s1M0cAAAAAAAAAgEgxEQ0AAAAAAAAAiBQT0QAAAAAAAACASDERDQAAAAAAAACIFBPRAAAAAAAAAIBIMRENAAAAAAAAAIhUaldvQE9TUlIS6vfb2tpCryMlJUUuS011n7JY1pMo+vfv74zX1dXJnNraWme8paXFGU9LS5NjNTY2OuP19fUyJykpyRmvrKyUOUoQBKHiZmbNzc2h15Oenh46B11r8ODBoeJmZscdd5wzXlVV5Yxv2rRJjqWW+XJ41iGMnlazVY0x03XBt82tra3OeFZWlsxRy1TNrKmpkWMlJ7u/71BcXCxzqNldW7PVOVPHxUxvWyz3ho9aTyz3k9o2dS67A2o2ejPqNfXahXpNvaZe9856zTeiAQAAAAAAAACRYiIaAAAAAAAAABApJqIBAAAAAAAAAJFiIhoAAAAAAAAAECkmogEAAAAAAAAAkQrfAjMB9O3bVy7Lz893xmPpmqo6farOqL5lvu6oiU51521oaJA5qkNvU1NTqLiZ7mibl5cnc9SyIUOGOOO+Lqfq2igvL5c56tj4OhfTGT2xqWfjmDFjZM6hhx7qjPuuJdXtt6ysLFROEARWXV0tc9Bz9Kaa7atLGRkZzrivO3xubq4zro6Lme7ormqpqnFmZv369XPGffWPmt21NVsdf193etXt3refKiclJUXmqG1ITXW/zqi4mVlra6tclgh6Us2mXvce1GvqNfWaeh0mbka97q31mm9EAwAAAAAAAAAixUQ0AAAAAAAAACBSTEQDAAAAAAAAACLFRDQAAAAAAAAAIFJMRAMAAAAAAAAAIqXbUyawoUOHymWqQ6/qDBrPbqK+8XydgxPd9u3bnXFfR8/s7GxnvH///s54cXGxHEt1OlXdkc10V+WKigpn3Ncd9z//8z/lMiBqvmtT8XX1HjZsWKi40tTUZA888ECoHHRPvalm+2q56hru22ZVy9LT02WOqjNq2/r06SPHKigocMZ93eG3bNnijCdKzS4qKnLGS0tLZU5NTY1cFjZH7YuKm5lVVVWF3q66ujpn3NfRXdUGFY+l/iS67lizqde9B/Waek29pl67UK/D6+n1mm9EAwAAAAAAAAAixUQ0AAAAAAAAACBSTEQDAAAAAAAAACLFRDQAAAAAAAAAIFJMRAMAAAAAAAAAIsVENAAAAAAAAAAgUqldvQHdUWlpqVzW2trqjGdmZjrjTU1Ncqy8vDxnPCcnR+bU1NQ4483NzTIn0VVUVDjjycn632GOO+44Z7y4uNgZP/DAA+VY2dnZzvhbb70lc771rW/JZUBXSkpK6upNADpIlJqt7j21Xb6cqqoqmdPW1uaMZ2VlOeO+/a+rq3PG9+zZI3N6Ys3euXOnM15dXe2Mq+vPzCwlJcUZHzVqlMxR98CmTZtkzvLly53xXbt2OeN9+/aVYw0ePDjUWGZmn3zyiTPe2Ngoc1paWpxxdc36rhnfst6Emo3uhHpNvaZeU69dqNeJV68T46wCAAAAAAAAALoME9EAAAAAAAAAgEgxEQ0AAAAAAAAAiBQT0QAAAAAAAACASDERDQAAAAAAAACIVGrYhJdfftl+/vOf24oVK2zr1q32+OOP2znnnNO+PAgCmzVrls2bN88qKips/PjxNnfuXBs5cmQ8tzsuMjIynHHVTdRMd+hV3TxVB2Az3bnWl1NfX++Mqw6k0OezT58+MicIAmdcHedly5bJsbZs2eKMq+7E3dlDDz0kl1100UWduCXdz7Rp07p6ExKCujd9nYZdOWqc3qQ31WuzxKjZaWlpcix1zaqu8WZmDQ0NzrjqZm6mO8SreG1trRxL1Tnf/dcTa7Y6Btu3b3fG1bVspq8ndS2ZmX344YfOeGqq/piv1lNSUuKMFxcXy7EOOOAAZ9z32fS9995zxlesWCFzdu7c6Yyr/QxbFxB/8ajZiXCuqNfUa+o19TrseqjXiKfOrtehvxFdW1trRx55pM2ZM8e5/Gc/+5ndcccddvfdd9uyZcssJyfHJk2aJB+uAAAg/qjXAAB0f9RrAEAiCf2N6DPPPNPOPPNM57IgCOz222+3H//4x3b22Webmdlvf/tbKyoqsieeeMK+/e1v79/WAgCAL4V6DQBA90e9BgAkkrj+jej169fbtm3bbOLEie2xgoICO+GEE+z111935jQ2NlpVVVWHHwAAEJ1Y6rUZNRsAgM5EvQYA9DZxnYjetm2bmZkVFRV1iBcVFbUv29vs2bOtoKCg/ae0tDSemwQAAPYSS702o2YDANCZqNcAgN4mrhPRsbjuuuussrKy/aesrKyrNwkAADhQswEA6P6o1wCA7iquE9Gfdejcu+vo9u3bZffOjIwMy8/P7/ADAACiE0u9NqNmAwDQmajXAIDeJnSzQp/hw4dbcXGxLVy40I466igzM6uqqrJly5bZjBkz4rmquCgpKXHG29raZE5KSooz3tra6oxnZmbKsdLS0pzxmpoamdPS0uKMJyd3+Zfbu62bb765qzeh17jooou6ehPQCZKSkkLnBEEQwZb0nPX3ND2tXpslRs1OT08PPVZWVpbM2bFjhzPum7xITXV/NFTrr66ulmOp46mOpZlZU1OTM+77Nt/mzZvlMhff30pV26auJTN9Dar9V9efmdkHH3zgjG/dujX0+vPy8mSOum4LCgqccd+fAFi6dKkzXl5eLnNycnKc8ezsbJnju25cfLWsN9UManbvRr2mXivU6+jrdUNDgxxrzZo1zvjatWtljtpm3/Wktlldz4cccogcq2/fvs74wQcfLHPUsdm1a5fMqaiokMtcqNdaV+9/VOsPPRFdU1PT4eZav369vf3229a3b18bOnSoXXnllfbTn/7URo4cacOHD7frr7/eBg8ebOecc048txsAAHhQrwEA6P6o1wCARBJ6Inr58uX2T//0T+3//+qrrzYzsylTptgDDzxgP/jBD6y2ttamT59uFRUVdvLJJ9szzzzj/VdLAAAQX9RrAAC6P+o1ACCRhJ6IPvXUU71fz05KSrKbbrrJbrrppv3aMAAAEDvqNQAA3R/1GgCQSPjDwgAAAAAAAACASDERDQAAAAAAAACIVOg/zdGblJaWhs5RnS5VN9P8/Hw5VnNzszPu686q1t/V3TSBsB555BG5bM+ePc54d+0ObmbW2NjojGdkZHTylnx5sXTuBbpKItRs1enezCw7O9sZr62tlTk5OTnOuK87e25urjOu/hbpSSedJMd67bXXnPG6ujqZU19f74yr429mlpKS4oyr46ye12b6HMTS0V7l/PCHP5RjzZ8/3xnftm2bzFHXmbpmzMyysrLkMpd169bJZUOHDnXGx40bJ3PUtVFVVSVz+vXr54yre6C1tVWOVV1d7YxTs4H9R72mXis9rV77aoLaT9+10dTU5Iz7jrOizvOGDRtkjroGly1bJnPS0tKccd+5oV5Tr78I34gGAAAAAAAAAESKiWgAAAAAAAAAQKSYiAYAAAAAAAAARIqJaAAAAAAAAABApJiIBgAAAAAAAABEKrWrN6ArlZSUOOO+Lpeq02pqqvtQqm62ZmaVlZXOuOqm6uPrNAp0RxdccEFcx5s7d64zPmPGjLiuR/ntb38bt7EuueQSZ1x1mvZJTu6+/96oOmTHgu7EvV8i1OysrCyZo7bZt/+FhYXOeE5Ojsw59NBDnfFRo0Y549u3b5djqe7ovmOmOtf7nheqc7rqXK86zZvpLvC+Z6k6N83Nzc74r3/9azmWOme+z3kjRoxwxjdv3ixzFLX/6l7yLXvmmWdkzoABA0LFzcwqKiqccXU+6+rq5FjqfPrOs7o/1bOBmq1Rs3s36jX1Wulp9TotLU2Opc6N7zyr8XzHTNWy+vp6Z9xXe9Tngg0bNsgcdZ5996DKoV53jp5Qr7vv0QMAAAAAAAAA9ApMRAMAAAAAAAAAIsVENAAAAAAAAAAgUkxEAwAAAAAAAAAixUQ0AAAAAAAAACBSTEQDAAAAAAAAACKV2tUbELWioiK5LCsrK/R4ra2tznhOTk7odWzZssUZb2trkzlJSUnOeHp6uswBXB5//HG57Nxzz418/Q899JBcdtFFF4Ueb8aMGaFzbrzxRmd81qxZocdaunSpM75gwQKZc/vttzvj6hmQkZEhx1LPgKqqKpmTlpbmjCcn63+jVMvUNvueZ8DeEr1m+9bf2NjojGdnZ8uc1FT3x7xBgwbJnEMPPdQZ37NnjzO+fPlyOVZTU5MzHgSBzAl7zszMUlJSnPHm5mZnXB0XM308fduszrN6Zp922mlyrEWLFjnjBQUFMkcdG7VdZmYlJSXOuDrPFRUVcqw+ffo443l5eTKnf//+znhNTY3MGT16tDNeV1fnjL/99ttyLLVthx9+uMx58803nfF//ud/dsbVuTTT58a3/9RsdCfUa+p1ItRr3/ozMzNDbZeZWX19vTPuuzbVMnXOfO9+saxfrUfVXjN9D6jjrK5lM30+fTVe3QNhz78Z79hR4RvRAAAAAAAAAIBIMRENAAAAAAAAAIgUE9EAAAAAAAAAgEgxEQ0AAAAAAAAAiBQT0QAAAAAAAACASOk2oL2E6gpuprtm+jpQqg6YqjtqbW2tHEt1mvV101RUN89rrrlG5jQ0NDjjAwYMkDmq0+2xxx4rcw477DBn/De/+Y0zXl5eLsdSnV7vuecemQO3c889t0vXf9FFF3Xp+s3MZs2a5YxfeOGFznhubq4c68QTT4zLNpnpzr3qnjUza2lpccZ93Z4V9WyMd47qguwbK5b1oOdI9Jqt1mGm73Ffp29Vs/v37y9z1P48+eSTzrivZqt73HfM1Db36dNH5qhrICsryxlXx99MPzNzcnJkjlqP6hqvjqWZ2a5du5zxG2+8Uebccsstzri6zs3MtmzZ4ozX1dU543l5eXKsjz/+2BkfNWqUzFHd7ocNGyZz1q9f74yrz4bq+jPT+/nBBx/IHHWem5qanPGamho5VmZmpjNOzUZPQb2mXidCvVbPdzO9LxUVFTJH3Ru+46yuJ3Vv+J676hpU59JMHxsftT+7d+92xtV8kZl+xy4oKJA5f//7351xVft5x+78es03ogEAAAAAAAAAkWIiGgAAAAAAAAAQKSaiAQAAAAAAAACRYiIaAAAAAAAAABApJqIBAAAAAAAAAJFiIhoAAAAAAAAAEKnUrt6AqJWUlMhlycnuefjW1laZk5mZ6Yynp6c741VVVZ6ti59Y9iUnJ8cZz8/PlzlqvC1btsicPXv2OOM7duxwxtW+APE2depUZzw3N9cZr6mpkWNddNFFzvhDDz0Ueruys7ND56j7pqWlJfRYnSUpKSlUHL0fNZuare7/wsJCmVNfX++Mq21T59/MrK6uzhlvamqSOUEQOOPq+Tt8+HA5Vt++fZ3xRx99VOaMGDHCGVfHxcysubnZGVf7WVRUJMdSVq1aJZepc7N161aZU1pa6oyrfVHHxcxs/fr1cpmiPgMsX77cGS8oKJBjqft5586dobers1Cz8XnUa+p1ItRr31gqR63DzCwlJSXUWGZmtbW1obbN90zOy8tzxrOysmSOqsu+bVb3QJ8+fZzxlStXyrEGDRrkjI8bN07mqOfD6tWrZY7CO3Y0mPEDAAAAAAAAAESKiWgAAAAAAAAAQKSYiAYAAAAAAAAARIqJaAAAAAAAAABApJiIBgAAAAAAAABEKrWrNyBeVGfOAQMGhB6rra1NLsvIyHDGVXfUhoaG0OvxdbNMTXWfsuzsbGe8vLxcjqU6s/vWX11d7Yz7unyrbuZq/9Xvm/m70AJhqW7Dxx13nDN+4YUXyrHWrVvnjD/00EMy53vf+54zrjoaV1ZWyrHUs6mzxNKdV93Pvvu8O3X7Reyo2Ylds333eG5urjOuupb7lsWyza2trc64b5tV53QV//jjj+VYhx12mDO+du1amdO/f39n3Hc/qWtNHbPBgwfLsZYtW+aMFxcXy5yUlBRn/LLLLpM5//3f/+2Mq2umvr5ejqXOs9ouM/08UevxjVVTUyOXdQZqNr4s6jX1WkmEeu27ZhXf/qu64FuPylHXjK/2qGWxPMczMzNlTtjj7Luf1WeMIUOGyJwjjjjCGd+yZYszzjt259drvhENAAAAAAAAAIgUE9EAAAAAAAAAgEgxEQ0AAAAAAAAAiBQT0QAAAAAAAACASDERDQAAAAAAAACIFBPRAAAAAAAAAIBIpXb1BsRLaWmpM56UlCRzWltbnfGMjAyZk5rqPmTNzc2h1uHj22a1bWr9dXV1cqyUlBRnvKamRua0tbWFipuZVVVVyWUuycn630daWlpCjYXE8eqrrzrj77zzjsxZsWKFM/7SSy854z/60Y/kWI2NjZ6tC5dz0EEHOeO7du2SYzU1NTnjvvtJCYJALvM9n+Lx+/FeP7onajY1W8nOznbGfedGrUc9Y9WxNNPn07d+dTzVs8x3XN566y1nPC8vT+aobVu1apXM6dOnjzM+dOjQUOswM6uvr3fGfce5f//+zvhvfvMbmdPQ0OCMq/vcd5zVudm9e7fMKSwsdMZV/b3iiivkWL/61a+c8a6umV29fnQ/1GvqtZII9dp3/GN5voVdv2+ZGst3zMJeM2ZmmZmZznhOTo7MUcemvLw89Po/+ugjZ/zFF1+UOSeccIIzzjt296nXfCMaAAAAAAAAABApJqIBAAAAAAAAAJFiIhoAAAAAAAAAECkmogEAAAAAAAAAkWIiGgAAAAAAAAAQKXd72h6opKQkdI7qGunr6KuoTt6+Tquq06avY6VaprqZZmVlybFUB1Bfp9W0tDRn3NdpNz093RlXx9+3/77jmchuvPFGuUwdZ1/X1LBjmZnddNNNoceLp/Hjx4fOmTdvnjP+97//3Rm/8MIL5ViffPKJMz59+nSZc+ihhzrjqgt1YWGhHGv37t3OeLzPcyzjhdXZXXvR+ajZiV2zfcdZPed8z57m5mZnfMSIEc74mjVr5Fjqeqqvr5c56jjH0tFeraeoqEjm7Ny50xkfM2aMzAnb0f7www+XYy1fvtwZb2xslDkbN250xn3X5sknn+yML1261Bn3XZvqnOXl5ckcdW2o58nPfvYzOZZ6BsSCmo0oUa+p10o867U6Zr5rJpb9VOdA5ahrybd+3zlTy3zHTJ2D1FT3VJ7vOmttbZXLFHUP7Nq1S+b06dPHGVf7r65/M70/K1eulDlh36V5x+58fCMaAAAAAAAAABApJqIBAAAAAAAAAJFiIhoAAAAAAAAAECkmogEAAAAAAAAAkWIiGgAAAAAAAAAQKXerTWH27Nn22GOP2YcffmhZWVl20kkn2f/8z//YqFGj2n+noaHBrrnmGluwYIE1NjbapEmT7K677vJ2+46H0tJSZzyWjrqxdLJWHX191Pp9XUPVtlVUVDjj8e6MqfbT14FVdfT1daBHOL6OvorqWhxrzjXXXOOMq2swlmtTXUtmZjfffLMzfuWVV8qcadOmOePTp093xg8//HC9cXF0++23O+O5ubkyRz0DfJ2b1fMxlnMTyzXYnTr39jbduV6bUbMTvWb7nmWq03ltba3MUfuzceNGZ/yUU06RYy1btswZ951ntc179uxxxn21dNiwYXKZkp+f74zX1NTIHLU/KueJJ56QY6n7dseOHTJHXZtDhgyROercFBcXO+O+/a+urnbGfdemr566qOecmdm2bdtCjWVGze7NunPNpl5XOOPU6/jW61ioY5aRkSFz1DL1fO8O79jqmKWmuqfyfNemyvG9Y6v1+54BO3fulMtcfMdZraepqUnmvP/++874AQcc4Izzjt35Qu3V4sWLbebMmbZ06VJ7/vnnrbm52U4//fQOD5urrrrK/vKXv9ijjz5qixcvti1btth5550X9w0HAABu1GsAAHoGajYAIJGE+kb0M8880+H/P/DAAzZw4EBbsWKFfeUrX7HKykq7//777eGHH7YJEyaYmdn8+fPtkEMOsaVLl9qJJ54Yvy0HAABO1GsAAHoGajYAIJHs19+IrqysNDOzvn37mpnZihUrrLm52SZOnNj+O6NHj7ahQ4fa66+/7hyjsbHRqqqqOvwAAID4iUe9NqNmAwAQNd6xAQC9WcwT0W1tbXbllVfa+PHj7bDDDjOzT//eWXp6uhUWFnb43aKiIvm30GbPnm0FBQXtP76/pwYAAMKJV702o2YDABAl3rEBAL1dzBPRM2fOtPfee88WLFiwXxtw3XXXWWVlZftPWVnZfo0HAAD+IV712oyaDQBAlHjHBgD0dqH+RvRnLrvsMvvrX/9qL7/8spWUlLTHi4uLrampySoqKjr8i+327dtlR+uMjAxvZ1MAABCbeNZrM2o2AABR4R0bAJAIQk1EB0Fgl19+uT3++OO2aNEiGz58eIflY8eOtbS0NFu4cKFNnjzZzMxWr15tGzdutHHjxu33xn6+IO8tJSVFbrOSmure/bS0NJnT3NzsjDc1NTnjycnhv3SelZUll7W0tDjjn++q/Hn5+flyrNbW1lDrMPv0PxdzUcfSTB+bpKQkmaOo8/zjH/9Y5gwbNswZV8fMzGz37t3OuLqefPvS2NjojGdnZ8uczMxMZ1xdf77rTB0z33lWOer8+3LUtqnrz0zfA77redasWc74nj17ZM7UqVOdcbWfM2bMkGOp/WxoaAids2HDBmfc90Khnlux3M+xPLcU372h1uO7zvDldHW9NqNmm1Gz1XkuKCiQOeoc+M6Num5OOOEEZ/zZZ5+VY6WnpzvjgwYNkjkVFRXOeF5enjNeXV0tx1LP+U2bNsmcWM6ZOmbqP9/3PZdVzpYtW2TOZ3/79suOZabreU1NjTPu+5yh7gHfNqt7/fvf/74zPm/ePDmWujZ8f8eXmt17dXXNpl5Tr7u6Xqv1+57jaizf+sMeG96x9WcW33tpfX29XOai9tFMPzd8zyB13/KO3X3qdaiJ6JkzZ9rDDz9sf/7zny0vL6/9b1IVFBRYVlaWFRQU2KWXXmpXX3219e3b1/Lz8+3yyy+3cePG0c0XAIBOQr0GAKBnoGYDABJJqInouXPnmpnZqaee2iE+f/58u/jii83M7Je//KUlJyfb5MmTrbGx0SZNmmR33XVXXDYWAAB8Meo1AAA9AzUbAJBIQv9pji+SmZlpc+bMsTlz5sS8UQAAIHbUawAAegZqNgAgkcTvj5QAAAAAAAAAAODARDQAAAAAAAAAIFKh/jRHV/N19FX/SZOvm2RmZmbobWhsbAy1nli61vo6nSpqX3ydOdWy3NxcmbN7925n3PeflKn1qA6kqmuymT6e2dnZMkd1B/V1c1U5sXRBVvvp6w6r1qPivuOv9sWXo46zb5vT09Od8ViOmbqe+/XrJ3PUeL5u02p/4tmF2kedm8rKSmfcd53H0oVXXZuxdLtWfMcs7FjoWajZGjW7c2r2smXLnPH+/fvLsQ477DBnXHWNNzPbuHGjM56Tk+OMl5aWyrH27NnjjPv2v7Cw0BlXtcRMXwMff/yxM66Oi5musw0NDTLnm9/8pjP+yCOPyJxBgwY54581dtubquVmZkOGDHHGfedZ7c/tt9/ujGdlZcmxqqqqnHHfM6CoqMgZ922z+mw0cuRIZ3z16tVyrOLiYmd869atMgc9A/Vao17zjq3wjs07tsI79hfjG9EAAAAAAAAAgEgxEQ0AAAAAAAAAiBQT0QAAAAAAAACASDERDQAAAAAAAACIFBPRAAAAAAAAAIBIMRENAAAAAAAAAIhUaldvQBhDhw4NnZOUlCSXZWRkOOOtra0yp7Gx0RlPTg4/p5+SkhIqbqa3rbm52RlPTdWnWG3znj17ZM6uXbuc8fz8fJlTW1vrjKttTktLk2PV1dU5475tzsvLc8Z914Y6bpmZmc54ZWWlHEudzyAIZI7az7a2tlDrMPNfz4raNt91rrZBnWefWMZqaGhwxisqKmSOutbUfra0tIQey3c9q+Pc1NQUarvMzNLT00ONZaafZ77rSW2DivueQYrv3vAtQ/dCzaZmd3XNVlS9MDN77733nPFvf/vbMmfVqlWh1u97Lg8aNMgZ9x1ntezggw+WOW+//bYzPmrUKGd806ZNcixVG0866SSZs2zZMmd85MiRMmfr1q3OuLo3fMdswoQJzvif//xnmaOeQWo/Fy5cKMdSnyd9zzP1eUZ9NjTTxyY7O9sZ990bZWVlzngsn3PQvVCvqdddXa95x+YdO+xYvGP37HdsvhENAAAAAAAAAIgUE9EAAAAAAAAAgEgxEQ0AAAAAAAAAiBQT0QAAAAAAAACASDERDQAAAAAAAACIVPhWi52koKBgn46rffr0kb+vOm3G0rXV1wE1bOdc31ixdADdvXt3qPXE0k00JydH5owYMcIZVx1ozcw+/vhjZ1x1Ac7KypJjqc696riY6WPg60KsukdXV1c7474uo/X19c646v5uZjZgwABnXHUn9nWtVcfTt82qC7PqaGymO7qqrrG+e1Ntm+oobaavjVi6EKucWDrTx0J15/Uds4KCAmfct83qPPs6+irqGey7ztQxi+XaCNvpt7M7A/d2Pa1mDxs2zBmfMWOGHOvGG290xn3PJbU/ffv2dcZ9tfTcc891xu+//36ZE0tH+YMOOsgZ37FjhzPenWv2u+++64yff/75cqy3337bGX/kkUdkjtpmdW0eeOCBciz1mWHt2rUyR31uU/XXzKyoqMgZ//DDD53xWDqt+7a5vLzcGfddT6WlpaHGGjx4sBzrvffec8bVM8PM7LLLLnPG77zzTmfc92yKhfoM5rueVq5c6Yxv2LDBGe/Xr58cS30GjrpmU6/jq6fVa96xecdWeMfmHTss3rG7T73mG9EAAAAAAAAAgEgxEQ0AAAAAAAAAiBQT0QAAAAAAAACASDERDQAAAAAAAACIFBPRAAAAAAAAAIBIMRENAAAAAAAAAIhUaldvgDJ48GBLSUnpEGtra5O/n5SU5Iynp6fLnCAInPHm5uYvsYUd3X333aFzepPnnntOListLQ0V953nsrIyZ7y6ulrmNDY2OuNbtmyROWvXrnXGS0pKnPE+ffrIsfLz853x/v37y5y0tDRnXF3PGRkZciylpaVFLquoqHDGKysrZY46n2qbk5P1v4Ope3Pbtm0yR923mZmZMkcdg9bW1lBx3zL1bDLTx2DvZ99ncnJy5FiFhYXOuO88V1VVOeO+bU5NdZeNWPZfUefft8yXE8s2IJyeVrP37NnjjL/55psyJzc31xlvaGiQOV/5ylec8Y0bNzrjO3fulGPNnTvXGfdd3+q+VHXRzKy2ttYZP//8853x7lyz1TlbuHChHEvx7eeUKVOc8fnz5zvjH3zwgRxL1f9vfetbMmfx4sXOeF5ensw58MADnfEdO3bIHGXlypXO+O7du2VOdna2M67qkpm+1tVnE9+9qc7nWWedJXPuvPNOZ1w9gyZPnizHevnll51x3zare/P999+XOQMHDnTG1bPmvPPOk2P98Y9/dMap2T1LT6vXvm1T1HPU9zlebfO4ceOc8XXr1smxNm/e7IwPGzZM5pxyyinOuKqjZrxj847NO7bCO7Zbd6rXfCMaAAAAAAAAABApJqIBAAAAAAAAAJFiIhoAAAAAAAAAECkmogEAAAAAAAAAkWIiGgAAAAAAAAAQKXdrxm5gyJAh+3Q19XVsVHydTlXn2KamptDrSXTl5eVyWb9+/ZzxI4880hn3dWE+9NBDnXFfd1jVBVZ1eTcze+6555xx1bF8wIABcizVaVV1jDfTXdO3b9/ujKsOvGb62Nx2220yJ1FMmTLFGVddgPPy8uRY6nlSV1cXertU596hQ4fKnIEDBzrjvg7lO3bscMZVF2wz3W1YxX3UMYvlWY+u1dNqtupA/cgjj8gc1Rna12l7+fLlzvigQYOc8SFDhsixFN8zRtXT4cOHy5z33nvPGe+JNVsdZ1+n9927dzvjvuf/b37zG2dcXWdFRUVyrF27doVev+q07rs2Vq1a5YyrLvCqXpjp+9b32VB9zvEdG/W5qaamJtQ6zHRt/Nvf/iZz1PlU98Czzz4rx8rKynLG1XPOTD9T1XaZmR177LHO+FNPPeWMP/HEE3IstR71bET31NPqtdo29dwz0zXGt371Lte/f39nvLq6OvRYX//610Pn+Gof79iFznhXv2Pn5+fLnBEjRjjjBQUFMkedN1XjfedMPcc3b94sc9Q96HvHVO+SvGPzjv0ZvhENAAAAAAAAAIgUE9EAAAAAAAAAgEgxEQ0AAAAAAAAAiBQT0QAAAAAAAACASDERDQAAAAAAAACIlG4328VKS0v36RCqOnmb6e6gvu6wiq+bJZ2h3TZt2iSXqe6oqmunr6Ox6mbv67JeUlISaiwz26eb9GcGDRrkjPuuzV27djnjqguzmb7OVKdfX2d4XxfeRNe3b19nXHVoHjx4sBxLdXteu3atzKmpqXHGs7KyQq9fdftV22VmtmHDBmfcdz2pbstKvJ+Z6rnhW49rGc/y+OppNVt15965c6ccq7i42Bn3dWc/5phjnPEXXnjBGQ97f5mZHXTQQXKZqnO+7uiq/vXEmq3O2fDhw+VYS5YsccZ93dGrq6ud8dNPP90Zf+utt+RY6pn9t7/9LXSO6g5vps+bumbVPWumu8OnpurXDLXNqv6ZmZWXlzvjI0aMcMZjqdmrVq2SOeoYqI72o0aNkmO98847zrivNs2cOdMZr6urkzl//etf5TKXtrY2ucx3PpV41GzqdXz1tHqtriHf+lWO7/qur693xt99993QYx144IHOeL9+/WSOqsu8Y/e8d+w+ffrInMLCQmc8NzdX5qi6qMaKpV6rscx0vfTlVFRUOOO8Y/fud+ww28Q3ogEAAAAAAAAAkWIiGgAAAAAAAAAQKSaiAQAAAAAAAACRYiIaAAAAAAAAABApJqIBAAAAAAAAAJFiIhoAAAAAAAAAEKnUrt4AJQgCC4KgQywpKUn+fmqqe1dSUlJkTlNTkzPe0tIic3zjJbLCwkK5bODAgaHGqq2tlcvUNeA7Z+o8+66ns88+2xnPz893xrdt2ybHWrZsmTO+bt06mTNy5EhnfNCgQc74zp075Vhqm2E2ePBgZ3zEiBGhft/M7JNPPnHGN27cKHOam5ud8bS0tFBxM7P09HRnPDMzU+a0tbU54757Q23D3s/rL0PlxDIWulZPq9k7duxwxo866ig5lnqWvv322zJn/Pjxzvjy5cud8TPPPFOO9eCDDzrjW7dulTlHHHGEMz569GiZc//998tlLt25Zi9ZssQZ99XsmpoaZ1ydSzP9XFy0aJEzPmHCBDnW66+/7oyr+m+mz0FWVpbMUfeAumd8z2W1rF+/fjJH1SzfejqjZqu6aGZWX1/vjJ944onOuPr8Z6afgWeccYbMaWxsdMYfeughmaOuzYyMDGf8G9/4hhzriSeecMZbW1tlDrqfnlav1T2pniFmsX2O3L17tzOurm/f+9VHH33kjB977LEyp6qqyhnnHbvnvWP7PmPl5uY64+qd0EyfA3VtJkq95h1b6wnv2HwjGgAAAAAAAAAQKSaiAQAAAAAAAACRYiIaAAAAAAAAABApJqIBAAAAAAAAAJFiIhoAAAAAAAAAECl3G9xuwNXR10d1f/aNEUun1+7UabI7UR1ozczGjBnjjKsOpKprsZlZcrL73058nWbr6uqccdUF2sysuLjYGc/JyXHGfZ2Ty8vLnXHVUdlMd45WnZOHDx8ux/J1dE10RUVFzrg6zw0NDXIsdQ2q54yP6oLsezap+2b79u0yR22brxO6Wqb237fN6D16Ws1WXcs//vhjOZa69tW+mJndcsstzvj3vvc9Z/zBBx+UY6nnku+YvfHGG8646jRvpjuX98SarY5NWVmZHKukpMQZX7RokcxRn4HUWKtXr5Zj1dbWOuO+bVbXZkVFhcxR9S+Wz7lq/VlZWTJnx44dznifPn1kTmfU7JaWFplz5plnOuPq2lD3kplZTU2NM/7000/LHPWs822z2s8hQ4Y440888UTo9aNn6Wn1Wj1ffNe9op6vZvodS8X79esnx/rwww+dcd/7onom8I7d896xffvf2NjojKvzb6bPZyx6U73mHbtn12S+EQ0AAAAAAAAAiBQT0QAAAAAAAACASDERDQAAAAAAAACIFBPRAAAAAAAAAIBIMRENAAAAAAAAAIhUqInouXPn2hFHHGH5+fmWn59v48aN69DduaGhwWbOnGn9+vWz3Nxcmzx5srebJAAAiAY1GwCA7o96DQBIJKlhfrmkpMRuvfVWGzlypAVBYA8++KCdffbZ9tZbb9mhhx5qV111lT311FP26KOPWkFBgV122WV23nnn2auvvhp6w4IgsCAIOsSSkpLk72dkZDjjbW1tMqexsdEZ961n723Cp9LS0uSy/Px8Z1wdS985S0lJccZTU/WlnJmZ6YyvWbNG5pSUlDjj2dnZznh9fb0cq7m52RlX+2Kmr8Hq6mpnPD09XY7Vp08fuSzRqWtTnTPfs6G2tjZU3ExftyqnsrJSjvXJJ5844xs2bJA5TU1NzrjvempoaJDLXHzXOc/TaFGz9TWmntkjR46UYw0dOtQZf+WVV2ROUVGRMz537lxnfODAgXIsdb+qdZiZZWVlOeN5eXkyx3cOwv5+V9fs0tJSZ/w73/mOHGvTpk3O+MsvvyxzKioq5DKXYcOGyWW7d+92xtX1b2Y2ceJEZ/ypp56SOeeee64zXlZW5oz7nv1qWdjjYmbWr18/uawzanZysv6Ojrpv1D2wa9cuOVZubm6odZiZVVVVOeNnnnmmzHnppZeccfXZwPd5Xj1PWlpaZA6+HOq1rte+e1JRzwTf9d23b19nXNWkmpoaOZa6v1944QWZoz57H3TQQTKHd+zu+Y7d2toqc9Q58F3nKkc9exOlXvOO3bPfsUNNRJ911lkd/v/NN99sc+fOtaVLl1pJSYndf//99vDDD9uECRPMzGz+/Pl2yCGH2NKlS+3EE0+M31YDAAAvajYAAN0f9RoAkEhi/hvRra2ttmDBAqutrbVx48bZihUrrLm5ucO3MkaPHm1Dhw61119/PS4bCwAAwqNmAwDQ/VGvAQC9XahvRJuZvfvuuzZu3DhraGiw3Nxce/zxx23MmDH29ttvW3p6uhUWFnb4/aKiItu2bZscr7GxscN/vqP+szMAABAONRsAgO6Peg0ASBShvxE9atQoe/vtt23ZsmU2Y8YMmzJliq1atSrmDZg9e7YVFBS0/6i/5wcAAMKhZgMA0P1RrwEAiSL0RHR6eroddNBBNnbsWJs9e7YdeeSR9qtf/cqKi4utqalpnz90vn37disuLpbjXXfddVZZWdn+o5qkAACAcKjZAAB0f9RrAECiCP2nOfbW1tZmjY2NNnbsWEtLS7OFCxfa5MmTzcxs9erVtnHjRhs3bpzMz8jIcHbjTUpK2qd7pq+bZixU185YupYmuueff14uU8f50EMPdcYHDhwox1KdRn3d5Ldv3+6M+z6Qqc6xaizVadfM7L333nPGfV3GCwoKnPHdu3c746o7s28shH+m+K4z9Z88+jonq2eN6tytOpeb6fvM1wVYrT+WLrzqWMbyzPR1+lXriXd96K2o2Z8aOXKkMz516lQ51g033OCM+/ZTPRdUp3Hfs1w9f3z/ubWqM6tXr5Y5//qv/+qMl5eXO+PduWYPGDDAGV+2bJkc66233nLGfTV7+PDhzriq2Zs3b5ZjHXnkkc54nz59ZI66N0477TSZo46z6jSv4mZmNTU1zrjqGm9mNnjwYGfc95mlM2r2kCFDZM6TTz7pjKtnUH19vRxL7f+uXbtkjroGX3jhBZmjZGZmOuO++qvOJzU7GtTrT2VlZTnjOTk5ciz1XuZbv/rsq+4VX+1V976qL2b6HKxfv17m8I7dPd+xffXS914YVqLXa96xe/Y7dqiJ6Ouuu87OPPNMGzp0qFVXV9vDDz9sixYtsmeffdYKCgrs0ksvtauvvtr69u1r+fn5dvnll9u4cePo5gsAQCejZgMA0P1RrwEAiSTURPSOHTvs3//9323r1q1WUFBgRxxxhD377LP2ta99zczMfvnLX1pycrJNnjzZGhsbbdKkSXbXXXdFsuEAAECjZgMA0P1RrwEAiSTURPT999/vXZ6ZmWlz5syxOXPm7NdGAQCA/UPNBgCg+6NeAwASSehmhQAAAAAAAAAAhMFENAAAAAAAAAAgUqH+NEd3prpmpqWlyRzVNdLXTdLXbTeRqU63ZmavvPKKM646gx911FFyLHU+165dK3NU515fp9UPPvjAGVfdeX3XRW1trTPep08fmaO2LS8vzxnv37+/HKukpEQuS3TqelKduPfs2SPHqqurc8Zj6dCtOjQXFRXJsVRXcd+9obbZ19FXdQ5WnXt9nXbVemJZf1QdfRGNrq7Z6ll+2223ybHUdak6kJvp57y693bs2CHH+vd//3dn/Mknn5Q52dnZzni/fv1kzmOPPeaM79y50xmnZndOzfZd52qbfcespaXFGVed6ysqKuRYgwcPDjWWma6zzz33nMw577zznPH333/fGT/++OPlWIWFhc74+vXrZY6qc4ceemjosTZu3OiM+87Z4Ycf7oyrc2lmtnr1amdcHRvfdf7SSy/JZei9urpeq/X77pWGhoZQ6/CNp/bTd9+pz+tbtmyROWrbeMemXsezXqvPnwUFBTKnuLjYGc/IyJA56jNzorxjq3cD3/tqfX19qPX7+K7B7oJZVQAAAAAAAABApJiIBgAAAAAAAABEioloAAAAAAAAAECkmIgGAAAAAAAAAESKiWgAAAAAAAAAQKSYiAYAAAAAAAAARCq1qzdACYLAgiDoEEtO1vPmGRkZznhtbW1M61ZSU7vtIetSTU1NctmmTZtCxZcuXSrHys3NdcZ3794tcxobG51x37ksKioKldO3b1851tChQ53xlpYWmaPk5+eHXn9hYWHo9SQK9UxR8YaGBjmWus58z63W1lZnPCcnxxnPy8uTY5WVlTnjbW1tMkc963z3s6LW49t/dQ/47s2kpKRwG4ZO0dNq9sEHHxwqbma2ZMkSZzw7O1vm3Hjjjc74hx9+6Izfcccdcqw//OEPznh9fb3M+eY3v+mMP/DAAzJH3cvPP/+8M07N1uJZs9VxMYtt2xT1jE1LS5M5o0ePdsZfeOEFmaOeAYcccojM2bBhgzP+7rvvOuNr1qyRYw0ZMkQuU9Sx+eCDD5xx37NJjZWSkiJzfPujqG144403nPFLLrlEjqXup3hef4heT6vXatt861fr6devn8xR+6lqbF1dnRxLfS7wPUfVfcQ7NvVaiaVeq2Xq3TNWif6OrZb5ctQ9qK4Z3/O0J+Ab0QAAAAAAAACASDERDQAAAAAAAACIFBPRAAAAAAAAAIBIMRENAAAAAAAAAIgUE9EAAAAAAAAAgEjplqZdzNXRV3UGNdOdbvfs2SNzfOMpvi6oiayiokIuGzx4sDOemZnpjPs6vapux76cgQMHOuPp6ekyp6CgwBlX2+zrDqw6mqqOymZmWVlZzrjqAFtdXS3H8h2bRFdZWemMq+tZ/b6Z7p7t62jb3NzsjJeUlDjj6jlnZvbRRx85475u27FcG2o8NZavC7LaH19HadUhuad3Du7pelrNVmP5Ospv2LDBGVc1xszsJz/5iTNeWlrqjE+ePFmOtWDBAmfc1+lbdeAeNGiQzFH3n3peUbM7p2b7jpnaT99nVrWf6vynpKTIse666y5n/Mwzz5Q5zz77rDPuq1nqvhk5cqQzrq5ZM7ONGzc648cdd5zMUddzUVGRM75kyRI51rBhw5zxTZs2yRzFd57Vta7O50MPPSTHamtrc8Z9z+ZYntuIVk+r12Gfr2ZmOTk5zrjvOaZy1HpUfTXzHxtFPXtrampkDu/Y1GsX33WuPjP63qPUu5zv3Kj34kR5x1b3gO88q2vQdz4VdWy6U73mG9EAAAAAAAAAgEgxEQ0AAAAAAAAAiBQT0QAAAAAAAACASDERDQAAAAAAAACIFBPRAAAAAAAAAIBIMRENAAAAAAAAAIhUaldvQBipqXpzW1tbQ8XNzJKSkpzxIAjCbZiZXXfddc54RUWFzMnMzHTGS0pKZE5zc7MzXltb64zX1NTIserr60Otw8ysb9++znj//v1D52RnZ4f6fTN9Dfi2OTnZ/e8tbW1tMic9PV0uc/FdM4WFhc64uv7MzBobG53x3NxcZ7y6ulqOpc5zoli2bJlcdsIJJ4Qaa+rUqXKZOjcHH3ywzFH359FHH+2Mf/zxx3KsXbt2OeO+61w9H33PWnU/qWs2lue2795Q++PLcd2fsTznEU53rtkbNmxwxvv06SNzzjjjDGc8Pz9f5qjnz6RJk5zxW265RY6VlZXljJ977rky57e//a0z7tvmlpYWZ/yII45wxqnZnVOzGxoaZI4ar6mpSeYo6vyruJnZUUcd5YzX1dXJnO9+97vO+FtvvSVzBg0a5Izv3r3bGX/11VflWOoaWL58ucz56le/6owvWrTIGf/nf/5nOZbK8V3nKSkpzrh6n/BR6+kOtXHvbegO29Tbded6ra5v3zu2uld8tU89L9U7tu/5pvbTV/vUu/To0aNlDu/Yhc449VrX67S0NGfcd8zUfvruQXWcFd+clTqeal/M9Dt+Z71jU6+/GN+IBgAAAAAAAABEioloAAAAAAAAAECkmIgGAAAAAAAAAESKiWgAAAAAAAAAQKSYiAYAAAAAAAAAREq3yO2GMjIy5DLVTdPXATQWqgutr6NsPKnOvaprqK/TayzH5sADD3TGVdda33pU11xfB1LVBdbX7VldN75uz6pzreqA2q9fPzmW6vLu66i7bds2Z7y8vNwZ9x3/nJwcZ/zdd9+VOTt37nTGfdusOier4+zrtKvuJ19H4WXLljnjv/nNb2SOcumllzrjvu68qnv25s2bZc4ZZ5zhjKvzvHr1ajmWOje+Z1Ms94ai7md1XZjF1iFadeON97Me+68712zVtVp1xjbTHb1LSkpkTlZWljP+4IMPOuO+e08dMzWWmdlxxx3njFOze17NVjXGTJ8D37lRz2Z1/Ovr6+VYb7zxhjPuO88qR3W6NzP7y1/+4oxfe+21zvghhxwix1qwYIEz7jvO7733njN+7rnnOuOPPfaYHEtdm76areqf797wPYeBz3Tnes07dmHo9VCvqddh1+97J1PX+p49e2SOOgbqOlNxM137fDnquumsd2zq9RfjG9EAAAAAAAAAgEgxEQ0AAAAAAAAAiBQT0QAAAAAAAACASDERDQAAAAAAAACIFBPRAAAAAAAAAIBIMRENAAAAAAAAAIhUaldvgNLW1mZtbW0dYpmZmfL3a2trnfHkZD3X3traKtft2y6XIAic8bS0NDmW2rbGxkaZM3r0aGc8PT3dGd+1a5ccq7m52RkfPHiwzOnXr1+o9ZuZNTQ0OOPZ2dnOeH19vRxLrUedfzOz1FT3Za7Ov5k+NwUFBc54cXGxHEvtZ2VlpcxRx0xdT2odZno/fcdZrcd3zKqrq53xlpYWZzwjI0OOpTQ1NcllBxxwgDO+cePG0OtRx/POO+8MPZaPum5PPfVUZ/z999+XY6njrK5/M32d+XLUvaHOje/ZkJSUFCoeq02bNu0TU8cLsaFmu1Gzqdlh1mHWM2u2us98Nfvkk092xl999VWZc9xxxznjzzzzjDOen58vx1LPp4qKCpmzdetWZ/zpp592xvPy8uRYVVVVzriv/qrz7Ds36nyqZ2B3sHfNpl7HF/XajXpNvQ6zDrOeWa8VX71Wx8x3bdTV1Tnjaj99+5+SkuKM++4NdT+/+eabzni837Gp11+Mb0QDAAAAAAAAACLFRDQAAAAAAAAAIFJMRAMAAAAAAAAAIsVENAAAAAAAAAAgUkxEAwAAAAAAAAAipVs9drHU1NR9OlH6OmPu2bMn9DpUd15fR1/VtVPxdblUYyUlJcmcsWPHOuNr1qxxxn370rdvX2d80KBBMkd1uvXtZ1ZWVqgc3zFWOb5jprbZtx413tChQ51xX2f27du3h4qb6W1WXa1Vp2Ezs82bN4dev7rXfNdTbm6uM646VPs6Vyu+Tqyqq/EBBxwgc+bMmeOMr1y5MtR2mZlNnTrVGT/77LNljuqe/dJLLznjqgOxme427LvO1Xn23c/q2lTxWJ6nvs7JsXQO3ruj7xetA+FRs92o2dRsl95Ws0eMGOGMf+tb35I5qv6eeeaZMqe6utoZP/TQQ51xX/1X96DPJZdc4ow/8MADzvjez8TPi6VmNzc3h85R92DYuJm+z3z3c9jtMtu3ZlOv44t67Ua9pl679LZ6rfjesWNZT9jryXedpaWlOePqPdpMf17YsmWLMx7vd2zq9RfjG9EAAAAAAAAAgEgxEQ0AAAAAAAAAiBQT0QAAAAAAAACASDERDQAAAAAAAACIFBPRAAAAAAAAAIBI6XbOXSwzM3OfrqL19fXy91UHVF8HUtU10tc5WHUUVZ2xfd1Es7OzQ63DzKxfv37O+LvvvuuMq46dZmaFhYXOuK+bp+ooqjrN+pap9cTSgdTXAbSystIZVx1ozfSxUZ1AN2zYIMdS21xTUyNz1PWk9tN3/NXx9N1Piq+jrOrcrI6Z795UOb71f+Mb33DGf/7zn8scda/379/fGb/kkkvkWPfdd58z7useO3/+fGf8V7/6lTNeW1srx8rLy3PGfZ1u1bPGdz+F7YLsW786NhkZGTInlk7Qe3f0/aLtQnjUbDdqNjXbpbfV7Pz8fGf8k08+kTnl5eXO+F/+8heZ881vftMZX7VqlTP+2muvybF27NjhjPvOzYsvvuiMH3fccc6479m0YsUKZ9xXm9T15HsGhR2rO9i7ZlOv44t67Ua9pl679LZ6Hcs7dnV1tTPuuwfU8VTHTO2jmdnAgQOd8aOOOkrmqOdWWlqaMx7vd2zq9RfjG9EAAAAAAAAAgEgxEQ0AAAAAAAAAiBQT0QAAAAAAAACASDERDQAAAAAAAACIFBPRAAAAAAAAAIBI7ddE9K233mpJSUl25ZVXtscaGhps5syZ1q9fP8vNzbXJkyfb9u3b93c7AQBAjKjXAAD0DNRsAEBvlhpr4t///ne755577IgjjugQv+qqq+ypp56yRx991AoKCuyyyy6z8847z1599dX93tjGxka5LDXVvSsNDQ0yJznZPQ/f1NQkczIzM53xtLQ0ZzwlJUWOpdaTm5srczZu3OiMl5SUyBxFbVtra6vMaWtrc8bVcfFRxz8pKUnmVFRUxG39PtXV1c54Xl6eM15UVCTHUufMp7a21hlX+6m210yfM3XN+nKCIJA56v5U59N3ntUy3/rvvPNOZzw7Ozt0zlVXXeWMz58/X471H//xH874GWecIXPU/qjr3HfO1PPEd2+o8XzPTbXNvvMZlu8ZrNZTXl4uc9TxTBRdUa/NunfNrq+vd8bHjBkjx1q7dq0zPnbsWJmzePFiZzwnJ8cZnzhxohxr06ZNzvg777wjc9LT053xgoICmbNhwwZn/Nvf/rYzTs2mZqt7c+XKlTJH3QO+z81PP/20Mz569GhnfNSoUXKs/v37O+O+z8Dq87m6zpcuXSrHUufGd2+o49zS0iJzwopnLffx1WVqNu/Yn8c7Nu/YCvW6c96xVY3xXc+KujZ96+/bt68zrj7jmult7qx3bOr1F4vpG9E1NTV24YUX2rx586xPnz7t8crKSrv//vvtF7/4hU2YMMHGjh1r8+fPt9dee837YQwAAMQf9RoAgJ6Bmg0ASAQxTUTPnDnTvv71r+/zzZ0VK1ZYc3Nzh/jo0aNt6NCh9vrrr+/flgIAgFCo1wAA9AzUbABAIgj9pzkWLFhgb775pv3973/fZ9m2bdssPT3dCgsLO8SLiops27ZtzvEaGxs7/OcGVVVVYTcJAADsJd712oyaDQBAFHjHBgAkilDfiC4rK7MrrrjCfv/738ft7wXNnj3bCgoK2n9KS0vjMi4AAIkqinptRs0GACDeeMcGACSSUBPRK1assB07dtgxxxxjqamplpqaaosXL7Y77rjDUlNTraioyJqamvb5o9Xbt2+34uJi55jXXXedVVZWtv+UlZXFvDMAACCaem1GzQYAIN54xwYAJJJQf5rjtNNOs3fffbdD7JJLLrHRo0fbD3/4QystLbW0tDRbuHChTZ482czMVq9ebRs3brRx48Y5x8zIyLCMjIx94i0tLbLbpIvqQKk6k5rpbpax5MTSAVQpLy+Xy1atWhVqLN+/qjc3Nzvjvi7Eqjup71ypc1NTU+OM+zqwqmMzbNgwmaPW46P+8zVf515l69atzrjq2mumj4Hqjurbx7q6Omc8lu64vu6svu7ZLqoLt5nef982z5kzJ9T6fVTn5hkzZsicwYMHO+NPP/20zJk5c6Yzftxxx3m2zm3atGnOuO9+imfn3q62adOmrt6EbiWKem3We2r2QQcd5Ixv2LBBjqW2eeXKlTJHPedPOukkZ9xXs99//31n/PMNrfa2c+dOZ/zwww+XOep4UrOp2YrqKD969GiZM2HCBGd8yZIlMkd1Z1f7+dxzz8mxPv8nCz4vJydH5qhz861vfcsZV5+Zzcxee+01Z5yanZh4x+Ydm3ds6rXS1e/Y6rr1rUftj7qeffdTLLVv9+7dzri6ZrKzs+VY6jMO9Xr/hJqIzsvLs8MOO6xDLCcnx/r169cev/TSS+3qq6+2vn37Wn5+vl1++eU2btw4O/HEE+O31QAAQKJeAwDQM1CzAQCJJHSzwi/yy1/+0pKTk23y5MnW2NhokyZNsrvuuiveqwEAAPuBeg0AQM9AzQYA9Bb7PRG9aNGiDv8/MzPT5syZE9f/TB4AAOwf6jUAAD0DNRsA0FuFalYIAAAAAAAAAEBYTEQDAAAAAAAAACLFRDQAAAAAAAAAIFJxb1YYL62trdba2toh1tbWJn8/CIJQcd+y5GQ9P6+2Ye9t/aLf9+U0NDTInPfeey8u22VmlprqPv0qbmaWkpISav1mZk1NTc54S0uLM56RkSHHKigocMYrKytlTmNjY6i4md4ftZ6qqio5ljqfvuusubnZGa+vr3fGy8vL5Vjq+PuOs9rmpKQkmaPGU+dZ7aOZWW1tbai4mdnll1/ujPvup3nz5jnjw4YNc8ZvuukmOdb//u//OuO+Z1A8m8yoc6OOvy+nJ9q0aVNXb0JC62k1e/369c74iBEj5FjV1dXO+GGHHSZzlixZ4oy/8sorznhWVpYcq1+/fs54TU2NzJk+fbozvmbNGpmjnn+qzlGzqdnqehowYIDMUedAfc41M8vJyXHGt27d6oyrzwVmumYfdNBBMmfgwIHOuPqs7TvP1GxqdlfpafWad2zesRXqdee8Y6v1+I6z2p8+ffo446WlpXKs7OxsZ3z16tUyR30uUPtSV1cnx8rMzAw1lhn1+svgG9EAAAAAAAAAgEgxEQ0AAAAAAAAAiBQT0QAAAAAAAACASDERDQAAAAAAAACIFBPRAAAAAAAAAIBI6datXaytrc3bJXZv8exM6et0qjrkqu6kWVlZocfydSBVXWh9XTsV1Z3XR61fdRM10x1lc3NznXHVldzMbPfu3c646nRrps+nr6Os6s6qOvqmp6fLsQoLC53xiooKmaOuDbV+X3ditS9paWkyRx1P332mjmd1dbUz/r3vfU+OpUybNk0umzdvXujxlEsvvdQZv/LKK2XOtddeG7f1x0J1KPed51ieG11NbXNUHX3x5fS0mn3SSSc54++++64cS+U8//zzMicvL88ZHzt2rDO+bt06OZZ6Lu/Zs0fmqPHWrl0rc1TndLX/1Gz9uW3Xrl3OuDouZmapqe6P5r7PbOXl5aG2y8xs2LBhobbtjTfekGOdd955znhNTY3MUed51KhRMqeqqsoZV5+1n3766dDrP+KII2TOQw895IwPHz7cGffVWFWb1Wfmnoqa3f30tHrNOzbv2Arv2O53bF8dUcfG91lSjafePc30faOuQfWZxExfZ++8847MUdezOv++54n6XEa93j98IxoAAAAAAAAAECkmogEAAAAAAAAAkWIiGgAAAAAAAAAQKSaiAQAAAAAAAACRYiIaAAAAAAAAABApJqIBAAAAAAAAAJFK7eoNCCMpKSn0siAIZE5ysnsevrW1Vea0tbWFWr9vmxsaGpzx5uZmmZOTk+OMNzU1OeO+/VfrSUlJkTmZmZnOuG8/Vc6wYcOc8fr6ejlWdXW1M15YWChzGhsbnXF1/s30MVBj+Y6ZumbUWD7qnLW0tMgcdQ2kpaXJnPT0dGfcd2/U1tY64zU1NTInrHnz5sllM2bMcMbnzp0rcy677DJn/Ne//rUzfvvtt+uN66Z814biuwezsrKccXUP+K6ZWJSVlcV1PESnO9fs8847zxnfunWrHOuFF15wxk899VSZs2TJEmdc1X9fXVL78k//9E8y55VXXnHGc3NzZU5qqvujITVb1+xnn33WGVfXxubNm+VYBxxwgDOuPueZmX388cfO+JgxY2TOM88844yfdNJJzvjYsWPlWLt373bGfcds8eLFzrj6zGimPwOpe+DAAw+UYx155JHO+GOPPRZ6/evXr3fGfecsIyPDGfc9N33Px+6Kmt0zdOd6zTs279gK79jud+zy8nI5ljo2vntDLfPVa3VsBg4c6Iz369dPjqWuzU2bNskctc3q3MRyzVCv9w/fiAYAAAAAAAAARIqJaAAAAAAAAABApJiIBgAAAAAAAABEioloAAAAAAAAAECkmIgGAAAAAAAAAETK3Rq9G0hOTvZ2XN1bLB11VadVFY9l/b6Omao7qupYb6a7AO/atcsZHzRokBwrLy/PGfd1h1Xb5usOqzqDb9++PfT61fH0nTPVUTY/P1/mqPHU+vfs2SPHUsfM101dnRu1ftVN1kx3G/Z1IVbn03c9Z2dnO+OVlZUyJ57mzp3rjP/4xz+WOT/96U9DreP//b//J5ddf/31ocaKt1ieQUpWVlboHF+353jydUhG1+lpNfsHP/iBM666mZvpZ+bzzz8vc4477jhn/J133nHGfffR4Ycf7owvXrxY5px00knO+KpVq2TO7t27nXFqtq7ZZ599dqixfPVXdW73PZdHjRrljPs62h977LHOuPo88eabb8qxSktLnfHi4mKZo86N7zgPHDjQGVefMysqKuRYO3fudMZ917OSk5PjjPs+m6nzGcv6uzNqdvfT0+o179i8Yyu8Y7vfsWtra+VYdXV1zrjvPFOvqddR4BvRAAAAAAAAAIBIMRENAAAAAAAAAIgUE9EAAAAAAAAAgEgxEQ0AAAAAAAAAiBQT0QAAAAAAAACASOnWsV0sKSlpny65vm6iapmvo6/qqOvrJBy2a2h6erocS3VNVR3TzXQXWtWZ3Lcv/fr1c8br6+tljuocPGDAAJmjNDc3O+O+DqyqC62va6nqzurrDqvOm7qefOtX15nq2mqmO92rjs6x3BuxdE72dW5Wy1Sn33i7+OKLnfGf/vSnoce66qqrnPHCwsLQY3UW3zXQm5SVlXX1JsChp9XsU045xRlXz14zXRsXLVokc7Zs2eKMH3rooc646gBvFtsztn///s54eXm5zPnBD37gjL/33nvOODXb7OWXX3bGJ06c6Iz7zvPu3budcXUsfct8+/nSSy8549OnT3fGX331VTnWqlWrnPERI0bIHHU9++5n9VlXnbMPP/xQjvXJJ5844+pzlpk+znV1dc54bm6uHMt3Pfcm1Ozup6fVa96xecdWeMd2f/7zbbO6B9S59K2fet27dHa95hvRAAAAAAAAAIBIMRENAAAAAAAAAIgUE9EAAAAAAAAAgEgxEQ0AAAAAAAAAiBQT0QAAAAAAAACASDERDQAAAAAAAACIVGpXb4ASBIEFQdAh1tbWJn8/Odk9p+7LUcvUWD4tLS3OeHp6usxJSUlxxmtqamROVlaWM56WluaMl5eXy7F2797tjF900UUyZ/Hixc74tm3bZE5RUZEzXlFR4YzHcp4bGxtlTlJSUuj17H3tfdFYGRkZocfyXWdqf1Q8NVXfyurarKurkzlqPHXN+taTmZkpc5Rp06aFzvHdN2H98pe/dMavuOKKuK0j3mJ5BsZC3QPqOo/Fzp075bLa2tq4rQfx09Nq9lNPPeWM9+3bV+Ycc8wxznhra6vM2bJlizOuamZeXp4c64ILLnDG1T1pZvbXv/7VGR8/frzMefHFF0NtGzXbbMyYMc74nj17nPHs7Gw51oABA5xx3/5v2LDBGR81apTM+epXv+qMqzp/8MEHy7E++ugjZ1xd/z6+/VSfdevr651x32dTVUt851l9zlHb7PtsFs+a2dWo2T1LT6vXvGPzjh12rER/xy4oKJBjqWdyLO+L1OuepzvVa74RDQAAAAAAAACIFBPRAAAAAAAAAIBIMRENAAAAAAAAAIgUE9EAAAAAAAAAgEgxEQ0AAAAAAAAAiJRuD9nFXB19fZ3huytfl82wXWPNdKfRwsJCZ9zXgfX99993xhctWiRzRo4c6YxXVVXJnOrqamdcdS1VXVbN/B1NFdWdvrW1Veao/VFjqc6sPr6OuuraUOfT1zW2qanJGfd1m1br922zum5VR18fdT3//Oc/Dz2Wz+9//3tn/JVXXnHGGxoa4rr+sKZNmyaXxdLtOBad0Tl406ZNka8D8dXTavYBBxzgjPu6SX/44YfO+Lnnnitznn76aWdc1TJVL83M3n77bWc8JydH5kyYMMEZX7JkicxRz7mTTz7ZGadmmxUXFzvjb775pjM+fPhwOZaqP0cccYTMUct8XeizsrKc8bVr1zrjJSUlcqyBAwc64776r86BOme+nB07djjjvvvJ9/k47Pp926x0Vs3uDNTsnqWn1WuFd2zesZVEf8fu06ePHEvVy4qKCpmj1kO97nm6U73mG9EAAAAAAAAAgEgxEQ0AAAAAAAAAiBQT0QAAAAAAAACASDERDQAAAAAAAACIFBPRAAAAAAAAAIBIMRENAAAAAAAAAIhUaldvQLwEQeCMJyUlyZzU1PC7r8ZrbW11xtva2uRY2dnZoberoaEh1Hr69Okjx9q+fbszvnjxYpmza9cuZ9x3nBsbG53x3NxcZ1ydSzO9n/X19TJHHc+MjAyZk56e7oynpKSEXn8s14Y6nsnJ7n87UnEzvS8q7tu2pqYmmVNYWOiMt7S0yBzl5z//eeicyy+/3Bn3bfOFF17ojN9+++3O+JVXXhl2szrNG2+84Ywff/zxnbwl+6+srKyrNwER6+qaXVNT44xnZmbKsdSz7IknnpA56tmsaklzc7Mc691333XGKyoqZM7QoUOd8Vie/+r4U7PNli9f7owfeuihznhtba0cq6ioyBn/+OOPZU5JSUmo7fLlqPUPGTJEjrVlyxZn/KOPPpI5BxxwgDM+cOBAmaOsW7fOGfcd57S0NGfc9wxS15m6NtVz5ovW09NQs3u3rq7XvGPzjq3wju3+XOo7ZiqnX79+MmfAgAHOOPW65+lO9ZpvRAMAAAAAAAAAIsVENAAAAAAAAAAgUkxEAwAAAAAAAAAixUQ0AAAAAAAAACBSTEQDAAAAAAAAACIVqqXtDTfcYDfeeGOH2KhRo+zDDz80s0+7zV5zzTW2YMECa2xstEmTJtldd90lO3D3Jqqbpupm68tRXXvNdEfV6upqZ7x///5yLNVpduvWrTJHdSBV3UzNzLKyspzxzZs3O+O+60Ut83XnVcfM1zk4bOdm3/qbm5tDr18tU5121XaZ6XOjuhOb6f1X++Ibr7y83BmfNWuWHCszM9MZv+6662SOum/mzZsnc5QNGzaEzukMvi7Mxx9/vDO+ZcsWmTN48ODQ26CO57Rp00L9vi9HdRtHONRsTT2vKisrZc7RRx/tjPs6qq9evdoZV93BVb0008//yZMny5xnn33WGfd9Njj88MOdcbUv1GyzMWPGOONDhw51xleuXCnHOuaYY5zxRx99VOYsWrTIGf/a174mc1Sd3bZtmzP+1FNPybHOPPNMZ/zggw+WOQMGDHDGq6qqZM4bb7zhjNfV1Tnjvs85qanuVyB1n/nGU9eZui57qvr6emecmr3/qNca79i8Yyu8Y7vfsVUdNzPLyclxxo877jiZM3LkSGecet199YR6Hfob0Yceeqht3bq1/WfJkiXty6666ir7y1/+Yo8++qgtXrzYtmzZYuedd15cNxgAAHw51GwAALo/6jUAIFGE+ka02af/IlFcXLxPvLKy0u6//357+OGHbcKECWZmNn/+fDvkkENs6dKlduKJJ+7/1gIAgC+Nmg0AQPdHvQYAJIrQ34hes2aNDR482EaMGGEXXnihbdy40czMVqxYYc3NzTZx4sT23x09erQNHTrUXn/9dTleY2OjVVVVdfgBAAD7j5oNAED3R70GACSKUBPRJ5xwgj3wwAP2zDPP2Ny5c239+vV2yimnWHV1tW3bts3S09OtsLCwQ05RUZH379TMnj3bCgoK2n9KS0tj2hEAAPAP1GwAALo/6jUAIJGE+tMcn29GcsQRR9gJJ5xgw4YNs//7v//zNtjxue666+zqq69u//9VVVUUSgAA9hM1GwCA7o96DQBIJKH/NMfnFRYW2sEHH2xr16614uJia2pqsoqKig6/s337duffu/pMRkaG5efnd/gBAADxRc0GAKD7o14DAHqz0M0KP6+mpsY+/vhju+iii2zs2LGWlpZmCxcutMmTJ5uZ2erVq23jxo02bty4uGxsT9TW1iaXtbS0OOOZmZkyp7W11Rmvra11xtPT0+VY6l/F6+rqZE5KSopcpqxfv94ZT052/zvI4MGD5Vj9+/d3xn3brI5Nc3OzzFHUOfNR10BaWprMUcdZnf+GhgY5VlJSUqi4b1lubq7M+exv2e2tvr7eGfddmyeffLIzvmXLFpnju27Cuv322+M2Vjw1NTWFzonncfFR9/O0adNkzkcffRTV5sCBmv0P6vnbr18/maOeWXl5eTLnK1/5ijP+xhtvOOO+GquWvfDCCzJH7Wd5ebnMWb16tTO+9yTIZ6jZZuvWrXPG1TXju84qKyud8dNPP13mLFu2zBn3nWe1DSUlJc74wIED5Vh9+/Z1xnfu3ClzioqKnHHf9fzBBx8442qCLZbPrKqWmekarD6b+T5nBUEQbsO6gbKysq7ehIRBvf5ivGPzjs07tvsd21ev1efSAw88UOao80m97r56Qr0ONRF97bXX2llnnWXDhg2zLVu22KxZsywlJcW+853vWEFBgV166aV29dVXW9++fS0/P98uv/xyGzduHN18AQDoZNRsAAC6P+o1ACCRhJqI3rRpk33nO9+x3bt324ABA+zkk0+2pUuX2oABA8zM7Je//KUlJyfb5MmTrbGx0SZNmmR33XVXJBsOAAA0ajYAAN0f9RoAkEhCTUQvWLDAuzwzM9PmzJljc+bM2a+NAgAA+4eaDQBA90e9BgAkkv1qVggAAAAAAAAAwBdhIhoAAAAAAAAAEKlQf5oD8aU6faam6tOSlZXljKuOrr5Ot2osX9dW1TXVt80HHXSQM666mebl5cmxRo4c6YxnZ2fLnA8//NAZV52GzXS33V27djnj3//+9+VYieLhhx92xocOHeqMDxo0SI7V2NjojC9btiz8hkGaN2+eMz5t2jSZo5b5upcrmzZtCp0DxIN6lvuo7vDp6ekyR9U5VbMzMzPlWP369XPGfd3RVc32dZRX26A+G/TEmt23b185VktLizOu6pKZ2QEHHOCML1y40Bk/9dRT5ViFhYXOeHl5ucxRDcuWL18ucy688MJQ66mvr5djbdy40Rnv06ePzHnggQdCjWVmlpKS4oyr+uPb5iAInHHf51n1uV2NFYv7779fLrv00kud8WeffVbmTJo0yRlPSkpyxn37cvDBBzvjixcvljlAV+Adu/e8Y8e7XqtzoI6zqjtmusb41q+uTbUvZmb5+fnOeGe9Y69cudIZ7+p67bs21P2pznMs77HdWU94x+Yb0QAAAAAAAACASDERDQAAAAAAAACIFBPRAAAAAAAAAIBIMRENAAAAAAAAAIgUE9EAAAAAAAAAgEjpNqyInOoO6utYHbaja01NjRxLdef1dTnPyMhwxisqKmSO6sytOveWlZXJsR5//HFnXHX6NdNdUKurq2WO6hqvOv1Cd1O/7777nHF1XZjpe8DXUXjGjBnOuK8LrtoGtR5f52TFd82oezCWjsqxmDZtWlzHC8t3rwPdDTW799TsAQMGyLEGDhzojNfW1sqc+++/3xn/p3/6J2dcda03M0tPT3fGc3JyZI6qGSeddJLM2blzpzOuro1XXnlFjvW1r33NGffV+fXr1zvjvjqr7idVG9X17+P7zKDMmzfPGY+lxk6dOlUuU8+aSZMmyRx1baj9/NOf/iTHmjx5slwGdCfUa+q1omqf2i51LZnp2tPQ0CBzhg8f7oyPGDEi9HrUdbZu3To51nvvveeMq+Nipq/1rq7Xav/NzDIzM53xsDWxp+oJ79h8IxoAAAAAAAAAECkmogEAAAAAAAAAkWIiGgAAAAAAAAAQKSaiAQAAAAAAAACRYiIaAAAAAAAAABApJqIBAAAAAAAAAJFK7eoN6I6CIJDL3njjDWf8lFNOccbr6urkWPPnzw+3YR7Tpk1zxtPT02VOc3OzM97Q0CBzMjMznfGsrCyZ09raGmr9vm1W52bjxo0yR61n586dMqe6utoZHzhwoMxJdCtXrnTG6+vrnfFvfvObcqxnn33WGU9JSZE5//7v/+6M//a3v5U5vns9XtT156Pume5s3rx5zrh6NpmZNTU1RbU5QNyp+1I948x0zVR1zvdMombHr2anpaXJsQ444ABn3PdcPu+885xxVReHDx8ux1LnrKKiQua88sorzvjUqVNlzuuvv+6Mf/WrX3XG+/fvL8c66qijnPGXXnpJ5vium7CSkpLiNpZPRkaGMz5p0qS4rSPen0va2tqc8dRU9yvg5MmT47p+oCtQr6nXSnKy+3uYu3btcsZ9tVe9x6jnrpm+BtevXy9zysvLnfE1a9Y44zt27JBjqXdpdVzMum+9TpR3bGXLli1yWU94x+Yb0QAAAAAAAACASDERDQAAAAAAAACIFBPRAAAAAAAAAIBIMRENAAAAAAAAAIgUE9EAAAAAAAAAgEi5WyYnOF83zeOPP94ZnzNnTuj1TJs2zRn3dVq9//77nfF58+Y541dccYUcS63H12VTdbrt27evzFFaWlqc8draWpmjlvk6Cqtuu9nZ2TInLy/PGVedZmH25ptvOuOqy/wLL7wgx1Kde3335m9+8xtnXHWGN9P3QLy71itqPZ21/nhSzzOgJ/F181a1xNe1u6GhwRlXHcgzMzPlWNTs+NXsdevWybE2b97sjOfm5socdQ5U/Xv99dflWLF0gVc148EHH5Q56lpfuXKlM67Ov5nZY4895oyrWv5Fy8LqrJr54osvOuPjx493xquqquRY+fn5odevPuv76q/KUXxjxfOcAfuLeq1Rr8PX68rKSmd827ZtcqxY6vWmTZuccXWdmelrvb6+3hn31WvffaP0xHrdm96xFXUt9RR8ogAAAAAAAAAARIqJaAAAAAAAAABApJiIBgAAAAAAAABEioloAAAAAAAAAECkmIgGAAAAAAAAAESKiWgAAAAAAAAAQKRSu3oDlMcee8ySkpI6xEpKSuTvl5aWOuO+nORk9zx8ZmamzGlpaZHLXL761a/KZXvv32fuv/9+mTNz5kxnfM6cOc54U1OTHCstLU0uU2pra53x5uZmmZOVleWMp6enh15/Y2OjM15XVydzUlJSnHHfeVbXRnZ2tmfrepZp06aFzvGds7y8PGd89uzZzvj06dPlWPfee2/oHHU/BUEgc+Kps9bT0/zpT3/q6k1AJ+jKmh1Pvlqmaqbvudja2hp6PWHX70PNdtfsWJ7XZWVlcpnan/r6emdc7aOZWVVVlTPuu/7VOWtoaJA56tqM5Tyr/Yn3PRvPOqvGWrVqlcwZP358qHU88sgjcpk6zlOmTJE56nPbvHnzQufEoq2tLW5joetQr92o19RrF+p1bDqjXic633XeE/CNaAAAAAAAAABApJiIBgAAAAAAAABEioloAAAAAAAAAECkmIgGAAAAAAAAAESKiWgAAAAAAAAAQKRSu3oDlOrq6n1ivk7WapmvA6jq9vu1r31N5jz00EPO+NSpU53x++67T4518MEHy2WKr3Oti68Dq5Kaqi8L1TFbdZr1bYPq9OrrtLt3l+fPNDU1hV6/6kDr2wbftnVX3/ve95zxe+65R+ZcfPHFzriva+3s2bOdcdWx3TeWup981LXpW4/qaj1kyBBnfPPmzaG3K1E6/arO0Xv27OnkLUFX6MqaXVpaGjonPz9f5ijqGZORkSFz1P6omkXN7pyaHUun+ZaWFrlM7Y+K++qC2k91Ls3c95+Z/9rwHU8X3z2jnv++4xxLbVQ58ayzY8aMCZ2Tm5sbOmfKlCnO+Lx582SO+jyl4r7x1GdD33Xm2zb0HNRrN+o19dqFet196zXv2D37HZtvRAMAAAAAAAAAIsVENAAAAAAAAAAgUkxEAwAAAAAAAAAixUQ0AAAAAAAAACBSTEQDAAAAAAAAACKVFHSzdpNVVVVWUFDQpdswYsQIuayiosIZ/5d/+ZfQ61EdUD/88EOZo7p5b9q0yRkvLi6WY6WkpDjjaWlpMkfxdXptbm4OFfd1mlXdjn3brC7x2tpamaOoY3bfffeFHquzqM7ovg68an+mTp0qc1S3ZXX8fcds+vTpzrivc7M6N76u3orK8V2biU51VX/11Vc7eUvCqaysjKkjOz7VHWp2LPr27euMl5SUyJwhQ4Y447HUWcX3XKZmx69mq3rlW78vJ57U+n3XUjzrnDrPvvWr69a3XWo/fa8l3eyV5QvNmzdPLps2bVrc1vPiiy/KZRMmTIjbehTffsYT9Xr/UK+p19Tr+KJe95563Z31xHfsL1Ov+UY0AAAAAAAAACBSTEQDAAAAAAAAACLFRDQAAAAAAAAAIFJMRAMAAAAAAAAAIsVENAAAAAAAAAAgUkxEAwAAAAAAAAAildrVG9AdrVu3Ti675JJLnPG2tjZn/L777ovLNn1mzJgxznhxcbEz3tzcLMdKTXWf/paWFpmjlqWlpckctSwpKSn0+hsbG51xdfzNzNLT053x5GT97zBq24IgkDnd1T333BM6Z8aMGc54a2urzFHHxnecw46lzqWZvgbUuTTT26buDXX9fdF6lJ54PSllZWVdvQnAl7Znz55QcTOzlStXOuO+51JJSYkzXlpaGipuRs2OZ82OpZbFUktU3DdWLLVE8R0zdQ5Uju+YpaSkhM4Ju12xiOU433vvvTJn2rRpodY/ffp0uSye9X/ChAlxGwvobqjX1GuFep3Y9dqHd+zuj29EAwAAAAAAAAAixUQ0AAAAAAAAACBSTEQDAAAAAAAAACLFRDQAAAAAAAAAIFJMRAMAAAAAAAAAIuVu6eqxefNm++EPf2hPP/201dXV2UEHHWTz58+3Y4891sw+7VA5a9YsmzdvnlVUVNj48eNt7ty5NnLkyLhvfFRmzJghlzU3Nzvj9913X+j1qO7bVVVVMkd1R1XdeefPny/HKioqcsZ9XYDVsgEDBsicsF2AMzIyQo/V1NQkcxoaGpxxdczM4tsFt6tdeumlzrivC7M6zrF0tFXdeadOnSrHUjmxdOeNpXNwfX29M+7raJwo1DnYtGlTJ28Jvkgi1OvuwFd/1q1bFyruQ82OX832dVNXy3w5YetfvLu5q/F8NTNsjm8stSyWz1K+HLXN8fzMpj6b+8ybNy9uY/nujfT09FDrj3UbFN96sP+o2dGjXlOvqde9p173Non2jh1qVqW8vNzGjx9vaWlp9vTTT9uqVavstttusz59+rT/zs9+9jO744477O6777Zly5ZZTk6OTZo0ST6oAABAfFGvAQDoGajZAIBEEuob0f/zP/9jpaWlHb5lO3z48Pb/HQSB3X777fbjH//Yzj77bDMz++1vf2tFRUX2xBNP2Le//e04bTYAAFCo1wAA9AzUbABAIgn1jegnn3zSjj32WPvWt75lAwcOtKOPPrrDf6a1fv1627Ztm02cOLE9VlBQYCeccIK9/vrrzjEbGxutqqqqww8AAIhdFPXajJoNAEC88Y4NAEgkoSai161b1/63qJ599lmbMWOG/ed//qc9+OCDZma2bds2M9v37yIVFRW1L9vb7NmzraCgoP3H97eTAADAF4uiXptRswEAiDfesQEAiSTURHRbW5sdc8wxdsstt9jRRx9t06dPt2nTptndd98d8wZcd911VllZ2f5TVlYW81gAACCaem1GzQYAIN54xwYAJJJQE9GDBg2yMWPGdIgdcsghtnHjRjMzKy4uNjOz7du3d/id7du3ty/bW0ZGhuXn53f4AQAAsYuiXptRswEAiDfesQEAiSRUs8Lx48fb6tWrO8Q++ugjGzZsmJl92lShuLjYFi5caEcddZSZmVVVVdmyZctsxowZ8dniTjB37ly5bNq0aaHGCvv7Zvv+Z1efd8cdd4QeT9n7w8wXxc3Mli9f7ozn5OTInJKSklBx3386lpWV5YynpKTIHLUsCAKZoyQlJYXO6Wpqm/f3m5F7U9e6up+mTp0qx1LnxnfO1H76zllLS4sznpwc6t/oEsqmTZuc8ba2tk7eEvgkSr1OJNTs+NXsWGp5LM84tc2+9ceyn/F8/nbnz0Zd/RlMrT+Wz/pKenp6XNe/ZcsWZ1zdz75rSa3n83/LGLGhZvcu1GvqtUK9hpJo79ihJqKvuuoqO+mkk+yWW26x888/39544w2799577d577zWzTy+4K6+80n7605/ayJEjbfjw4Xb99dfb4MGD7Zxzzoli+wEAwF6o1wAA9AzUbABAIgk1EX3cccfZ448/btddd53ddNNNNnz4cLv99tvtwgsvbP+dH/zgB1ZbW2vTp0+3iooKO/nkk+2ZZ56xzMzMuG88AADYF/UaAICegZoNAEgkoSaizcy+8Y1v2De+8Q25PCkpyW666Sa76aab9mvDAABA7KjXAAD0DNRsAECi4A+hAgAAAAAAAAAixUQ0AAAAAAAAACBSof80R6IL2xk6UTpJ19bWymV7d4H+oriP6vY7dOjQ0DkFBQUyJ2wX2u9973uhft/M3wFVrV91J/Z1pu2sTqu5ubmhfv++++6Ty+LZgb6zxNK5uCcqKyvr6k0AsJ8SvWbH0s09njXbR43l28d41p9EqWVqP32f29Vnk+Rk9/d6li5dKsc6/vjjnfFHHnlE5lxwwQVymfLUU0+F+v2e+PkL6M2o19TrzhirO0uU/Uy0d2y+EQ0AAAAAAAAAiBQT0QAAAAAAAACASDERDQAAAAAAAACIFBPRAAAAAAAAAIBIMRENAAAAAAAAAIgUE9EAAAAAAAAAgEildvUGAGGUlZWFivsUFhbKZSUlJaHipaWlcqykpKRQ22VmFgSBM97W1hZ6Hffee68zPm3aNJkzb9680Dn19fVyWVixrB+dY9OmTV29CQB6CGp2+Joddh3xzlFi2eaeyPc5Qx2De+65J/R6TjjhBGf8ggsuCD2WD5+bAHwZ1GvqNbpWor1j841oAAAAAAAAAECkmIgGAAAAAAAAAESKiWgAAAAAAAAAQKSYiAYAAAAAAAAARKrbNSuM5x9qB3x811pra6sz3tLS4ow3NTXJsVTDAN/6VcOE5GT3vx3F0pTAt82x5KSkpIQeL57rR+fobc/o3rY/nY3jh86S6DWb5kddL57HQF2DPFM1js3+4fihs1CvqdcIrzc9o7/MviQF3WyPN23a5O2OCgBAvJSVlclO3fhi1GwAQGegXu8f6jUAoDN8mXrd7Sai29rabMuWLZaXl2dJSUlWVVVlpaWlVlZWZvn5+V29eZ2O/Wf/2X/2n/2P//4HQWDV1dU2ePBg+S0IfLHP1+zq6mquV/af/Wf/2f8E3H+z6I4B9To+qNf/kOj3a6LvvxnHgP1n/7u6Xne7P82RnJzsnD3Pz89PyIvkM+w/+8/+s/+JKqr9LygoiPuYiebzNfuz/xSQ65X9Z//Z/0SV6PtvFs0xoF7vP+r1vtj/xN5/M44B+8/+d1W95p+VAQAAAAAAAACRYiIaAAAAAAAAABCpbj8RnZGRYbNmzbKMjIyu3pQuwf6z/+w/+8/+J+b+9zSJfr7Yf/af/Wf/E3X/zTgGPUminyv2P7H334xjwP6z/129/92uWSEAAAAAAAAAoHfp9t+IBgAAAAAAAAD0bExEAwAAAAAAAAAixUQ0AAAAAAAAACBSTEQDAAAAAAAAACLVrSei58yZYwcccIBlZmbaCSecYG+88UZXb1IkXn75ZTvrrLNs8ODBlpSUZE888USH5UEQ2E9+8hMbNGiQZWVl2cSJE23NmjVds7ERmD17th133HGWl5dnAwcOtHPOOcdWr17d4XcaGhps5syZ1q9fP8vNzbXJkyfb9u3bu2iL42vu3Ll2xBFHWH5+vuXn59u4cePs6aefbl/em/fd5dZbb7WkpCS78sor22O9+RjccMMNlpSU1OFn9OjR7ct7875/ZvPmzfZv//Zv1q9fP8vKyrLDDz/cli9f3r68tz8De4NEqddmiV2zE71em1GzPy/R6rUZNduMmt0bJErNTuR6bUbNpl53lGg1m3rdvet1t52IfuSRR+zqq6+2WbNm2ZtvvmlHHnmkTZo0yXbs2NHVmxZ3tbW1duSRR9qcOXOcy3/2s5/ZHXfcYXfffbctW7bMcnJybNKkSdbQ0NDJWxqNxYsX28yZM23p0qX2/PPPW3Nzs51++ulWW1vb/jtXXXWV/eUvf7FHH33UFi9ebFu2bLHzzjuvC7c6fkpKSuzWW2+1FStW2PLly23ChAl29tln2/vvv29mvXvf9/b3v//d7rnnHjviiCM6xHv7MTj00ENt69at7T9LlixpX9bb9728vNzGjx9vaWlp9vTTT9uqVavstttusz59+rT/Tm9/BvZ0iVSvzRK7Zid6vTajZn8mUeu1GTWbmt2zJVLNTuR6bUbNpl7/Q6LWbOp1N67XQTd1/PHHBzNnzmz//62trcHgwYOD2bNnd+FWRc/Mgscff7z9/7e1tQXFxcXBz3/+8/ZYRUVFkJGREfzhD3/ogi2M3o4dOwIzCxYvXhwEwaf7m5aWFjz66KPtv/PBBx8EZha8/vrrXbWZkerTp09w3333JdS+V1dXByNHjgyef/754Ktf/WpwxRVXBEHQ+8//rFmzgiOPPNK5rLfvexAEwQ9/+MPg5JNPlssT8RnY0yRqvQ4Cajb1+lOJVrMTtV4HATWbmt3zJWrNTvR6HQTU7CBIvHodBIlbs6nX3bted8tvRDc1NdmKFSts4sSJ7bHk5GSbOHGivf766124ZZ1v/fr1tm3btg7HoqCgwE444YReeywqKyvNzKxv375mZrZixQprbm7ucAxGjx5tQ4cO7XXHoLW11RYsWGC1tbU2bty4hNr3mTNn2te//vUO+2qWGOd/zZo1NnjwYBsxYoRdeOGFtnHjRjNLjH1/8skn7dhjj7VvfetbNnDgQDv66KNt3rx57csT8RnYk1CvO0q06zWR67VZ4tbsRK7XZtRsanbPRc3+h0S8VhO5ZidqvTZL7JpNve6+9bpbTkTv2rXLWltbraioqEO8qKjItm3b1kVb1TU+299EORZtbW125ZVX2vjx4+2www4zs0+PQXp6uhUWFnb43d50DN59913Lzc21jIwM+4//+A97/PHHbcyYMQmx72ZmCxYssDfffNNmz569z7LefgxOOOEEe+CBB+yZZ56xuXPn2vr16+2UU06x6urqXr/vZmbr1q2zuXPn2siRI+3ZZ5+1GTNm2H/+53/agw8+aGaJ9wzsaajXHSXS9Zqo9dossWt2ItdrM2o2Nbtno2b/Q6Jdq4lasxO5Xpslds2mXnfvep0a+RqAEGbOnGnvvfdeh7/fkwhGjRplb7/9tlVWVtof//hHmzJlii1evLirN6tTlJWV2RVXXGHPP/+8ZWZmdvXmdLozzzyz/X8fccQRdsIJJ9iwYcPs//7v/ywrK6sLt6xztLW12bHHHmu33HKLmZkdffTR9t5779ndd99tU6ZM6eKtA6Akar02S9yanej12oyaTc0GeqZErdmJWq/NqNnU6+5dr7vlN6L79+9vKSkp+3St3L59uxUXF3fRVnWNz/Y3EY7FZZddZn/961/tpZdespKSkvZ4cXGxNTU1WUVFRYff703HID093Q466CAbO3aszZ4924488kj71a9+lRD7vmLFCtuxY4cdc8wxlpqaaqmpqbZ48WK74447LDU11YqKinr9Mfi8wsJCO/jgg23t2rUJcf4HDRpkY8aM6RA75JBD2v/TqUR6BvZE1OuOEuV6TeR6bZa4NZt6vS9qNjW7J6Fm/0MiXauJXLMTtV6bUbP3Rr3uXvW6W05Ep6en29ixY23hwoXtsba2Nlu4cKGNGzeuC7es8w0fPtyKi4s7HIuqqipbtmxZrzkWQRDYZZddZo8//ri9+OKLNnz48A7Lx44da2lpaR2OwerVq23jxo295hjsra2tzRobGxNi30877TR799137e23327/OfbYY+3CCy9s/9+9/Rh8Xk1NjX388cc2aNCghDj/48ePt9WrV3eIffTRRzZs2DAzS4xnYE9Gve6ot1+v1Gu3RKnZ1Ot9UbOp2T0JNfsfEuFapWbvK1HqtRk1e2/U625WryNvhxijBQsWBBkZGcEDDzwQrFq1Kpg+fXpQWFgYbNu2ras3Le6qq6uDt956K3jrrbcCMwt+8YtfBG+99VbwySefBEEQBLfeemtQWFgY/PnPfw5WrlwZnH322cHw4cOD+vr6Lt7y+JgxY0ZQUFAQLFq0KNi6dWv7T11dXfvv/Md//EcwdOjQ4MUXXwyWL18ejBs3Lhg3blwXbnX8/OhHPwoWL14crF+/Pli5cmXwox/9KEhKSgqee+65IAh6974rn+/oGwS9+xhcc801waJFi4L169cHr776ajBx4sSgf//+wY4dO4Ig6N37HgRB8MYbbwSpqanBzTffHKxZsyb4/e9/H2RnZwe/+93v2n+ntz8De7pEqtdBkNg1O9HrdRBQs/eWSPU6CKjZ1OyeL5FqdiLX6yCgZlOv95VINZt63b3rdbediA6CILjzzjuDoUOHBunp6cHxxx8fLF26tKs3KRIvvfRSYGb7/EyZMiUIgiBoa2sLrr/++qCoqCjIyMgITjvttGD16tVdu9Fx5Np3Mwvmz5/f/jv19fXB97///aBPnz5BdnZ2cO655wZbt27tuo2Oo+9+97vBsGHDgvT09GDAgAHBaaed1l4gg6B377uyd5HszcfgggsuCAYNGhSkp6cHQ4YMCS644IJg7dq17ct7875/5i9/+Utw2GGHBRkZGcHo0aODe++9t8Py3v4M7A0SpV4HQWLX7ESv10FAzd5bItXrIKBmBwE1uzdIlJqdyPU6CKjZ1Ot9JVLNpl5373qdFARBEO13rgEAAAAAAAAAiaxb/o1oAAAAAAAAAEDvwUQ0AAAAAAAAACBSTEQDAAAAAAAAACLFRDQAAAAAAAAAIFJMRAMAAAAAAAAAIsVENAAAAAAAAAAgUkxEAwAAAAAAAAAixUQ0AAAAAAAAACBSTEQDAAAAAAAAACLFRDQAAAAAAAAAIFJMRAMAAAAAAAAAIsVENAAAAAAAAAAgUv8fiXAgQvoGAewAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1800x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define Training Transforms\n",
    "holes = 10\n",
    "hole_spatial_size = 10\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\"], image_only=True),\n",
    "        EnsureChannelFirstd(keys=[\"image\"]),\n",
    "        # Spacingd(keys=[\"image\"], pixdim=(2.0, 2.0, 2.0), mode=(\"bilinear\")),\n",
    "        # ScaleIntensityRanged(\n",
    "        #     keys=[\"image\"],\n",
    "        #     a_min=-57,\n",
    "        #     a_max=164,\n",
    "        #     b_min=0.0,\n",
    "        #     b_max=1.0,\n",
    "        #     clip=True,\n",
    "        # ),\n",
    "        # CropForegroundd(keys=[\"image\"], source_key=\"image\"),\n",
    "        # SpatialPadd(keys=[\"image\"], spatial_size=(96, 96, 96)),\n",
    "        # RandSpatialCropSamplesd(keys=[\"image\"], roi_size=(96, 96, 96), random_size=False, num_samples=2),\n",
    "        CopyItemsd(keys=[\"image\"], times=2, names=[\"gt_image\", \"image_2\"], allow_missing_keys=False),\n",
    "        RandAffined(\n",
    "            keys=[\"image\", \"image_2\", \"gt_image\"],\n",
    "            prob=0.8,\n",
    "            rotate_range=0.5,\n",
    "            shear_range=0.0,\n",
    "            translate_range=0.0,\n",
    "            scale_range=0.0,\n",
    "            mode=(\"bilinear\", \"bilinear\", \"bilinear\"),\n",
    "            padding_mode=\"zeros\",\n",
    "        ),\n",
    "        RandFlipd(keys=[\"image\", \"image_2\", \"gt_image\"], prob=0.8, spatial_axis=0),\n",
    "        RandFlipd(keys=[\"image\", \"image_2\", \"gt_image\"], prob=0.8, spatial_axis=1),\n",
    "        OneOf(\n",
    "            transforms=[\n",
    "                RandCoarseDropoutd(\n",
    "                    keys=[\"image\"], prob=1.0, holes=holes, spatial_size=hole_spatial_size, dropout_holes=True, fill_value=0\n",
    "                ),\n",
    "                RandCoarseDropoutd(\n",
    "                    keys=[\"image\"], prob=1.0, holes=holes, spatial_size=hole_spatial_size, dropout_holes=False, fill_value=0\n",
    "                ),\n",
    "            ]\n",
    "        ),\n",
    "        RandCoarseShuffled(keys=[\"image\"], prob=0.8, holes=10, spatial_size=8),\n",
    "        # Please note that that if image, image_2 are called via the same transform call because of the determinism\n",
    "        # they will get augmented the exact same way which is not the required case here, hence two calls are made\n",
    "        # OneOf(\n",
    "        #     transforms=[\n",
    "        #         RandCoarseDropoutd(\n",
    "        #             keys=[\"image_2\"], prob=1.0, holes=holes, spatial_size=hole_spatial_size, dropout_holes=True, fill_value=0\n",
    "        #         ),\n",
    "        #         RandCoarseDropoutd(\n",
    "        #             keys=[\"image_2\"], prob=1.0, holes=holes, spatial_size=hole_spatial_size, dropout_holes=False, fill_value=0\n",
    "        #         ),\n",
    "        #     ]\n",
    "        # ),\n",
    "        RandCoarseShuffled(keys=[\"image_2\"], prob=0.8, holes=10, spatial_size=12\n",
    "                           ),\n",
    "        \n",
    "        # Resized(keys=[\"image\", \"image_2\"], spatial_size=(224, 224, 3), mode=(\"bilinear\", \"nearest\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "check_ds = Dataset(data=train_data, transform=train_transforms)\n",
    "check_loader = DataLoader(check_ds, batch_size=1)\n",
    "check_data = first(check_loader)\n",
    "image = check_data[\"image\"][0][0]\n",
    "image2 = check_data[\"image_2\"][0][0]\n",
    "gt_image = check_data[\"gt_image\"][0][0]\n",
    "print(f\"image shape: {image.shape}\")\n",
    "print(f\"image2 shape: {image2.shape}\")\n",
    "print(f\"gt_image shape: {gt_image.shape}\")\n",
    "\n",
    "plt.figure(\"check\", (18, 6))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"image\")\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"image2\")\n",
    "plt.imshow(image2, cmap=\"gray\")\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"gt_image\")\n",
    "plt.imshow(gt_image, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3453c4",
   "metadata": {},
   "source": [
    "##### Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca341d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda:0\n",
      "torch.Size([1, 1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# Define Network ViT backbone & Loss & Optimizer\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device: ', device)\n",
    "model = AutoEncoder(\n",
    "    spatial_dims=2,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    channels=(2, 4, 8),\n",
    "    strides=(2, 2, 2)\n",
    ").to(device)\n",
    "# model = ViTAutoEnc(in_channels=1,\n",
    "#                    img_size=64,\n",
    "#                    patch_size=4,\n",
    "#                    out_channels=1,\n",
    "#                    spatial_dims=2).to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(torch.rand(1, 1, 64, 64).to(device))\n",
    "    print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb5728e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Config\n",
    "# Define Hyper-paramters for training loop\n",
    "max_epochs = 4\n",
    "val_interval = 1\n",
    "batch_size = 128\n",
    "lr = 1e-4\n",
    "\n",
    "epoch_loss_values = []\n",
    "step_loss_values = []\n",
    "epoch_cl_loss_values = []\n",
    "epoch_recon_loss_values = []\n",
    "val_loss_values = []\n",
    "best_val_loss = 1000.0\n",
    "\n",
    "recon_loss = L1Loss()\n",
    "contrastive_loss = ContrastiveLoss(temperature=0.05)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "# Define DataLoader using MONAI, CacheDataset needs to be used\n",
    "train_ds = Dataset(data=train_data, transform=train_transforms)\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "val_ds = Dataset(data=val_data, transform=train_transforms)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b1912d",
   "metadata": {},
   "source": [
    "##### Training loop with validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d71ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 1/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/589, train_loss: 289.6571, time taken: 0.026793718338012695s\n",
      "2/589, train_loss: 301.6974, time taken: 0.02266216278076172s\n",
      "3/589, train_loss: 311.2759, time taken: 0.02259659767150879s\n",
      "4/589, train_loss: 206.9546, time taken: 0.02249431610107422s\n",
      "5/589, train_loss: 235.5140, time taken: 0.02257370948791504s\n",
      "6/589, train_loss: 242.9349, time taken: 0.0208284854888916s\n",
      "7/589, train_loss: 243.8040, time taken: 0.022586345672607422s\n",
      "8/589, train_loss: 233.7434, time taken: 0.020091533660888672s\n",
      "9/589, train_loss: 263.6326, time taken: 0.022890090942382812s\n",
      "10/589, train_loss: 246.9023, time taken: 0.022947311401367188s\n",
      "11/589, train_loss: 248.4586, time taken: 0.022653818130493164s\n",
      "12/589, train_loss: 202.3685, time taken: 0.02242898941040039s\n",
      "13/589, train_loss: 244.6330, time taken: 0.02279067039489746s\n",
      "14/589, train_loss: 248.4532, time taken: 0.020353317260742188s\n",
      "15/589, train_loss: 260.2238, time taken: 0.02013683319091797s\n",
      "16/589, train_loss: 250.7183, time taken: 0.02249741554260254s\n",
      "17/589, train_loss: 287.6568, time taken: 0.02298736572265625s\n",
      "18/589, train_loss: 312.5786, time taken: 0.022771835327148438s\n",
      "19/589, train_loss: 225.7985, time taken: 0.022689104080200195s\n",
      "20/589, train_loss: 248.0652, time taken: 0.02285623550415039s\n",
      "21/589, train_loss: 234.9020, time taken: 0.02052450180053711s\n",
      "22/589, train_loss: 254.4614, time taken: 0.020061969757080078s\n",
      "23/589, train_loss: 238.0702, time taken: 0.020599365234375s\n",
      "24/589, train_loss: 325.0435, time taken: 0.02013707160949707s\n",
      "25/589, train_loss: 232.0760, time taken: 0.022900819778442383s\n",
      "26/589, train_loss: 271.6125, time taken: 0.02292609214782715s\n",
      "27/589, train_loss: 252.5836, time taken: 0.02010345458984375s\n",
      "28/589, train_loss: 165.6107, time taken: 0.020000219345092773s\n",
      "29/589, train_loss: 307.2651, time taken: 0.02033209800720215s\n",
      "30/589, train_loss: 245.7480, time taken: 0.02045416831970215s\n",
      "31/589, train_loss: 194.5885, time taken: 0.022882461547851562s\n",
      "32/589, train_loss: 244.1884, time taken: 0.022943496704101562s\n",
      "33/589, train_loss: 280.6544, time taken: 0.02290034294128418s\n",
      "34/589, train_loss: 220.6672, time taken: 0.022922277450561523s\n",
      "35/589, train_loss: 184.8506, time taken: 0.023035287857055664s\n",
      "36/589, train_loss: 289.6201, time taken: 0.02279496192932129s\n",
      "37/589, train_loss: 246.3721, time taken: 0.02043461799621582s\n",
      "38/589, train_loss: 283.1166, time taken: 0.022081851959228516s\n",
      "39/589, train_loss: 262.1840, time taken: 0.022769689559936523s\n",
      "40/589, train_loss: 237.9969, time taken: 0.022664546966552734s\n",
      "41/589, train_loss: 211.5516, time taken: 0.022519350051879883s\n",
      "42/589, train_loss: 242.5858, time taken: 0.02327132225036621s\n",
      "43/589, train_loss: 227.2792, time taken: 0.0231168270111084s\n",
      "44/589, train_loss: 189.0089, time taken: 0.02274346351623535s\n",
      "45/589, train_loss: 225.3674, time taken: 0.021886587142944336s\n",
      "46/589, train_loss: 289.6442, time taken: 0.023113489151000977s\n",
      "47/589, train_loss: 225.4117, time taken: 0.02289104461669922s\n",
      "48/589, train_loss: 260.4747, time taken: 0.022882938385009766s\n",
      "49/589, train_loss: 146.2157, time taken: 0.02301955223083496s\n",
      "50/589, train_loss: 243.5398, time taken: 0.022984027862548828s\n",
      "51/589, train_loss: 237.1156, time taken: 0.02285933494567871s\n",
      "52/589, train_loss: 241.6221, time taken: 0.022562742233276367s\n",
      "53/589, train_loss: 239.2623, time taken: 0.021434307098388672s\n",
      "54/589, train_loss: 287.2388, time taken: 0.021142244338989258s\n",
      "55/589, train_loss: 250.9322, time taken: 0.020371675491333008s\n",
      "56/589, train_loss: 275.9288, time taken: 0.021271944046020508s\n",
      "57/589, train_loss: 271.6108, time taken: 0.021697282791137695s\n",
      "58/589, train_loss: 238.3643, time taken: 0.020653486251831055s\n",
      "59/589, train_loss: 276.2728, time taken: 0.020926952362060547s\n",
      "60/589, train_loss: 245.1856, time taken: 0.02335524559020996s\n",
      "61/589, train_loss: 219.3434, time taken: 0.02057027816772461s\n",
      "62/589, train_loss: 225.5695, time taken: 0.023180484771728516s\n",
      "63/589, train_loss: 295.1033, time taken: 0.020627975463867188s\n",
      "64/589, train_loss: 334.7323, time taken: 0.02153921127319336s\n",
      "65/589, train_loss: 261.8345, time taken: 0.021422863006591797s\n",
      "66/589, train_loss: 257.3743, time taken: 0.02157449722290039s\n",
      "67/589, train_loss: 238.5831, time taken: 0.021751880645751953s\n",
      "68/589, train_loss: 273.4984, time taken: 0.019840002059936523s\n",
      "69/589, train_loss: 275.5068, time taken: 0.02212071418762207s\n",
      "70/589, train_loss: 223.7970, time taken: 0.020904064178466797s\n",
      "71/589, train_loss: 261.0002, time taken: 0.020618915557861328s\n",
      "72/589, train_loss: 228.5756, time taken: 0.02034473419189453s\n",
      "73/589, train_loss: 212.5737, time taken: 0.021327972412109375s\n",
      "74/589, train_loss: 272.8525, time taken: 0.02162337303161621s\n",
      "75/589, train_loss: 248.5267, time taken: 0.02116870880126953s\n",
      "76/589, train_loss: 253.4119, time taken: 0.023125648498535156s\n",
      "77/589, train_loss: 188.3405, time taken: 0.02330493927001953s\n",
      "78/589, train_loss: 253.2229, time taken: 0.02219557762145996s\n",
      "79/589, train_loss: 213.3547, time taken: 0.022688865661621094s\n",
      "80/589, train_loss: 224.4018, time taken: 0.022214174270629883s\n",
      "81/589, train_loss: 310.2059, time taken: 0.023087024688720703s\n",
      "82/589, train_loss: 206.4330, time taken: 0.022490739822387695s\n",
      "83/589, train_loss: 243.0664, time taken: 0.02291083335876465s\n",
      "84/589, train_loss: 240.5702, time taken: 0.021569490432739258s\n",
      "85/589, train_loss: 255.2606, time taken: 0.02319622039794922s\n",
      "86/589, train_loss: 241.0107, time taken: 0.023157358169555664s\n",
      "87/589, train_loss: 278.5714, time taken: 0.022580623626708984s\n",
      "88/589, train_loss: 237.5542, time taken: 0.02286982536315918s\n",
      "89/589, train_loss: 272.7130, time taken: 0.0203249454498291s\n",
      "90/589, train_loss: 195.3666, time taken: 0.022392749786376953s\n",
      "91/589, train_loss: 344.9689, time taken: 0.023106098175048828s\n",
      "92/589, train_loss: 220.4136, time taken: 0.023101091384887695s\n",
      "93/589, train_loss: 253.4280, time taken: 0.02227306365966797s\n",
      "94/589, train_loss: 238.1573, time taken: 0.020063161849975586s\n",
      "95/589, train_loss: 236.6070, time taken: 0.021615266799926758s\n",
      "96/589, train_loss: 268.4066, time taken: 0.022583961486816406s\n",
      "97/589, train_loss: 253.3238, time taken: 0.02128744125366211s\n",
      "98/589, train_loss: 224.2236, time taken: 0.020425796508789062s\n",
      "99/589, train_loss: 263.2007, time taken: 0.022031307220458984s\n",
      "100/589, train_loss: 166.8197, time taken: 0.02088642120361328s\n",
      "101/589, train_loss: 245.7324, time taken: 0.021476030349731445s\n",
      "102/589, train_loss: 224.3343, time taken: 0.020094871520996094s\n",
      "103/589, train_loss: 300.7760, time taken: 0.020125627517700195s\n",
      "104/589, train_loss: 248.5563, time taken: 0.023355960845947266s\n",
      "105/589, train_loss: 246.4264, time taken: 0.02039957046508789s\n",
      "106/589, train_loss: 249.5190, time taken: 0.022360563278198242s\n",
      "107/589, train_loss: 223.5099, time taken: 0.020185470581054688s\n",
      "108/589, train_loss: 229.8268, time taken: 0.022571802139282227s\n",
      "109/589, train_loss: 172.7251, time taken: 0.0212705135345459s\n",
      "110/589, train_loss: 365.1783, time taken: 0.022916793823242188s\n",
      "111/589, train_loss: 247.5982, time taken: 0.02252507209777832s\n",
      "112/589, train_loss: 306.0664, time taken: 0.023010730743408203s\n",
      "113/589, train_loss: 317.5117, time taken: 0.02271890640258789s\n",
      "114/589, train_loss: 262.1839, time taken: 0.022124528884887695s\n",
      "115/589, train_loss: 209.6168, time taken: 0.020521879196166992s\n",
      "116/589, train_loss: 279.0348, time taken: 0.02033829689025879s\n",
      "117/589, train_loss: 209.7347, time taken: 0.021653413772583008s\n",
      "118/589, train_loss: 196.0685, time taken: 0.02069568634033203s\n",
      "119/589, train_loss: 245.8654, time taken: 0.022433996200561523s\n",
      "120/589, train_loss: 271.2796, time taken: 0.022597312927246094s\n",
      "121/589, train_loss: 229.2673, time taken: 0.020273447036743164s\n",
      "122/589, train_loss: 202.8066, time taken: 0.021020174026489258s\n",
      "123/589, train_loss: 324.0963, time taken: 0.022367477416992188s\n",
      "124/589, train_loss: 294.4934, time taken: 0.02266693115234375s\n",
      "125/589, train_loss: 216.0592, time taken: 0.020392417907714844s\n",
      "126/589, train_loss: 218.5304, time taken: 0.020090103149414062s\n",
      "127/589, train_loss: 255.3900, time taken: 0.02275848388671875s\n",
      "128/589, train_loss: 226.0319, time taken: 0.021445274353027344s\n",
      "129/589, train_loss: 310.0580, time taken: 0.023172855377197266s\n",
      "130/589, train_loss: 180.0012, time taken: 0.02300405502319336s\n",
      "131/589, train_loss: 290.9999, time taken: 0.020512104034423828s\n",
      "132/589, train_loss: 247.1781, time taken: 0.02278900146484375s\n",
      "133/589, train_loss: 233.8258, time taken: 0.024984359741210938s\n",
      "134/589, train_loss: 245.9277, time taken: 0.023332595825195312s\n",
      "135/589, train_loss: 241.4993, time taken: 0.02299356460571289s\n",
      "136/589, train_loss: 239.3112, time taken: 0.020277976989746094s\n",
      "137/589, train_loss: 254.9445, time taken: 0.021655797958374023s\n",
      "138/589, train_loss: 228.5701, time taken: 0.019956111907958984s\n",
      "139/589, train_loss: 294.3849, time taken: 0.020857810974121094s\n",
      "140/589, train_loss: 272.3951, time taken: 0.020544052124023438s\n",
      "141/589, train_loss: 230.3051, time taken: 0.021114826202392578s\n",
      "142/589, train_loss: 280.5718, time taken: 0.020218610763549805s\n",
      "143/589, train_loss: 236.3331, time taken: 0.021262168884277344s\n",
      "144/589, train_loss: 264.0036, time taken: 0.022634506225585938s\n",
      "145/589, train_loss: 227.0112, time taken: 0.021862268447875977s\n",
      "146/589, train_loss: 270.1086, time taken: 0.021868467330932617s\n",
      "147/589, train_loss: 212.5311, time taken: 0.021010875701904297s\n",
      "148/589, train_loss: 222.8117, time taken: 0.021316051483154297s\n",
      "149/589, train_loss: 211.9113, time taken: 0.02235579490661621s\n",
      "150/589, train_loss: 186.0002, time taken: 0.023268938064575195s\n",
      "151/589, train_loss: 225.7401, time taken: 0.023149728775024414s\n",
      "152/589, train_loss: 271.8679, time taken: 0.023090124130249023s\n",
      "153/589, train_loss: 233.8305, time taken: 0.022598743438720703s\n",
      "154/589, train_loss: 195.4006, time taken: 0.023386240005493164s\n",
      "155/589, train_loss: 221.3817, time taken: 0.0213010311126709s\n",
      "156/589, train_loss: 254.0128, time taken: 0.021915674209594727s\n",
      "157/589, train_loss: 305.9037, time taken: 0.02285170555114746s\n",
      "158/589, train_loss: 263.6151, time taken: 0.02142643928527832s\n",
      "159/589, train_loss: 202.3259, time taken: 0.020886659622192383s\n",
      "160/589, train_loss: 379.6105, time taken: 0.020505666732788086s\n",
      "161/589, train_loss: 242.2672, time taken: 0.02165055274963379s\n",
      "162/589, train_loss: 243.7032, time taken: 0.02195143699645996s\n",
      "163/589, train_loss: 244.5036, time taken: 0.022853851318359375s\n",
      "164/589, train_loss: 279.9833, time taken: 0.023638010025024414s\n",
      "165/589, train_loss: 181.1205, time taken: 0.023550987243652344s\n",
      "166/589, train_loss: 279.3633, time taken: 0.023180723190307617s\n",
      "167/589, train_loss: 259.3976, time taken: 0.02044200897216797s\n",
      "168/589, train_loss: 234.8759, time taken: 0.02341747283935547s\n",
      "169/589, train_loss: 200.5067, time taken: 0.020025968551635742s\n",
      "170/589, train_loss: 270.9249, time taken: 0.02213311195373535s\n",
      "171/589, train_loss: 226.1806, time taken: 0.02018284797668457s\n",
      "172/589, train_loss: 216.0092, time taken: 0.021250009536743164s\n",
      "173/589, train_loss: 251.4937, time taken: 0.022750377655029297s\n",
      "174/589, train_loss: 301.6483, time taken: 0.02299332618713379s\n",
      "175/589, train_loss: 202.1130, time taken: 0.022755861282348633s\n",
      "176/589, train_loss: 304.3495, time taken: 0.023253202438354492s\n",
      "177/589, train_loss: 249.1272, time taken: 0.022841453552246094s\n",
      "178/589, train_loss: 184.7065, time taken: 0.02261829376220703s\n",
      "179/589, train_loss: 271.6757, time taken: 0.023008346557617188s\n",
      "180/589, train_loss: 290.1640, time taken: 0.02294182777404785s\n",
      "181/589, train_loss: 253.5045, time taken: 0.02069878578186035s\n",
      "182/589, train_loss: 297.5711, time taken: 0.02031993865966797s\n",
      "183/589, train_loss: 255.6395, time taken: 0.023314952850341797s\n",
      "184/589, train_loss: 228.9267, time taken: 0.0214691162109375s\n",
      "185/589, train_loss: 337.2477, time taken: 0.022200345993041992s\n",
      "186/589, train_loss: 278.4647, time taken: 0.021845102310180664s\n",
      "187/589, train_loss: 218.8330, time taken: 0.02174997329711914s\n",
      "188/589, train_loss: 263.0602, time taken: 0.022419452667236328s\n",
      "189/589, train_loss: 249.0829, time taken: 0.020567655563354492s\n",
      "190/589, train_loss: 294.1043, time taken: 0.02249431610107422s\n",
      "191/589, train_loss: 247.8653, time taken: 0.020785093307495117s\n",
      "192/589, train_loss: 238.2698, time taken: 0.020381689071655273s\n",
      "193/589, train_loss: 217.0652, time taken: 0.020348310470581055s\n",
      "194/589, train_loss: 174.5566, time taken: 0.02268052101135254s\n",
      "195/589, train_loss: 275.8497, time taken: 0.02269458770751953s\n",
      "196/589, train_loss: 268.1862, time taken: 0.020391464233398438s\n",
      "197/589, train_loss: 375.1775, time taken: 0.020235538482666016s\n",
      "198/589, train_loss: 218.7660, time taken: 0.023146867752075195s\n",
      "199/589, train_loss: 239.3378, time taken: 0.021335363388061523s\n",
      "200/589, train_loss: 256.6243, time taken: 0.020371198654174805s\n",
      "201/589, train_loss: 203.8926, time taken: 0.020177364349365234s\n",
      "202/589, train_loss: 380.4948, time taken: 0.0203397274017334s\n",
      "203/589, train_loss: 156.2834, time taken: 0.022347211837768555s\n",
      "204/589, train_loss: 241.9539, time taken: 0.02024984359741211s\n",
      "205/589, train_loss: 223.0083, time taken: 0.019742488861083984s\n",
      "206/589, train_loss: 220.2205, time taken: 0.02134990692138672s\n",
      "207/589, train_loss: 221.6205, time taken: 0.02141261100769043s\n",
      "208/589, train_loss: 262.4212, time taken: 0.02099323272705078s\n",
      "209/589, train_loss: 253.0243, time taken: 0.02027583122253418s\n",
      "210/589, train_loss: 259.6947, time taken: 0.022827625274658203s\n",
      "211/589, train_loss: 307.6125, time taken: 0.020453453063964844s\n",
      "212/589, train_loss: 179.1339, time taken: 0.022726774215698242s\n",
      "213/589, train_loss: 318.9208, time taken: 0.021185636520385742s\n",
      "214/589, train_loss: 301.0000, time taken: 0.0227963924407959s\n",
      "215/589, train_loss: 217.3928, time taken: 0.022739410400390625s\n",
      "216/589, train_loss: 324.4546, time taken: 0.02324819564819336s\n",
      "217/589, train_loss: 197.2446, time taken: 0.02345442771911621s\n",
      "218/589, train_loss: 244.4789, time taken: 0.02194380760192871s\n",
      "219/589, train_loss: 293.6996, time taken: 0.021490812301635742s\n",
      "220/589, train_loss: 256.3490, time taken: 0.020467758178710938s\n",
      "221/589, train_loss: 206.1763, time taken: 0.023276567459106445s\n",
      "222/589, train_loss: 217.7103, time taken: 0.022199392318725586s\n",
      "223/589, train_loss: 343.9419, time taken: 0.022650480270385742s\n",
      "224/589, train_loss: 266.2518, time taken: 0.020398616790771484s\n",
      "225/589, train_loss: 266.3116, time taken: 0.023429155349731445s\n",
      "226/589, train_loss: 267.9268, time taken: 0.02155447006225586s\n",
      "227/589, train_loss: 210.7992, time taken: 0.021836519241333008s\n",
      "228/589, train_loss: 202.2690, time taken: 0.021539688110351562s\n",
      "229/589, train_loss: 246.5759, time taken: 0.022636890411376953s\n",
      "230/589, train_loss: 263.2867, time taken: 0.020297765731811523s\n",
      "231/589, train_loss: 228.6673, time taken: 0.021601438522338867s\n",
      "232/589, train_loss: 203.8999, time taken: 0.022737503051757812s\n",
      "233/589, train_loss: 187.8315, time taken: 0.022755146026611328s\n",
      "234/589, train_loss: 233.6862, time taken: 0.020604610443115234s\n",
      "235/589, train_loss: 237.5172, time taken: 0.021892786026000977s\n",
      "236/589, train_loss: 257.2461, time taken: 0.02319622039794922s\n",
      "237/589, train_loss: 226.1991, time taken: 0.021914005279541016s\n",
      "238/589, train_loss: 233.7027, time taken: 0.022166013717651367s\n",
      "239/589, train_loss: 276.6179, time taken: 0.020478248596191406s\n",
      "240/589, train_loss: 276.7517, time taken: 0.022003173828125s\n",
      "241/589, train_loss: 273.1047, time taken: 0.022357940673828125s\n",
      "242/589, train_loss: 257.3603, time taken: 0.023116588592529297s\n",
      "243/589, train_loss: 268.0234, time taken: 0.02017831802368164s\n",
      "244/589, train_loss: 307.5463, time taken: 0.021974563598632812s\n",
      "245/589, train_loss: 246.1396, time taken: 0.022829771041870117s\n",
      "246/589, train_loss: 323.3022, time taken: 0.02280735969543457s\n",
      "247/589, train_loss: 245.9789, time taken: 0.022052764892578125s\n",
      "248/589, train_loss: 228.7060, time taken: 0.02181553840637207s\n",
      "249/589, train_loss: 218.1304, time taken: 0.023130178451538086s\n",
      "250/589, train_loss: 217.1176, time taken: 0.020891189575195312s\n",
      "251/589, train_loss: 269.0781, time taken: 0.020552396774291992s\n",
      "252/589, train_loss: 260.3115, time taken: 0.022680044174194336s\n",
      "253/589, train_loss: 195.3957, time taken: 0.020858049392700195s\n",
      "254/589, train_loss: 303.4613, time taken: 0.022663354873657227s\n",
      "255/589, train_loss: 187.5951, time taken: 0.020245075225830078s\n",
      "256/589, train_loss: 199.3404, time taken: 0.020661354064941406s\n",
      "257/589, train_loss: 257.8972, time taken: 0.022983551025390625s\n",
      "258/589, train_loss: 219.4153, time taken: 0.020196199417114258s\n",
      "259/589, train_loss: 268.7223, time taken: 0.022114276885986328s\n",
      "260/589, train_loss: 265.0571, time taken: 0.02002716064453125s\n",
      "261/589, train_loss: 199.4152, time taken: 0.020872116088867188s\n",
      "262/589, train_loss: 197.5016, time taken: 0.02226114273071289s\n",
      "263/589, train_loss: 295.3920, time taken: 0.023955106735229492s\n",
      "264/589, train_loss: 200.4155, time taken: 0.02094101905822754s\n",
      "265/589, train_loss: 284.1265, time taken: 0.02117609977722168s\n",
      "266/589, train_loss: 194.1650, time taken: 0.023053407669067383s\n",
      "267/589, train_loss: 292.6729, time taken: 0.02170109748840332s\n",
      "268/589, train_loss: 225.5800, time taken: 0.022382497787475586s\n",
      "269/589, train_loss: 165.8136, time taken: 0.022148609161376953s\n",
      "270/589, train_loss: 234.2099, time taken: 0.0222933292388916s\n",
      "271/589, train_loss: 253.3665, time taken: 0.0204007625579834s\n",
      "272/589, train_loss: 195.8078, time taken: 0.019894838333129883s\n",
      "273/589, train_loss: 289.0011, time taken: 0.02193164825439453s\n",
      "274/589, train_loss: 290.8595, time taken: 0.02288055419921875s\n",
      "275/589, train_loss: 243.1174, time taken: 0.022881507873535156s\n",
      "276/589, train_loss: 204.3169, time taken: 0.022175312042236328s\n",
      "277/589, train_loss: 263.3653, time taken: 0.022196531295776367s\n",
      "278/589, train_loss: 241.8815, time taken: 0.023004770278930664s\n",
      "279/589, train_loss: 313.4615, time taken: 0.02186727523803711s\n",
      "280/589, train_loss: 314.3311, time taken: 0.0220029354095459s\n",
      "281/589, train_loss: 228.5261, time taken: 0.022179841995239258s\n",
      "282/589, train_loss: 226.5351, time taken: 0.020614147186279297s\n",
      "283/589, train_loss: 236.1892, time taken: 0.022571086883544922s\n",
      "284/589, train_loss: 281.6824, time taken: 0.022858381271362305s\n",
      "285/589, train_loss: 257.7932, time taken: 0.0227508544921875s\n",
      "286/589, train_loss: 247.8708, time taken: 0.021013259887695312s\n",
      "287/589, train_loss: 234.2740, time taken: 0.020375728607177734s\n",
      "288/589, train_loss: 263.2071, time taken: 0.02159571647644043s\n",
      "289/589, train_loss: 247.0176, time taken: 0.022889375686645508s\n",
      "290/589, train_loss: 276.1395, time taken: 0.020585298538208008s\n",
      "291/589, train_loss: 283.2865, time taken: 0.020304203033447266s\n",
      "292/589, train_loss: 219.3556, time taken: 0.022481918334960938s\n",
      "293/589, train_loss: 346.9088, time taken: 0.02332019805908203s\n",
      "294/589, train_loss: 222.0711, time taken: 0.02020430564880371s\n",
      "295/589, train_loss: 243.0091, time taken: 0.02013421058654785s\n",
      "296/589, train_loss: 243.9236, time taken: 0.020014524459838867s\n",
      "297/589, train_loss: 203.7216, time taken: 0.02219676971435547s\n",
      "298/589, train_loss: 284.2778, time taken: 0.021869182586669922s\n",
      "299/589, train_loss: 226.2259, time taken: 0.020231246948242188s\n",
      "300/589, train_loss: 333.7639, time taken: 0.023120403289794922s\n",
      "301/589, train_loss: 223.2333, time taken: 0.022282838821411133s\n",
      "302/589, train_loss: 195.6256, time taken: 0.023183107376098633s\n",
      "303/589, train_loss: 222.0443, time taken: 0.023006200790405273s\n",
      "304/589, train_loss: 250.2863, time taken: 0.022301435470581055s\n",
      "305/589, train_loss: 236.9869, time taken: 0.021976709365844727s\n",
      "306/589, train_loss: 196.2329, time taken: 0.020368576049804688s\n",
      "307/589, train_loss: 281.1292, time taken: 0.020690202713012695s\n",
      "308/589, train_loss: 193.9850, time taken: 0.020537853240966797s\n",
      "309/589, train_loss: 257.0923, time taken: 0.020032882690429688s\n",
      "310/589, train_loss: 199.8440, time taken: 0.0201873779296875s\n",
      "311/589, train_loss: 258.6931, time taken: 0.021109342575073242s\n",
      "312/589, train_loss: 258.7386, time taken: 0.020192384719848633s\n",
      "313/589, train_loss: 236.6963, time taken: 0.020430564880371094s\n",
      "314/589, train_loss: 263.5472, time taken: 0.022655010223388672s\n",
      "315/589, train_loss: 206.9643, time taken: 0.02209925651550293s\n",
      "316/589, train_loss: 175.0630, time taken: 0.02310919761657715s\n",
      "317/589, train_loss: 315.4676, time taken: 0.022336959838867188s\n",
      "318/589, train_loss: 267.5067, time taken: 0.022215843200683594s\n",
      "319/589, train_loss: 239.5714, time taken: 0.023044824600219727s\n",
      "320/589, train_loss: 290.1627, time taken: 0.022373676300048828s\n",
      "321/589, train_loss: 162.5510, time taken: 0.020051956176757812s\n",
      "322/589, train_loss: 229.4041, time taken: 0.023308992385864258s\n",
      "323/589, train_loss: 286.9304, time taken: 0.022463560104370117s\n",
      "324/589, train_loss: 213.1471, time taken: 0.02212214469909668s\n",
      "325/589, train_loss: 347.5028, time taken: 0.023275375366210938s\n",
      "326/589, train_loss: 301.8814, time taken: 0.021984100341796875s\n",
      "327/589, train_loss: 238.6598, time taken: 0.02301788330078125s\n",
      "328/589, train_loss: 260.1753, time taken: 0.023196697235107422s\n",
      "329/589, train_loss: 318.4376, time taken: 0.022342205047607422s\n",
      "330/589, train_loss: 258.3778, time taken: 0.02279376983642578s\n",
      "331/589, train_loss: 217.2902, time taken: 0.023858308792114258s\n",
      "332/589, train_loss: 246.9905, time taken: 0.020445823669433594s\n",
      "333/589, train_loss: 229.7038, time taken: 0.020220518112182617s\n",
      "334/589, train_loss: 217.6190, time taken: 0.020383119583129883s\n",
      "335/589, train_loss: 275.5638, time taken: 0.02271413803100586s\n",
      "336/589, train_loss: 324.4702, time taken: 0.023122787475585938s\n",
      "337/589, train_loss: 245.0899, time taken: 0.02287912368774414s\n",
      "338/589, train_loss: 240.9397, time taken: 0.02309274673461914s\n",
      "339/589, train_loss: 210.3538, time taken: 0.022095918655395508s\n",
      "340/589, train_loss: 216.1123, time taken: 0.020641088485717773s\n",
      "341/589, train_loss: 220.4167, time taken: 0.0200808048248291s\n",
      "342/589, train_loss: 286.2335, time taken: 0.023145437240600586s\n",
      "343/589, train_loss: 255.3936, time taken: 0.021134138107299805s\n",
      "344/589, train_loss: 318.2268, time taken: 0.0228269100189209s\n",
      "345/589, train_loss: 256.9227, time taken: 0.0203094482421875s\n",
      "346/589, train_loss: 169.4639, time taken: 0.02186131477355957s\n",
      "347/589, train_loss: 258.5586, time taken: 0.02035212516784668s\n",
      "348/589, train_loss: 240.6829, time taken: 0.02257084846496582s\n",
      "349/589, train_loss: 292.2000, time taken: 0.022771835327148438s\n",
      "350/589, train_loss: 235.5578, time taken: 0.022639989852905273s\n",
      "351/589, train_loss: 192.2568, time taken: 0.02285599708557129s\n",
      "352/589, train_loss: 168.2175, time taken: 0.02286672592163086s\n",
      "353/589, train_loss: 274.4379, time taken: 0.02257227897644043s\n",
      "354/589, train_loss: 243.4409, time taken: 0.022834062576293945s\n",
      "355/589, train_loss: 211.7252, time taken: 0.022263765335083008s\n",
      "356/589, train_loss: 250.7400, time taken: 0.022124528884887695s\n",
      "357/589, train_loss: 214.5657, time taken: 0.022814512252807617s\n",
      "358/589, train_loss: 278.4166, time taken: 0.022437334060668945s\n",
      "359/589, train_loss: 263.8866, time taken: 0.022690296173095703s\n",
      "360/589, train_loss: 297.7979, time taken: 0.022918701171875s\n",
      "361/589, train_loss: 202.1799, time taken: 0.02263665199279785s\n",
      "362/589, train_loss: 243.7293, time taken: 0.02256298065185547s\n",
      "363/589, train_loss: 207.9792, time taken: 0.022373199462890625s\n",
      "364/589, train_loss: 200.2957, time taken: 0.02267003059387207s\n",
      "365/589, train_loss: 232.2652, time taken: 0.020280838012695312s\n",
      "366/589, train_loss: 227.4289, time taken: 0.020061969757080078s\n",
      "367/589, train_loss: 291.2902, time taken: 0.02005314826965332s\n",
      "368/589, train_loss: 357.6158, time taken: 0.022676706314086914s\n",
      "369/589, train_loss: 187.3372, time taken: 0.022868871688842773s\n",
      "370/589, train_loss: 214.0756, time taken: 0.02291584014892578s\n",
      "371/589, train_loss: 282.6201, time taken: 0.022958040237426758s\n",
      "372/589, train_loss: 217.4494, time taken: 0.02272510528564453s\n",
      "373/589, train_loss: 262.7037, time taken: 0.020378589630126953s\n",
      "374/589, train_loss: 258.2210, time taken: 0.022768020629882812s\n",
      "375/589, train_loss: 230.7110, time taken: 0.0229794979095459s\n",
      "376/589, train_loss: 265.8000, time taken: 0.022464752197265625s\n",
      "377/589, train_loss: 225.2489, time taken: 0.02288651466369629s\n",
      "378/589, train_loss: 212.7836, time taken: 0.023173809051513672s\n",
      "379/589, train_loss: 273.6006, time taken: 0.020800352096557617s\n",
      "380/589, train_loss: 311.5305, time taken: 0.020288705825805664s\n",
      "381/589, train_loss: 177.0920, time taken: 0.020257234573364258s\n",
      "382/589, train_loss: 366.5520, time taken: 0.020749330520629883s\n",
      "383/589, train_loss: 185.4211, time taken: 0.020467042922973633s\n",
      "384/589, train_loss: 300.1490, time taken: 0.02318263053894043s\n",
      "385/589, train_loss: 215.4067, time taken: 0.022495031356811523s\n",
      "386/589, train_loss: 231.1197, time taken: 0.022715330123901367s\n",
      "387/589, train_loss: 245.0178, time taken: 0.022786378860473633s\n",
      "388/589, train_loss: 275.2009, time taken: 0.022882699966430664s\n",
      "389/589, train_loss: 253.4955, time taken: 0.022881269454956055s\n",
      "390/589, train_loss: 256.6711, time taken: 0.022806882858276367s\n",
      "391/589, train_loss: 223.7527, time taken: 0.02295207977294922s\n",
      "392/589, train_loss: 315.0466, time taken: 0.02246689796447754s\n",
      "393/589, train_loss: 207.4513, time taken: 0.02236175537109375s\n",
      "394/589, train_loss: 211.3748, time taken: 0.0225980281829834s\n",
      "395/589, train_loss: 284.5845, time taken: 0.022587060928344727s\n",
      "396/589, train_loss: 252.8887, time taken: 0.020783424377441406s\n",
      "397/589, train_loss: 257.1399, time taken: 0.02027416229248047s\n",
      "398/589, train_loss: 229.8940, time taken: 0.02274942398071289s\n",
      "399/589, train_loss: 240.8532, time taken: 0.020513057708740234s\n",
      "400/589, train_loss: 190.2330, time taken: 0.02249741554260254s\n",
      "401/589, train_loss: 292.4948, time taken: 0.022414207458496094s\n",
      "402/589, train_loss: 285.7286, time taken: 0.02031111717224121s\n",
      "403/589, train_loss: 290.6613, time taken: 0.0203855037689209s\n",
      "404/589, train_loss: 193.1904, time taken: 0.02229619026184082s\n",
      "405/589, train_loss: 252.7102, time taken: 0.019942045211791992s\n",
      "406/589, train_loss: 222.4072, time taken: 0.019980430603027344s\n",
      "407/589, train_loss: 218.1108, time taken: 0.020246028900146484s\n",
      "408/589, train_loss: 272.6379, time taken: 0.020318031311035156s\n",
      "409/589, train_loss: 249.2430, time taken: 0.020888090133666992s\n",
      "410/589, train_loss: 229.4744, time taken: 0.022620439529418945s\n",
      "411/589, train_loss: 267.0133, time taken: 0.022546052932739258s\n",
      "412/589, train_loss: 189.4899, time taken: 0.02301764488220215s\n",
      "413/589, train_loss: 299.0170, time taken: 0.022841453552246094s\n",
      "414/589, train_loss: 202.7628, time taken: 0.023032665252685547s\n",
      "415/589, train_loss: 188.9857, time taken: 0.023195743560791016s\n",
      "416/589, train_loss: 234.3715, time taken: 0.023040771484375s\n",
      "417/589, train_loss: 224.8333, time taken: 0.020554304122924805s\n",
      "418/589, train_loss: 250.5965, time taken: 0.022785425186157227s\n",
      "419/589, train_loss: 289.0706, time taken: 0.023104190826416016s\n",
      "420/589, train_loss: 258.9454, time taken: 0.022769451141357422s\n",
      "421/589, train_loss: 257.3344, time taken: 0.02017664909362793s\n",
      "422/589, train_loss: 253.9451, time taken: 0.022802114486694336s\n",
      "423/589, train_loss: 176.4695, time taken: 0.020269393920898438s\n",
      "424/589, train_loss: 234.0273, time taken: 0.022634267807006836s\n",
      "425/589, train_loss: 245.0892, time taken: 0.022312641143798828s\n",
      "426/589, train_loss: 334.4466, time taken: 0.020097970962524414s\n",
      "427/589, train_loss: 243.1163, time taken: 0.020158767700195312s\n",
      "428/589, train_loss: 263.1072, time taken: 0.02029728889465332s\n",
      "429/589, train_loss: 239.6769, time taken: 0.02107858657836914s\n",
      "430/589, train_loss: 259.7935, time taken: 0.02006077766418457s\n",
      "431/589, train_loss: 214.4663, time taken: 0.02269124984741211s\n",
      "432/589, train_loss: 239.3628, time taken: 0.02036285400390625s\n",
      "433/589, train_loss: 217.8940, time taken: 0.022777318954467773s\n",
      "434/589, train_loss: 246.3920, time taken: 0.02048015594482422s\n",
      "435/589, train_loss: 188.4769, time taken: 0.02026987075805664s\n",
      "436/589, train_loss: 264.1973, time taken: 0.02318120002746582s\n",
      "437/589, train_loss: 244.3991, time taken: 0.02238750457763672s\n",
      "438/589, train_loss: 197.8260, time taken: 0.020354032516479492s\n",
      "439/589, train_loss: 218.2070, time taken: 0.0206298828125s\n",
      "440/589, train_loss: 274.0984, time taken: 0.020344018936157227s\n",
      "441/589, train_loss: 226.5868, time taken: 0.02186131477355957s\n",
      "442/589, train_loss: 210.9726, time taken: 0.020554304122924805s\n",
      "443/589, train_loss: 244.5818, time taken: 0.02044224739074707s\n",
      "444/589, train_loss: 260.3638, time taken: 0.02056884765625s\n",
      "445/589, train_loss: 293.4607, time taken: 0.02073383331298828s\n",
      "446/589, train_loss: 259.5182, time taken: 0.021052837371826172s\n",
      "447/589, train_loss: 253.4940, time taken: 0.020824909210205078s\n",
      "448/589, train_loss: 227.9368, time taken: 0.02050042152404785s\n",
      "449/589, train_loss: 245.2969, time taken: 0.020284175872802734s\n",
      "450/589, train_loss: 309.3529, time taken: 0.020544052124023438s\n",
      "451/589, train_loss: 259.1350, time taken: 0.023043155670166016s\n",
      "452/589, train_loss: 214.1891, time taken: 0.022933006286621094s\n",
      "453/589, train_loss: 241.4316, time taken: 0.02289438247680664s\n",
      "454/589, train_loss: 316.6382, time taken: 0.020421504974365234s\n",
      "455/589, train_loss: 228.0630, time taken: 0.022899389266967773s\n",
      "456/589, train_loss: 272.9806, time taken: 0.02288532257080078s\n",
      "457/589, train_loss: 286.1414, time taken: 0.02246856689453125s\n",
      "458/589, train_loss: 229.8431, time taken: 0.02299785614013672s\n",
      "459/589, train_loss: 220.2399, time taken: 0.0209505558013916s\n",
      "460/589, train_loss: 261.6171, time taken: 0.020210742950439453s\n",
      "461/589, train_loss: 280.4117, time taken: 0.02093648910522461s\n",
      "462/589, train_loss: 182.4944, time taken: 0.0218050479888916s\n",
      "463/589, train_loss: 231.7960, time taken: 0.020222187042236328s\n",
      "464/589, train_loss: 313.3721, time taken: 0.021773338317871094s\n",
      "465/589, train_loss: 238.0983, time taken: 0.021409273147583008s\n",
      "466/589, train_loss: 313.9462, time taken: 0.02280569076538086s\n",
      "467/589, train_loss: 237.4704, time taken: 0.020860671997070312s\n",
      "468/589, train_loss: 287.1682, time taken: 0.02106022834777832s\n",
      "469/589, train_loss: 202.9166, time taken: 0.020042896270751953s\n",
      "470/589, train_loss: 179.7269, time taken: 0.02038264274597168s\n",
      "471/589, train_loss: 222.7769, time taken: 0.020159482955932617s\n",
      "472/589, train_loss: 296.5210, time taken: 0.022704362869262695s\n",
      "473/589, train_loss: 236.6319, time taken: 0.022770166397094727s\n",
      "474/589, train_loss: 231.7383, time taken: 0.023101091384887695s\n",
      "475/589, train_loss: 304.3133, time taken: 0.020462989807128906s\n",
      "476/589, train_loss: 292.0571, time taken: 0.02092885971069336s\n",
      "477/589, train_loss: 272.2596, time taken: 0.021335363388061523s\n",
      "478/589, train_loss: 251.9028, time taken: 0.023122787475585938s\n",
      "479/589, train_loss: 261.1129, time taken: 0.023105859756469727s\n",
      "480/589, train_loss: 248.1918, time taken: 0.023555278778076172s\n",
      "481/589, train_loss: 185.9268, time taken: 0.02234363555908203s\n",
      "482/589, train_loss: 215.4007, time taken: 0.021508455276489258s\n",
      "483/589, train_loss: 209.7082, time taken: 0.021909475326538086s\n",
      "484/589, train_loss: 350.3895, time taken: 0.02235102653503418s\n",
      "485/589, train_loss: 257.0675, time taken: 0.02178335189819336s\n",
      "486/589, train_loss: 255.2133, time taken: 0.021910905838012695s\n",
      "487/589, train_loss: 349.5789, time taken: 0.023160934448242188s\n",
      "488/589, train_loss: 324.9421, time taken: 0.021974802017211914s\n",
      "489/589, train_loss: 185.4619, time taken: 0.020348787307739258s\n",
      "490/589, train_loss: 267.6784, time taken: 0.023264408111572266s\n",
      "491/589, train_loss: 248.7062, time taken: 0.02332139015197754s\n",
      "492/589, train_loss: 259.3487, time taken: 0.022840261459350586s\n",
      "493/589, train_loss: 196.8064, time taken: 0.02023768424987793s\n",
      "494/589, train_loss: 279.5529, time taken: 0.020211458206176758s\n",
      "495/589, train_loss: 219.5643, time taken: 0.02303147315979004s\n",
      "496/589, train_loss: 198.7840, time taken: 0.023003339767456055s\n",
      "497/589, train_loss: 236.3668, time taken: 0.022998571395874023s\n",
      "498/589, train_loss: 261.0374, time taken: 0.022652387619018555s\n",
      "499/589, train_loss: 293.9263, time taken: 0.023209095001220703s\n",
      "500/589, train_loss: 242.4855, time taken: 0.022705078125s\n",
      "501/589, train_loss: 161.8317, time taken: 0.020447492599487305s\n",
      "502/589, train_loss: 276.7560, time taken: 0.023287296295166016s\n",
      "503/589, train_loss: 316.4048, time taken: 0.022929668426513672s\n",
      "504/589, train_loss: 175.0539, time taken: 0.020142316818237305s\n",
      "505/589, train_loss: 238.0956, time taken: 0.023035526275634766s\n",
      "506/589, train_loss: 236.5583, time taken: 0.0227510929107666s\n",
      "507/589, train_loss: 166.2583, time taken: 0.020228147506713867s\n",
      "508/589, train_loss: 226.0557, time taken: 0.0201723575592041s\n",
      "509/589, train_loss: 299.9700, time taken: 0.023530244827270508s\n",
      "510/589, train_loss: 222.1040, time taken: 0.02035045623779297s\n",
      "511/589, train_loss: 221.1294, time taken: 0.020328998565673828s\n",
      "512/589, train_loss: 269.0667, time taken: 0.022354841232299805s\n",
      "513/589, train_loss: 223.8919, time taken: 0.022943973541259766s\n",
      "514/589, train_loss: 216.2892, time taken: 0.021162748336791992s\n",
      "515/589, train_loss: 187.7452, time taken: 0.02366948127746582s\n",
      "516/589, train_loss: 199.4227, time taken: 0.020191669464111328s\n",
      "517/589, train_loss: 227.7025, time taken: 0.02012944221496582s\n",
      "518/589, train_loss: 266.4828, time taken: 0.023311138153076172s\n",
      "519/589, train_loss: 227.5375, time taken: 0.02266669273376465s\n",
      "520/589, train_loss: 203.1252, time taken: 0.020161867141723633s\n",
      "521/589, train_loss: 256.9918, time taken: 0.02004098892211914s\n",
      "522/589, train_loss: 283.4216, time taken: 0.022582054138183594s\n",
      "523/589, train_loss: 192.0301, time taken: 0.021229028701782227s\n",
      "524/589, train_loss: 230.4487, time taken: 0.022193193435668945s\n",
      "525/589, train_loss: 270.6686, time taken: 0.0203702449798584s\n",
      "526/589, train_loss: 282.4557, time taken: 0.020116329193115234s\n",
      "527/589, train_loss: 221.0116, time taken: 0.023401975631713867s\n",
      "528/589, train_loss: 227.3860, time taken: 0.023965835571289062s\n",
      "529/589, train_loss: 222.0107, time taken: 0.02362203598022461s\n",
      "530/589, train_loss: 231.2063, time taken: 0.02023005485534668s\n",
      "531/589, train_loss: 218.1687, time taken: 0.021947622299194336s\n",
      "532/589, train_loss: 244.6368, time taken: 0.021547317504882812s\n",
      "533/589, train_loss: 192.8105, time taken: 0.0222017765045166s\n",
      "534/589, train_loss: 214.8083, time taken: 0.020264625549316406s\n",
      "535/589, train_loss: 238.9916, time taken: 0.020338773727416992s\n",
      "536/589, train_loss: 210.7800, time taken: 0.02316570281982422s\n",
      "537/589, train_loss: 327.6310, time taken: 0.0231931209564209s\n",
      "538/589, train_loss: 195.6395, time taken: 0.02059316635131836s\n",
      "539/589, train_loss: 266.5110, time taken: 0.023334264755249023s\n",
      "540/589, train_loss: 216.4139, time taken: 0.022822856903076172s\n",
      "541/589, train_loss: 258.4743, time taken: 0.022951602935791016s\n",
      "542/589, train_loss: 224.9189, time taken: 0.02250838279724121s\n",
      "543/589, train_loss: 201.9299, time taken: 0.022246360778808594s\n",
      "544/589, train_loss: 228.5853, time taken: 0.022823572158813477s\n",
      "545/589, train_loss: 187.6662, time taken: 0.022675514221191406s\n",
      "546/589, train_loss: 267.3779, time taken: 0.022163867950439453s\n",
      "547/589, train_loss: 259.7728, time taken: 0.022897005081176758s\n",
      "548/589, train_loss: 213.6529, time taken: 0.022393465042114258s\n",
      "549/589, train_loss: 246.4527, time taken: 0.02062511444091797s\n",
      "550/589, train_loss: 231.7961, time taken: 0.023137331008911133s\n",
      "551/589, train_loss: 267.0457, time taken: 0.023135662078857422s\n",
      "552/589, train_loss: 255.6694, time taken: 0.02278876304626465s\n",
      "553/589, train_loss: 191.8412, time taken: 0.02293086051940918s\n",
      "554/589, train_loss: 267.4525, time taken: 0.022584915161132812s\n",
      "555/589, train_loss: 246.2740, time taken: 0.020229339599609375s\n",
      "556/589, train_loss: 260.0778, time taken: 0.023200273513793945s\n",
      "557/589, train_loss: 223.3427, time taken: 0.022279739379882812s\n",
      "558/589, train_loss: 238.9455, time taken: 0.02017378807067871s\n",
      "559/589, train_loss: 266.0610, time taken: 0.021609067916870117s\n",
      "560/589, train_loss: 231.1831, time taken: 0.023360013961791992s\n",
      "561/589, train_loss: 166.6455, time taken: 0.021839141845703125s\n",
      "562/589, train_loss: 257.5638, time taken: 0.023041248321533203s\n",
      "563/589, train_loss: 218.8791, time taken: 0.021290302276611328s\n",
      "564/589, train_loss: 260.2575, time taken: 0.021013736724853516s\n",
      "565/589, train_loss: 209.9617, time taken: 0.023237228393554688s\n",
      "566/589, train_loss: 238.8672, time taken: 0.022834062576293945s\n",
      "567/589, train_loss: 182.4615, time taken: 0.022575855255126953s\n",
      "568/589, train_loss: 257.2734, time taken: 0.02027153968811035s\n",
      "569/589, train_loss: 209.0799, time taken: 0.02158641815185547s\n",
      "570/589, train_loss: 287.2454, time taken: 0.021529197692871094s\n",
      "571/589, train_loss: 221.3672, time taken: 0.02112269401550293s\n",
      "572/589, train_loss: 243.4967, time taken: 0.021201372146606445s\n",
      "573/589, train_loss: 196.1995, time taken: 0.023095369338989258s\n",
      "574/589, train_loss: 205.1499, time taken: 0.02323174476623535s\n",
      "575/589, train_loss: 255.5513, time taken: 0.0232086181640625s\n",
      "576/589, train_loss: 283.3631, time taken: 0.022640466690063477s\n",
      "577/589, train_loss: 243.8524, time taken: 0.02338862419128418s\n",
      "578/589, train_loss: 214.8554, time taken: 0.02050328254699707s\n",
      "579/589, train_loss: 219.2518, time taken: 0.023136615753173828s\n",
      "580/589, train_loss: 248.0551, time taken: 0.022379398345947266s\n",
      "581/589, train_loss: 199.3742, time taken: 0.02302265167236328s\n",
      "582/589, train_loss: 263.8772, time taken: 0.021638870239257812s\n",
      "583/589, train_loss: 319.3812, time taken: 0.020026683807373047s\n",
      "584/589, train_loss: 198.7786, time taken: 0.02062392234802246s\n",
      "585/589, train_loss: 173.6618, time taken: 0.020746707916259766s\n",
      "586/589, train_loss: 327.4783, time taken: 0.02145075798034668s\n",
      "587/589, train_loss: 241.7004, time taken: 0.020984649658203125s\n",
      "588/589, train_loss: 214.3285, time taken: 0.020837068557739258s\n",
      "589/589, train_loss: 216.4916, time taken: 0.02091217041015625s\n",
      "590/589, train_loss: 306.2099, time taken: 0.011121273040771484s\n",
      "epoch 1 average loss: 246.5211\n",
      "Entering Validation for epoch: 1\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([13, 1, 64, 64])\n",
      "epoch 1 Validation avg loss: 86.4350, time taken: 0.01905965805053711s\n",
      "Saving new model based on validation loss 86.4350\n",
      "----------\n",
      "epoch 2/4\n",
      "1/589, train_loss: 242.0346, time taken: 0.02815103530883789s\n",
      "2/589, train_loss: 190.1912, time taken: 0.02104473114013672s\n",
      "3/589, train_loss: 212.4086, time taken: 0.020813465118408203s\n",
      "4/589, train_loss: 261.9045, time taken: 0.022386550903320312s\n",
      "5/589, train_loss: 237.7670, time taken: 0.022975444793701172s\n",
      "6/589, train_loss: 241.1677, time taken: 0.02301931381225586s\n",
      "7/589, train_loss: 259.3326, time taken: 0.022970914840698242s\n",
      "8/589, train_loss: 223.1199, time taken: 0.021831274032592773s\n",
      "9/589, train_loss: 211.6443, time taken: 0.02315354347229004s\n",
      "10/589, train_loss: 307.5244, time taken: 0.021285533905029297s\n",
      "11/589, train_loss: 166.5922, time taken: 0.022190332412719727s\n",
      "12/589, train_loss: 244.4473, time taken: 0.0203399658203125s\n",
      "13/589, train_loss: 265.7582, time taken: 0.022420406341552734s\n",
      "14/589, train_loss: 229.6493, time taken: 0.021965980529785156s\n",
      "15/589, train_loss: 239.7668, time taken: 0.020541667938232422s\n",
      "16/589, train_loss: 261.1046, time taken: 0.020591020584106445s\n",
      "17/589, train_loss: 241.2109, time taken: 0.022870779037475586s\n",
      "18/589, train_loss: 284.6947, time taken: 0.021847009658813477s\n",
      "19/589, train_loss: 287.4537, time taken: 0.020268678665161133s\n",
      "20/589, train_loss: 286.1151, time taken: 0.019907712936401367s\n",
      "21/589, train_loss: 213.0126, time taken: 0.022997140884399414s\n",
      "22/589, train_loss: 176.4332, time taken: 0.02025127410888672s\n",
      "23/589, train_loss: 296.2550, time taken: 0.023068666458129883s\n",
      "24/589, train_loss: 226.8039, time taken: 0.023111581802368164s\n",
      "25/589, train_loss: 283.5884, time taken: 0.022127866744995117s\n",
      "26/589, train_loss: 293.9746, time taken: 0.021766185760498047s\n",
      "27/589, train_loss: 273.1127, time taken: 0.02317953109741211s\n",
      "28/589, train_loss: 225.5414, time taken: 0.022881269454956055s\n",
      "29/589, train_loss: 275.4190, time taken: 0.022903919219970703s\n",
      "30/589, train_loss: 218.6046, time taken: 0.022139549255371094s\n",
      "31/589, train_loss: 202.6051, time taken: 0.02300119400024414s\n",
      "32/589, train_loss: 213.1893, time taken: 0.021799564361572266s\n",
      "33/589, train_loss: 265.2705, time taken: 0.023149728775024414s\n",
      "34/589, train_loss: 185.7691, time taken: 0.0202486515045166s\n",
      "35/589, train_loss: 206.9282, time taken: 0.023106813430786133s\n",
      "36/589, train_loss: 196.2758, time taken: 0.020370006561279297s\n",
      "37/589, train_loss: 206.9811, time taken: 0.022821664810180664s\n",
      "38/589, train_loss: 233.3012, time taken: 0.020844697952270508s\n",
      "39/589, train_loss: 193.1965, time taken: 0.021152019500732422s\n",
      "40/589, train_loss: 311.9105, time taken: 0.023099184036254883s\n",
      "41/589, train_loss: 281.4479, time taken: 0.02286982536315918s\n",
      "42/589, train_loss: 239.8080, time taken: 0.023131132125854492s\n",
      "43/589, train_loss: 211.1202, time taken: 0.02043604850769043s\n",
      "44/589, train_loss: 216.9084, time taken: 0.02089858055114746s\n",
      "45/589, train_loss: 191.7751, time taken: 0.021561861038208008s\n",
      "46/589, train_loss: 197.9077, time taken: 0.02331233024597168s\n",
      "47/589, train_loss: 254.4711, time taken: 0.023009300231933594s\n",
      "48/589, train_loss: 245.4048, time taken: 0.021494150161743164s\n",
      "49/589, train_loss: 204.3288, time taken: 0.02336740493774414s\n",
      "50/589, train_loss: 225.7349, time taken: 0.023664474487304688s\n",
      "51/589, train_loss: 271.4998, time taken: 0.022810935974121094s\n",
      "52/589, train_loss: 214.6266, time taken: 0.0212399959564209s\n",
      "53/589, train_loss: 277.7075, time taken: 0.022139787673950195s\n",
      "54/589, train_loss: 248.7492, time taken: 0.021771907806396484s\n",
      "55/589, train_loss: 213.4832, time taken: 0.022979259490966797s\n",
      "56/589, train_loss: 275.7613, time taken: 0.02346515655517578s\n",
      "57/589, train_loss: 231.7622, time taken: 0.025592565536499023s\n",
      "58/589, train_loss: 235.4716, time taken: 0.023059606552124023s\n",
      "59/589, train_loss: 308.6862, time taken: 0.022032499313354492s\n",
      "60/589, train_loss: 215.9355, time taken: 0.02298569679260254s\n",
      "61/589, train_loss: 223.1258, time taken: 0.0219423770904541s\n",
      "62/589, train_loss: 196.0699, time taken: 0.022957324981689453s\n",
      "63/589, train_loss: 220.8987, time taken: 0.020329713821411133s\n",
      "64/589, train_loss: 344.1687, time taken: 0.023641347885131836s\n",
      "65/589, train_loss: 182.1322, time taken: 0.020301103591918945s\n",
      "66/589, train_loss: 299.4261, time taken: 0.023438692092895508s\n",
      "67/589, train_loss: 268.4137, time taken: 0.022874832153320312s\n",
      "68/589, train_loss: 239.2647, time taken: 0.020308732986450195s\n",
      "69/589, train_loss: 324.0820, time taken: 0.023671865463256836s\n",
      "70/589, train_loss: 229.2347, time taken: 0.02355194091796875s\n",
      "71/589, train_loss: 248.5180, time taken: 0.022704124450683594s\n",
      "72/589, train_loss: 191.7461, time taken: 0.022415876388549805s\n",
      "73/589, train_loss: 274.8317, time taken: 0.021075963973999023s\n",
      "74/589, train_loss: 200.7264, time taken: 0.02023005485534668s\n",
      "75/589, train_loss: 225.1306, time taken: 0.02278733253479004s\n",
      "76/589, train_loss: 242.9405, time taken: 0.021254301071166992s\n",
      "77/589, train_loss: 252.6558, time taken: 0.023171424865722656s\n",
      "78/589, train_loss: 249.4145, time taken: 0.02348780632019043s\n",
      "79/589, train_loss: 194.8684, time taken: 0.022339582443237305s\n",
      "80/589, train_loss: 256.4778, time taken: 0.020200729370117188s\n",
      "81/589, train_loss: 263.1080, time taken: 0.020339250564575195s\n",
      "82/589, train_loss: 283.0031, time taken: 0.021689891815185547s\n",
      "83/589, train_loss: 242.7782, time taken: 0.02060079574584961s\n",
      "84/589, train_loss: 224.0268, time taken: 0.020620107650756836s\n",
      "85/589, train_loss: 243.3991, time taken: 0.021615982055664062s\n",
      "86/589, train_loss: 261.5109, time taken: 0.02250957489013672s\n",
      "87/589, train_loss: 224.0487, time taken: 0.023782968521118164s\n",
      "88/589, train_loss: 262.1431, time taken: 0.023014307022094727s\n",
      "89/589, train_loss: 275.4090, time taken: 0.023438215255737305s\n",
      "90/589, train_loss: 256.3196, time taken: 0.02028059959411621s\n",
      "91/589, train_loss: 225.3219, time taken: 0.02410149574279785s\n",
      "92/589, train_loss: 232.0699, time taken: 0.0215911865234375s\n",
      "93/589, train_loss: 244.7574, time taken: 0.021494150161743164s\n",
      "94/589, train_loss: 200.6719, time taken: 0.022001028060913086s\n",
      "95/589, train_loss: 229.1019, time taken: 0.02028059959411621s\n",
      "96/589, train_loss: 251.3267, time taken: 0.020485639572143555s\n",
      "97/589, train_loss: 286.6244, time taken: 0.023339271545410156s\n",
      "98/589, train_loss: 204.1377, time taken: 0.02337050437927246s\n",
      "99/589, train_loss: 260.9796, time taken: 0.025098562240600586s\n",
      "100/589, train_loss: 310.0966, time taken: 0.02074599266052246s\n",
      "101/589, train_loss: 239.6645, time taken: 0.020247459411621094s\n",
      "102/589, train_loss: 352.7350, time taken: 0.020975351333618164s\n",
      "103/589, train_loss: 228.5038, time taken: 0.021842002868652344s\n",
      "104/589, train_loss: 159.8523, time taken: 0.022266864776611328s\n",
      "105/589, train_loss: 217.2516, time taken: 0.021161556243896484s\n",
      "106/589, train_loss: 279.8875, time taken: 0.022842884063720703s\n",
      "107/589, train_loss: 234.3335, time taken: 0.023270130157470703s\n",
      "108/589, train_loss: 239.8325, time taken: 0.022760868072509766s\n",
      "109/589, train_loss: 234.2871, time taken: 0.020665645599365234s\n",
      "110/589, train_loss: 210.1915, time taken: 0.02137160301208496s\n",
      "111/589, train_loss: 251.0030, time taken: 0.02298450469970703s\n",
      "112/589, train_loss: 201.4297, time taken: 0.02305293083190918s\n",
      "113/589, train_loss: 255.3960, time taken: 0.020297765731811523s\n",
      "114/589, train_loss: 236.7385, time taken: 0.020556211471557617s\n",
      "115/589, train_loss: 206.9863, time taken: 0.023447275161743164s\n",
      "116/589, train_loss: 243.1241, time taken: 0.020362377166748047s\n",
      "117/589, train_loss: 246.5547, time taken: 0.022922277450561523s\n",
      "118/589, train_loss: 246.5229, time taken: 0.0205533504486084s\n",
      "119/589, train_loss: 244.0474, time taken: 0.020319461822509766s\n",
      "120/589, train_loss: 280.5720, time taken: 0.02054119110107422s\n",
      "121/589, train_loss: 244.8449, time taken: 0.023673057556152344s\n",
      "122/589, train_loss: 258.2877, time taken: 0.02329087257385254s\n",
      "123/589, train_loss: 221.0287, time taken: 0.020319700241088867s\n",
      "124/589, train_loss: 277.2440, time taken: 0.0203096866607666s\n",
      "125/589, train_loss: 309.2139, time taken: 0.02271127700805664s\n",
      "126/589, train_loss: 187.2336, time taken: 0.022591352462768555s\n",
      "127/589, train_loss: 254.8521, time taken: 0.02103257179260254s\n",
      "128/589, train_loss: 233.8559, time taken: 0.02290201187133789s\n",
      "129/589, train_loss: 174.5221, time taken: 0.020065784454345703s\n",
      "130/589, train_loss: 210.9047, time taken: 0.02309560775756836s\n",
      "131/589, train_loss: 135.8388, time taken: 0.023128986358642578s\n",
      "132/589, train_loss: 209.2712, time taken: 0.023136138916015625s\n",
      "133/589, train_loss: 213.2747, time taken: 0.020170211791992188s\n",
      "134/589, train_loss: 263.6586, time taken: 0.021326065063476562s\n",
      "135/589, train_loss: 250.6191, time taken: 0.02033710479736328s\n",
      "136/589, train_loss: 178.3650, time taken: 0.021247148513793945s\n",
      "137/589, train_loss: 298.2619, time taken: 0.022959470748901367s\n",
      "138/589, train_loss: 243.3847, time taken: 0.021083354949951172s\n",
      "139/589, train_loss: 222.7964, time taken: 0.021938562393188477s\n",
      "140/589, train_loss: 268.8801, time taken: 0.022980928421020508s\n",
      "141/589, train_loss: 219.6782, time taken: 0.020763874053955078s\n",
      "142/589, train_loss: 253.7787, time taken: 0.022620439529418945s\n",
      "143/589, train_loss: 280.5489, time taken: 0.02251124382019043s\n",
      "144/589, train_loss: 280.1129, time taken: 0.02070140838623047s\n",
      "145/589, train_loss: 265.6519, time taken: 0.02282881736755371s\n",
      "146/589, train_loss: 237.3848, time taken: 0.023254871368408203s\n",
      "147/589, train_loss: 239.7990, time taken: 0.021830081939697266s\n",
      "148/589, train_loss: 289.3936, time taken: 0.022440671920776367s\n",
      "149/589, train_loss: 196.7243, time taken: 0.021014928817749023s\n",
      "150/589, train_loss: 280.9410, time taken: 0.020709514617919922s\n",
      "151/589, train_loss: 293.3188, time taken: 0.022899627685546875s\n",
      "152/589, train_loss: 241.0838, time taken: 0.022883892059326172s\n",
      "153/589, train_loss: 295.6801, time taken: 0.02108621597290039s\n",
      "154/589, train_loss: 236.1687, time taken: 0.021464109420776367s\n",
      "155/589, train_loss: 198.4671, time taken: 0.02140212059020996s\n",
      "156/589, train_loss: 254.9155, time taken: 0.021651029586791992s\n",
      "157/589, train_loss: 177.9100, time taken: 0.02190852165222168s\n",
      "158/589, train_loss: 291.3115, time taken: 0.023172855377197266s\n",
      "159/589, train_loss: 214.5047, time taken: 0.02141880989074707s\n",
      "160/589, train_loss: 231.0486, time taken: 0.02251267433166504s\n",
      "161/589, train_loss: 272.8706, time taken: 0.021162986755371094s\n",
      "162/589, train_loss: 220.5843, time taken: 0.019829511642456055s\n",
      "163/589, train_loss: 255.0655, time taken: 0.020220279693603516s\n",
      "164/589, train_loss: 240.3904, time taken: 0.021495580673217773s\n",
      "165/589, train_loss: 229.1920, time taken: 0.02064061164855957s\n",
      "166/589, train_loss: 274.7236, time taken: 0.023604154586791992s\n",
      "167/589, train_loss: 208.3707, time taken: 0.02299666404724121s\n",
      "168/589, train_loss: 186.5979, time taken: 0.025638580322265625s\n",
      "169/589, train_loss: 199.9723, time taken: 0.023438453674316406s\n",
      "170/589, train_loss: 205.3523, time taken: 0.023023366928100586s\n",
      "171/589, train_loss: 185.1984, time taken: 0.021504640579223633s\n",
      "172/589, train_loss: 204.2282, time taken: 0.022371768951416016s\n",
      "173/589, train_loss: 184.6218, time taken: 0.023177623748779297s\n",
      "174/589, train_loss: 258.0872, time taken: 0.02277851104736328s\n",
      "175/589, train_loss: 160.4380, time taken: 0.022904396057128906s\n",
      "176/589, train_loss: 242.4160, time taken: 0.02385544776916504s\n",
      "177/589, train_loss: 265.9265, time taken: 0.02286529541015625s\n",
      "178/589, train_loss: 228.0253, time taken: 0.020376920700073242s\n",
      "179/589, train_loss: 242.6359, time taken: 0.021489620208740234s\n",
      "180/589, train_loss: 254.4804, time taken: 0.021900653839111328s\n",
      "181/589, train_loss: 186.9464, time taken: 0.020074844360351562s\n",
      "182/589, train_loss: 209.1498, time taken: 0.020069360733032227s\n",
      "183/589, train_loss: 279.6047, time taken: 0.022803068161010742s\n",
      "184/589, train_loss: 298.3892, time taken: 0.02043294906616211s\n",
      "185/589, train_loss: 254.8951, time taken: 0.02002096176147461s\n",
      "186/589, train_loss: 327.3371, time taken: 0.02299046516418457s\n",
      "187/589, train_loss: 248.4723, time taken: 0.022466421127319336s\n",
      "188/589, train_loss: 297.7472, time taken: 0.0211794376373291s\n",
      "189/589, train_loss: 270.0360, time taken: 0.023081541061401367s\n",
      "190/589, train_loss: 205.4852, time taken: 0.020201444625854492s\n",
      "191/589, train_loss: 253.5089, time taken: 0.022080659866333008s\n",
      "192/589, train_loss: 229.5814, time taken: 0.020255327224731445s\n",
      "193/589, train_loss: 220.8484, time taken: 0.022004127502441406s\n",
      "194/589, train_loss: 248.7660, time taken: 0.02307605743408203s\n",
      "195/589, train_loss: 252.7716, time taken: 0.021818876266479492s\n",
      "196/589, train_loss: 221.0124, time taken: 0.02308034896850586s\n",
      "197/589, train_loss: 257.7045, time taken: 0.021081924438476562s\n",
      "198/589, train_loss: 185.9420, time taken: 0.022257328033447266s\n",
      "199/589, train_loss: 231.1215, time taken: 0.023315906524658203s\n",
      "200/589, train_loss: 206.3601, time taken: 0.023872852325439453s\n",
      "201/589, train_loss: 207.3010, time taken: 0.02145099639892578s\n",
      "202/589, train_loss: 209.5918, time taken: 0.020342350006103516s\n",
      "203/589, train_loss: 248.1376, time taken: 0.02270650863647461s\n",
      "204/589, train_loss: 201.6458, time taken: 0.02219247817993164s\n",
      "205/589, train_loss: 184.6610, time taken: 0.020594120025634766s\n",
      "206/589, train_loss: 242.1545, time taken: 0.020219802856445312s\n",
      "207/589, train_loss: 260.1220, time taken: 0.023519039154052734s\n",
      "208/589, train_loss: 262.6104, time taken: 0.021218538284301758s\n",
      "209/589, train_loss: 223.7272, time taken: 0.023066282272338867s\n",
      "210/589, train_loss: 169.2189, time taken: 0.022214889526367188s\n",
      "211/589, train_loss: 220.7500, time taken: 0.02435469627380371s\n",
      "212/589, train_loss: 248.9583, time taken: 0.0208132266998291s\n",
      "213/589, train_loss: 212.2711, time taken: 0.022029876708984375s\n",
      "214/589, train_loss: 224.1661, time taken: 0.021340608596801758s\n",
      "215/589, train_loss: 227.0008, time taken: 0.020244598388671875s\n",
      "216/589, train_loss: 282.0240, time taken: 0.022542476654052734s\n",
      "217/589, train_loss: 166.7849, time taken: 0.0230255126953125s\n",
      "218/589, train_loss: 291.6753, time taken: 0.020004987716674805s\n",
      "219/589, train_loss: 200.8270, time taken: 0.022104978561401367s\n",
      "220/589, train_loss: 199.3959, time taken: 0.020132064819335938s\n",
      "221/589, train_loss: 218.9674, time taken: 0.022124767303466797s\n",
      "222/589, train_loss: 156.6354, time taken: 0.02324652671813965s\n",
      "223/589, train_loss: 318.7382, time taken: 0.022615909576416016s\n",
      "224/589, train_loss: 280.2721, time taken: 0.020270347595214844s\n",
      "225/589, train_loss: 272.6548, time taken: 0.022320985794067383s\n",
      "226/589, train_loss: 280.1064, time taken: 0.021830081939697266s\n",
      "227/589, train_loss: 231.0291, time taken: 0.022902488708496094s\n",
      "228/589, train_loss: 288.7346, time taken: 0.023220300674438477s\n",
      "229/589, train_loss: 183.2349, time taken: 0.020290136337280273s\n",
      "230/589, train_loss: 285.5094, time taken: 0.0246279239654541s\n",
      "231/589, train_loss: 261.0735, time taken: 0.023313522338867188s\n",
      "232/589, train_loss: 227.0362, time taken: 0.02035665512084961s\n",
      "233/589, train_loss: 178.0717, time taken: 0.02045416831970215s\n",
      "234/589, train_loss: 200.5383, time taken: 0.023465394973754883s\n",
      "235/589, train_loss: 225.0097, time taken: 0.02060675621032715s\n",
      "236/589, train_loss: 206.3075, time taken: 0.022348642349243164s\n",
      "237/589, train_loss: 233.3192, time taken: 0.022221088409423828s\n",
      "238/589, train_loss: 206.4250, time taken: 0.02243971824645996s\n",
      "239/589, train_loss: 217.2029, time taken: 0.02054738998413086s\n",
      "240/589, train_loss: 242.0385, time taken: 0.02297663688659668s\n",
      "241/589, train_loss: 246.0690, time taken: 0.02274346351623535s\n",
      "242/589, train_loss: 263.1683, time taken: 0.023180246353149414s\n",
      "243/589, train_loss: 211.3187, time taken: 0.023139476776123047s\n",
      "244/589, train_loss: 210.0248, time taken: 0.022948026657104492s\n",
      "245/589, train_loss: 253.8237, time taken: 0.022928953170776367s\n",
      "246/589, train_loss: 321.8887, time taken: 0.02184915542602539s\n",
      "247/589, train_loss: 231.0917, time taken: 0.02346324920654297s\n",
      "248/589, train_loss: 223.8477, time taken: 0.022395610809326172s\n",
      "249/589, train_loss: 267.6683, time taken: 0.023125648498535156s\n",
      "250/589, train_loss: 193.7271, time taken: 0.02147984504699707s\n",
      "251/589, train_loss: 332.3295, time taken: 0.022803306579589844s\n",
      "252/589, train_loss: 283.0304, time taken: 0.020810604095458984s\n",
      "253/589, train_loss: 293.5939, time taken: 0.021393775939941406s\n",
      "254/589, train_loss: 191.7327, time taken: 0.02148294448852539s\n",
      "255/589, train_loss: 219.4224, time taken: 0.021371841430664062s\n",
      "256/589, train_loss: 179.5366, time taken: 0.02025628089904785s\n",
      "257/589, train_loss: 220.6212, time taken: 0.02115011215209961s\n",
      "258/589, train_loss: 324.4638, time taken: 0.02233433723449707s\n",
      "259/589, train_loss: 218.0347, time taken: 0.022191762924194336s\n",
      "260/589, train_loss: 278.5923, time taken: 0.021866321563720703s\n",
      "261/589, train_loss: 254.0153, time taken: 0.022820234298706055s\n",
      "262/589, train_loss: 227.6850, time taken: 0.022295236587524414s\n",
      "263/589, train_loss: 246.6829, time taken: 0.022812843322753906s\n",
      "264/589, train_loss: 257.0036, time taken: 0.0232088565826416s\n",
      "265/589, train_loss: 214.6307, time taken: 0.021203279495239258s\n",
      "266/589, train_loss: 199.7048, time taken: 0.023981809616088867s\n",
      "267/589, train_loss: 218.1552, time taken: 0.02054119110107422s\n",
      "268/589, train_loss: 158.0253, time taken: 0.020240068435668945s\n",
      "269/589, train_loss: 259.5905, time taken: 0.02032947540283203s\n",
      "270/589, train_loss: 198.9671, time taken: 0.020183563232421875s\n",
      "271/589, train_loss: 273.7344, time taken: 0.02122640609741211s\n",
      "272/589, train_loss: 245.8483, time taken: 0.023201704025268555s\n",
      "273/589, train_loss: 215.3499, time taken: 0.02306652069091797s\n",
      "274/589, train_loss: 298.8744, time taken: 0.0221710205078125s\n",
      "275/589, train_loss: 214.1438, time taken: 0.02054572105407715s\n",
      "276/589, train_loss: 224.7826, time taken: 0.0222470760345459s\n",
      "277/589, train_loss: 218.3094, time taken: 0.021250009536743164s\n",
      "278/589, train_loss: 215.3698, time taken: 0.023418426513671875s\n",
      "279/589, train_loss: 243.8217, time taken: 0.022325754165649414s\n",
      "280/589, train_loss: 237.5724, time taken: 0.022915124893188477s\n",
      "281/589, train_loss: 239.3895, time taken: 0.021383285522460938s\n",
      "282/589, train_loss: 208.0308, time taken: 0.01983189582824707s\n",
      "283/589, train_loss: 210.0260, time taken: 0.02295374870300293s\n",
      "284/589, train_loss: 247.7993, time taken: 0.02089405059814453s\n",
      "285/589, train_loss: 270.5410, time taken: 0.021788597106933594s\n",
      "286/589, train_loss: 232.1164, time taken: 0.022457599639892578s\n",
      "287/589, train_loss: 273.5144, time taken: 0.022416353225708008s\n",
      "288/589, train_loss: 272.6685, time taken: 0.022801637649536133s\n",
      "289/589, train_loss: 169.6089, time taken: 0.02047443389892578s\n",
      "290/589, train_loss: 229.0753, time taken: 0.020021915435791016s\n",
      "291/589, train_loss: 244.7959, time taken: 0.0230715274810791s\n",
      "292/589, train_loss: 269.5826, time taken: 0.022820711135864258s\n",
      "293/589, train_loss: 247.1035, time taken: 0.02323293685913086s\n",
      "294/589, train_loss: 225.7621, time taken: 0.023085832595825195s\n",
      "295/589, train_loss: 196.5045, time taken: 0.023053884506225586s\n",
      "296/589, train_loss: 218.5257, time taken: 0.02275538444519043s\n",
      "297/589, train_loss: 303.4019, time taken: 0.020365476608276367s\n",
      "298/589, train_loss: 256.3817, time taken: 0.019948482513427734s\n",
      "299/589, train_loss: 216.3713, time taken: 0.020671367645263672s\n",
      "300/589, train_loss: 191.7756, time taken: 0.020697355270385742s\n",
      "301/589, train_loss: 237.8267, time taken: 0.022712230682373047s\n",
      "302/589, train_loss: 185.4428, time taken: 0.022336244583129883s\n",
      "303/589, train_loss: 230.1485, time taken: 0.022974014282226562s\n",
      "304/589, train_loss: 234.0426, time taken: 0.02296900749206543s\n",
      "305/589, train_loss: 243.4798, time taken: 0.021791458129882812s\n",
      "306/589, train_loss: 268.6041, time taken: 0.020088911056518555s\n",
      "307/589, train_loss: 266.0483, time taken: 0.022698640823364258s\n",
      "308/589, train_loss: 198.6921, time taken: 0.022642850875854492s\n",
      "309/589, train_loss: 247.6503, time taken: 0.021917104721069336s\n",
      "310/589, train_loss: 249.3992, time taken: 0.020992040634155273s\n",
      "311/589, train_loss: 239.1024, time taken: 0.020228147506713867s\n",
      "312/589, train_loss: 183.6129, time taken: 0.02002406120300293s\n",
      "313/589, train_loss: 231.5938, time taken: 0.021298646926879883s\n",
      "314/589, train_loss: 183.3689, time taken: 0.02252936363220215s\n",
      "315/589, train_loss: 218.2729, time taken: 0.021876811981201172s\n",
      "316/589, train_loss: 253.0819, time taken: 0.020216703414916992s\n",
      "317/589, train_loss: 246.4466, time taken: 0.02269887924194336s\n",
      "318/589, train_loss: 208.9541, time taken: 0.02150869369506836s\n",
      "319/589, train_loss: 245.5429, time taken: 0.022617101669311523s\n",
      "320/589, train_loss: 205.5109, time taken: 0.021892309188842773s\n",
      "321/589, train_loss: 232.4737, time taken: 0.02069997787475586s\n",
      "322/589, train_loss: 192.2475, time taken: 0.020074129104614258s\n",
      "323/589, train_loss: 201.9769, time taken: 0.022109270095825195s\n",
      "324/589, train_loss: 180.3472, time taken: 0.023020029067993164s\n",
      "325/589, train_loss: 195.9119, time taken: 0.020230531692504883s\n",
      "326/589, train_loss: 221.7102, time taken: 0.020287275314331055s\n",
      "327/589, train_loss: 413.7662, time taken: 0.022722721099853516s\n",
      "328/589, train_loss: 242.7547, time taken: 0.02124619483947754s\n",
      "329/589, train_loss: 228.2901, time taken: 0.022267818450927734s\n",
      "330/589, train_loss: 265.2045, time taken: 0.02079486846923828s\n",
      "331/589, train_loss: 316.5774, time taken: 0.02207493782043457s\n",
      "332/589, train_loss: 254.1262, time taken: 0.02214503288269043s\n",
      "333/589, train_loss: 219.0597, time taken: 0.023198604583740234s\n",
      "334/589, train_loss: 228.0676, time taken: 0.02332592010498047s\n",
      "335/589, train_loss: 222.2430, time taken: 0.022892236709594727s\n",
      "336/589, train_loss: 223.8362, time taken: 0.022710561752319336s\n",
      "337/589, train_loss: 266.1799, time taken: 0.02337646484375s\n",
      "338/589, train_loss: 227.6321, time taken: 0.022482633590698242s\n",
      "339/589, train_loss: 195.4458, time taken: 0.02265000343322754s\n",
      "340/589, train_loss: 259.9418, time taken: 0.021761417388916016s\n",
      "341/589, train_loss: 241.0671, time taken: 0.020259857177734375s\n",
      "342/589, train_loss: 185.2968, time taken: 0.02050042152404785s\n",
      "343/589, train_loss: 260.5112, time taken: 0.021636247634887695s\n",
      "344/589, train_loss: 261.5072, time taken: 0.022642135620117188s\n",
      "345/589, train_loss: 243.2916, time taken: 0.022929906845092773s\n",
      "346/589, train_loss: 233.7729, time taken: 0.02300715446472168s\n",
      "347/589, train_loss: 264.4413, time taken: 0.020414352416992188s\n",
      "348/589, train_loss: 183.3247, time taken: 0.02215743064880371s\n",
      "349/589, train_loss: 310.2185, time taken: 0.02292490005493164s\n",
      "350/589, train_loss: 325.9831, time taken: 0.022898435592651367s\n",
      "351/589, train_loss: 200.8771, time taken: 0.021725177764892578s\n",
      "352/589, train_loss: 184.8085, time taken: 0.022804737091064453s\n",
      "353/589, train_loss: 279.3128, time taken: 0.022960901260375977s\n",
      "354/589, train_loss: 307.7601, time taken: 0.022832155227661133s\n",
      "355/589, train_loss: 206.6902, time taken: 0.022843122482299805s\n",
      "356/589, train_loss: 247.0992, time taken: 0.022842884063720703s\n",
      "357/589, train_loss: 211.0258, time taken: 0.023164987564086914s\n",
      "358/589, train_loss: 289.2952, time taken: 0.022990942001342773s\n",
      "359/589, train_loss: 227.5264, time taken: 0.022934675216674805s\n",
      "360/589, train_loss: 276.1975, time taken: 0.020648479461669922s\n",
      "361/589, train_loss: 247.9761, time taken: 0.02060699462890625s\n",
      "362/589, train_loss: 244.4765, time taken: 0.020545005798339844s\n",
      "363/589, train_loss: 199.9348, time taken: 0.020631790161132812s\n",
      "364/589, train_loss: 217.7219, time taken: 0.021208524703979492s\n",
      "365/589, train_loss: 206.2466, time taken: 0.02179741859436035s\n",
      "366/589, train_loss: 226.8271, time taken: 0.02103447914123535s\n",
      "367/589, train_loss: 233.7267, time taken: 0.022222518920898438s\n",
      "368/589, train_loss: 149.2584, time taken: 0.02307581901550293s\n",
      "369/589, train_loss: 202.0447, time taken: 0.021601438522338867s\n",
      "370/589, train_loss: 241.3462, time taken: 0.023077726364135742s\n",
      "371/589, train_loss: 336.1678, time taken: 0.022788286209106445s\n",
      "372/589, train_loss: 226.0653, time taken: 0.0215609073638916s\n",
      "373/589, train_loss: 305.6996, time taken: 0.02021050453186035s\n",
      "374/589, train_loss: 286.0755, time taken: 0.02245616912841797s\n",
      "375/589, train_loss: 151.2107, time taken: 0.022727489471435547s\n",
      "376/589, train_loss: 218.6640, time taken: 0.02274775505065918s\n",
      "377/589, train_loss: 204.0547, time taken: 0.023547649383544922s\n",
      "378/589, train_loss: 224.2620, time taken: 0.021916866302490234s\n",
      "379/589, train_loss: 178.5770, time taken: 0.020453691482543945s\n",
      "380/589, train_loss: 141.1749, time taken: 0.02302694320678711s\n",
      "381/589, train_loss: 278.3248, time taken: 0.023099422454833984s\n",
      "382/589, train_loss: 228.0923, time taken: 0.022017478942871094s\n",
      "383/589, train_loss: 220.5139, time taken: 0.02007150650024414s\n",
      "384/589, train_loss: 224.6914, time taken: 0.020427227020263672s\n",
      "385/589, train_loss: 226.0113, time taken: 0.020282983779907227s\n",
      "386/589, train_loss: 274.8877, time taken: 0.02289867401123047s\n",
      "387/589, train_loss: 213.6299, time taken: 0.02312183380126953s\n",
      "388/589, train_loss: 309.8677, time taken: 0.022904157638549805s\n",
      "389/589, train_loss: 244.8650, time taken: 0.02082657814025879s\n",
      "390/589, train_loss: 190.3682, time taken: 0.020592212677001953s\n",
      "391/589, train_loss: 194.8516, time taken: 0.022804975509643555s\n",
      "392/589, train_loss: 250.9796, time taken: 0.022630929946899414s\n",
      "393/589, train_loss: 269.1267, time taken: 0.023357391357421875s\n",
      "394/589, train_loss: 238.0712, time taken: 0.02316904067993164s\n",
      "395/589, train_loss: 275.8403, time taken: 0.020398855209350586s\n",
      "396/589, train_loss: 263.7506, time taken: 0.023537635803222656s\n",
      "397/589, train_loss: 144.4047, time taken: 0.02265644073486328s\n",
      "398/589, train_loss: 233.9009, time taken: 0.020264625549316406s\n",
      "399/589, train_loss: 281.9633, time taken: 0.020430326461791992s\n",
      "400/589, train_loss: 184.5956, time taken: 0.022011280059814453s\n",
      "401/589, train_loss: 234.6158, time taken: 0.023575544357299805s\n",
      "402/589, train_loss: 202.6775, time taken: 0.023873090744018555s\n",
      "403/589, train_loss: 181.2747, time taken: 0.022034883499145508s\n",
      "404/589, train_loss: 250.2383, time taken: 0.022315025329589844s\n",
      "405/589, train_loss: 272.5249, time taken: 0.024309396743774414s\n",
      "406/589, train_loss: 179.0952, time taken: 0.022523164749145508s\n",
      "407/589, train_loss: 242.7221, time taken: 0.021106243133544922s\n",
      "408/589, train_loss: 232.7478, time taken: 0.02070307731628418s\n",
      "409/589, train_loss: 262.4866, time taken: 0.023091793060302734s\n",
      "410/589, train_loss: 248.5080, time taken: 0.021529674530029297s\n",
      "411/589, train_loss: 217.4713, time taken: 0.0203549861907959s\n",
      "412/589, train_loss: 225.5982, time taken: 0.01985788345336914s\n",
      "413/589, train_loss: 239.6397, time taken: 0.021851778030395508s\n",
      "414/589, train_loss: 221.8533, time taken: 0.020470857620239258s\n",
      "415/589, train_loss: 280.0364, time taken: 0.021781444549560547s\n",
      "416/589, train_loss: 326.8693, time taken: 0.023405075073242188s\n",
      "417/589, train_loss: 301.1389, time taken: 0.02223682403564453s\n",
      "418/589, train_loss: 193.0352, time taken: 0.020429611206054688s\n",
      "419/589, train_loss: 192.6503, time taken: 0.02299785614013672s\n",
      "420/589, train_loss: 211.7444, time taken: 0.02313685417175293s\n",
      "421/589, train_loss: 291.6599, time taken: 0.02003955841064453s\n",
      "422/589, train_loss: 202.5115, time taken: 0.02306675910949707s\n",
      "423/589, train_loss: 167.1031, time taken: 0.020229816436767578s\n",
      "424/589, train_loss: 235.9372, time taken: 0.02190089225769043s\n",
      "425/589, train_loss: 216.6111, time taken: 0.021281003952026367s\n",
      "426/589, train_loss: 189.3165, time taken: 0.02285909652709961s\n",
      "427/589, train_loss: 200.8858, time taken: 0.02072882652282715s\n",
      "428/589, train_loss: 243.1933, time taken: 0.021598339080810547s\n",
      "429/589, train_loss: 276.5577, time taken: 0.020386219024658203s\n",
      "430/589, train_loss: 269.2039, time taken: 0.020555496215820312s\n",
      "431/589, train_loss: 241.0240, time taken: 0.023393869400024414s\n",
      "432/589, train_loss: 222.4055, time taken: 0.023288726806640625s\n",
      "433/589, train_loss: 252.6875, time taken: 0.02017354965209961s\n",
      "434/589, train_loss: 261.2238, time taken: 0.021375656127929688s\n",
      "435/589, train_loss: 198.9387, time taken: 0.02283501625061035s\n",
      "436/589, train_loss: 175.0932, time taken: 0.022701263427734375s\n",
      "437/589, train_loss: 170.5307, time taken: 0.02195429801940918s\n",
      "438/589, train_loss: 296.6655, time taken: 0.02322077751159668s\n",
      "439/589, train_loss: 266.5131, time taken: 0.022960186004638672s\n",
      "440/589, train_loss: 182.7958, time taken: 0.02001500129699707s\n",
      "441/589, train_loss: 261.4332, time taken: 0.01963329315185547s\n",
      "442/589, train_loss: 198.4716, time taken: 0.02029132843017578s\n",
      "443/589, train_loss: 205.7596, time taken: 0.020281553268432617s\n",
      "444/589, train_loss: 214.6317, time taken: 0.023096561431884766s\n",
      "445/589, train_loss: 271.4292, time taken: 0.023285865783691406s\n",
      "446/589, train_loss: 252.2237, time taken: 0.02287888526916504s\n",
      "447/589, train_loss: 229.9077, time taken: 0.02278900146484375s\n",
      "448/589, train_loss: 176.4878, time taken: 0.022053956985473633s\n",
      "449/589, train_loss: 219.6377, time taken: 0.02017498016357422s\n",
      "450/589, train_loss: 302.6549, time taken: 0.020206212997436523s\n",
      "451/589, train_loss: 192.2359, time taken: 0.020284652709960938s\n",
      "452/589, train_loss: 208.4885, time taken: 0.01996779441833496s\n",
      "453/589, train_loss: 278.0592, time taken: 0.022992849349975586s\n",
      "454/589, train_loss: 222.7247, time taken: 0.02285289764404297s\n",
      "455/589, train_loss: 197.6617, time taken: 0.022795915603637695s\n",
      "456/589, train_loss: 201.6999, time taken: 0.020189523696899414s\n",
      "457/589, train_loss: 280.3152, time taken: 0.02008533477783203s\n",
      "458/589, train_loss: 242.8113, time taken: 0.02021503448486328s\n",
      "459/589, train_loss: 233.6317, time taken: 0.020231008529663086s\n",
      "460/589, train_loss: 181.4021, time taken: 0.020144224166870117s\n",
      "461/589, train_loss: 224.6785, time taken: 0.020398378372192383s\n",
      "462/589, train_loss: 196.2156, time taken: 0.020256519317626953s\n",
      "463/589, train_loss: 176.3892, time taken: 0.02243828773498535s\n",
      "464/589, train_loss: 313.7906, time taken: 0.01996612548828125s\n",
      "465/589, train_loss: 229.4632, time taken: 0.020079612731933594s\n",
      "466/589, train_loss: 219.0008, time taken: 0.020008325576782227s\n",
      "467/589, train_loss: 202.4250, time taken: 0.02020859718322754s\n",
      "468/589, train_loss: 204.9243, time taken: 0.020064830780029297s\n",
      "469/589, train_loss: 201.2979, time taken: 0.02278280258178711s\n",
      "470/589, train_loss: 190.2849, time taken: 0.02262425422668457s\n",
      "471/589, train_loss: 278.0005, time taken: 0.02050495147705078s\n",
      "472/589, train_loss: 227.4736, time taken: 0.020102262496948242s\n",
      "473/589, train_loss: 219.5836, time taken: 0.020505428314208984s\n",
      "474/589, train_loss: 206.3140, time taken: 0.02313518524169922s\n",
      "475/589, train_loss: 224.0743, time taken: 0.023466825485229492s\n",
      "476/589, train_loss: 244.7593, time taken: 0.02034759521484375s\n",
      "477/589, train_loss: 332.0283, time taken: 0.02038741111755371s\n",
      "478/589, train_loss: 286.5965, time taken: 0.020347118377685547s\n",
      "479/589, train_loss: 279.4656, time taken: 0.02039647102355957s\n",
      "480/589, train_loss: 219.2534, time taken: 0.02020859718322754s\n",
      "481/589, train_loss: 246.7565, time taken: 0.02049541473388672s\n",
      "482/589, train_loss: 212.3031, time taken: 0.0229947566986084s\n",
      "483/589, train_loss: 195.7231, time taken: 0.02034759521484375s\n",
      "484/589, train_loss: 226.6308, time taken: 0.02309703826904297s\n",
      "485/589, train_loss: 188.9987, time taken: 0.022916078567504883s\n",
      "486/589, train_loss: 166.4002, time taken: 0.023083209991455078s\n",
      "487/589, train_loss: 222.7937, time taken: 0.022502422332763672s\n",
      "488/589, train_loss: 251.8595, time taken: 0.02036309242248535s\n",
      "489/589, train_loss: 232.5209, time taken: 0.020459890365600586s\n",
      "490/589, train_loss: 228.5954, time taken: 0.022768735885620117s\n",
      "491/589, train_loss: 245.7503, time taken: 0.022740840911865234s\n",
      "492/589, train_loss: 211.2525, time taken: 0.023058176040649414s\n",
      "493/589, train_loss: 304.6788, time taken: 0.0226595401763916s\n",
      "494/589, train_loss: 184.9407, time taken: 0.020228862762451172s\n",
      "495/589, train_loss: 288.5861, time taken: 0.020379304885864258s\n",
      "496/589, train_loss: 176.3097, time taken: 0.020132064819335938s\n",
      "497/589, train_loss: 198.4798, time taken: 0.020369291305541992s\n",
      "498/589, train_loss: 214.8067, time taken: 0.020702362060546875s\n",
      "499/589, train_loss: 224.6103, time taken: 0.02020740509033203s\n",
      "500/589, train_loss: 259.0708, time taken: 0.020302772521972656s\n",
      "501/589, train_loss: 273.2205, time taken: 0.020097970962524414s\n",
      "502/589, train_loss: 263.3197, time taken: 0.020652294158935547s\n",
      "503/589, train_loss: 187.9255, time taken: 0.023408889770507812s\n",
      "504/589, train_loss: 221.1566, time taken: 0.020586252212524414s\n",
      "505/589, train_loss: 251.6201, time taken: 0.020160913467407227s\n",
      "506/589, train_loss: 259.8850, time taken: 0.022592544555664062s\n",
      "507/589, train_loss: 275.8984, time taken: 0.022730350494384766s\n",
      "508/589, train_loss: 234.7056, time taken: 0.020363569259643555s\n",
      "509/589, train_loss: 220.8040, time taken: 0.019911527633666992s\n",
      "510/589, train_loss: 265.7095, time taken: 0.020443201065063477s\n",
      "511/589, train_loss: 295.0890, time taken: 0.02006363868713379s\n",
      "512/589, train_loss: 222.7557, time taken: 0.023182153701782227s\n",
      "513/589, train_loss: 243.4481, time taken: 0.023260831832885742s\n",
      "514/589, train_loss: 154.7532, time taken: 0.02281641960144043s\n",
      "515/589, train_loss: 260.1041, time taken: 0.022887706756591797s\n",
      "516/589, train_loss: 236.8503, time taken: 0.022904157638549805s\n",
      "517/589, train_loss: 206.8811, time taken: 0.022026538848876953s\n",
      "518/589, train_loss: 292.5974, time taken: 0.020441293716430664s\n",
      "519/589, train_loss: 238.4754, time taken: 0.02008819580078125s\n",
      "520/589, train_loss: 237.9976, time taken: 0.020661115646362305s\n",
      "521/589, train_loss: 221.5849, time taken: 0.020000934600830078s\n",
      "522/589, train_loss: 199.8351, time taken: 0.020147323608398438s\n",
      "523/589, train_loss: 192.5386, time taken: 0.02286505699157715s\n",
      "524/589, train_loss: 234.8620, time taken: 0.020302772521972656s\n",
      "525/589, train_loss: 220.4486, time taken: 0.022615671157836914s\n",
      "526/589, train_loss: 228.8839, time taken: 0.021319150924682617s\n",
      "527/589, train_loss: 267.9624, time taken: 0.022835493087768555s\n",
      "528/589, train_loss: 221.7949, time taken: 0.020307064056396484s\n",
      "529/589, train_loss: 225.0843, time taken: 0.02018141746520996s\n",
      "530/589, train_loss: 277.2910, time taken: 0.0205075740814209s\n",
      "531/589, train_loss: 211.4813, time taken: 0.02318859100341797s\n",
      "532/589, train_loss: 222.6260, time taken: 0.022971391677856445s\n",
      "533/589, train_loss: 174.5031, time taken: 0.020291566848754883s\n",
      "534/589, train_loss: 228.8407, time taken: 0.02017354965209961s\n",
      "535/589, train_loss: 359.9826, time taken: 0.02001810073852539s\n",
      "536/589, train_loss: 266.1200, time taken: 0.02029561996459961s\n",
      "537/589, train_loss: 260.0334, time taken: 0.020200014114379883s\n",
      "538/589, train_loss: 289.6609, time taken: 0.019847393035888672s\n",
      "539/589, train_loss: 263.8742, time taken: 0.02235555648803711s\n",
      "540/589, train_loss: 197.9245, time taken: 0.022553682327270508s\n",
      "541/589, train_loss: 224.2687, time taken: 0.02277851104736328s\n",
      "542/589, train_loss: 235.0426, time taken: 0.02276301383972168s\n",
      "543/589, train_loss: 271.0960, time taken: 0.02268075942993164s\n",
      "544/589, train_loss: 286.0249, time taken: 0.022810935974121094s\n",
      "545/589, train_loss: 229.8474, time taken: 0.020574092864990234s\n",
      "546/589, train_loss: 299.0720, time taken: 0.022777080535888672s\n",
      "547/589, train_loss: 266.9933, time taken: 0.02037787437438965s\n",
      "548/589, train_loss: 241.9139, time taken: 0.02331256866455078s\n",
      "549/589, train_loss: 271.2582, time taken: 0.0231170654296875s\n",
      "550/589, train_loss: 254.6771, time taken: 0.020618200302124023s\n",
      "551/589, train_loss: 251.3573, time taken: 0.02291584014892578s\n",
      "552/589, train_loss: 195.5183, time taken: 0.020366191864013672s\n",
      "553/589, train_loss: 270.4283, time taken: 0.0206298828125s\n",
      "554/589, train_loss: 172.3262, time taken: 0.022590160369873047s\n",
      "555/589, train_loss: 207.6415, time taken: 0.02291131019592285s\n",
      "556/589, train_loss: 169.5776, time taken: 0.022719860076904297s\n",
      "557/589, train_loss: 212.8653, time taken: 0.02252984046936035s\n",
      "558/589, train_loss: 186.7155, time taken: 0.02290797233581543s\n",
      "559/589, train_loss: 244.5315, time taken: 0.02295398712158203s\n",
      "560/589, train_loss: 193.0480, time taken: 0.022871732711791992s\n",
      "561/589, train_loss: 315.2844, time taken: 0.02039504051208496s\n",
      "562/589, train_loss: 251.9739, time taken: 0.023231983184814453s\n",
      "563/589, train_loss: 272.3948, time taken: 0.022724151611328125s\n",
      "564/589, train_loss: 226.1004, time taken: 0.022510766983032227s\n",
      "565/589, train_loss: 207.7958, time taken: 0.023018598556518555s\n",
      "566/589, train_loss: 202.9639, time taken: 0.023290395736694336s\n",
      "567/589, train_loss: 169.4402, time taken: 0.023030757904052734s\n",
      "568/589, train_loss: 279.9029, time taken: 0.02273869514465332s\n",
      "569/589, train_loss: 210.6736, time taken: 0.022556304931640625s\n",
      "570/589, train_loss: 221.5509, time taken: 0.022567272186279297s\n",
      "571/589, train_loss: 245.3907, time taken: 0.02298712730407715s\n",
      "572/589, train_loss: 234.8754, time taken: 0.02280735969543457s\n",
      "573/589, train_loss: 180.8718, time taken: 0.02305459976196289s\n",
      "574/589, train_loss: 328.0629, time taken: 0.02301192283630371s\n",
      "575/589, train_loss: 214.6943, time taken: 0.02361583709716797s\n",
      "576/589, train_loss: 197.7910, time taken: 0.023453474044799805s\n",
      "577/589, train_loss: 240.5699, time taken: 0.021283388137817383s\n",
      "578/589, train_loss: 303.3560, time taken: 0.02019476890563965s\n",
      "579/589, train_loss: 198.6328, time taken: 0.02013993263244629s\n",
      "580/589, train_loss: 205.2733, time taken: 0.020175695419311523s\n",
      "581/589, train_loss: 262.9537, time taken: 0.02236771583557129s\n",
      "582/589, train_loss: 263.3110, time taken: 0.02225208282470703s\n",
      "583/589, train_loss: 266.3728, time taken: 0.022789478302001953s\n",
      "584/589, train_loss: 242.3845, time taken: 0.020292043685913086s\n",
      "585/589, train_loss: 286.9058, time taken: 0.01996326446533203s\n",
      "586/589, train_loss: 181.2744, time taken: 0.0202791690826416s\n",
      "587/589, train_loss: 226.2040, time taken: 0.019868850708007812s\n",
      "588/589, train_loss: 185.2478, time taken: 0.020548343658447266s\n",
      "589/589, train_loss: 231.9885, time taken: 0.020735740661621094s\n",
      "590/589, train_loss: 192.0537, time taken: 0.011037588119506836s\n",
      "epoch 2 average loss: 236.2606\n",
      "----------\n",
      "epoch 3/4\n",
      "1/589, train_loss: 276.2717, time taken: 0.022469043731689453s\n",
      "2/589, train_loss: 265.3513, time taken: 0.020221710205078125s\n",
      "3/589, train_loss: 220.9418, time taken: 0.020301342010498047s\n",
      "4/589, train_loss: 304.4142, time taken: 0.021120071411132812s\n",
      "5/589, train_loss: 297.0587, time taken: 0.020371675491333008s\n",
      "6/589, train_loss: 217.4641, time taken: 0.02286696434020996s\n",
      "7/589, train_loss: 290.1122, time taken: 0.022643566131591797s\n",
      "8/589, train_loss: 272.8450, time taken: 0.02249455451965332s\n",
      "9/589, train_loss: 251.8897, time taken: 0.02297353744506836s\n",
      "10/589, train_loss: 214.0045, time taken: 0.020114898681640625s\n",
      "11/589, train_loss: 245.0417, time taken: 0.01989912986755371s\n",
      "12/589, train_loss: 217.8940, time taken: 0.022727012634277344s\n",
      "13/589, train_loss: 212.1069, time taken: 0.023020029067993164s\n",
      "14/589, train_loss: 221.5053, time taken: 0.020702123641967773s\n",
      "15/589, train_loss: 174.0334, time taken: 0.02041006088256836s\n",
      "16/589, train_loss: 274.3654, time taken: 0.02141594886779785s\n",
      "17/589, train_loss: 303.3221, time taken: 0.021892547607421875s\n",
      "18/589, train_loss: 251.7766, time taken: 0.022359609603881836s\n",
      "19/589, train_loss: 164.6541, time taken: 0.020470380783081055s\n",
      "20/589, train_loss: 186.7971, time taken: 0.020026683807373047s\n",
      "21/589, train_loss: 208.5309, time taken: 0.020307064056396484s\n",
      "22/589, train_loss: 209.1360, time taken: 0.02081131935119629s\n",
      "23/589, train_loss: 172.3063, time taken: 0.02021932601928711s\n",
      "24/589, train_loss: 226.8913, time taken: 0.02307915687561035s\n",
      "25/589, train_loss: 233.1696, time taken: 0.023131132125854492s\n",
      "26/589, train_loss: 225.7739, time taken: 0.023394107818603516s\n",
      "27/589, train_loss: 185.0419, time taken: 0.020560026168823242s\n",
      "28/589, train_loss: 212.3561, time taken: 0.020763874053955078s\n",
      "29/589, train_loss: 332.5214, time taken: 0.020682096481323242s\n",
      "30/589, train_loss: 229.8950, time taken: 0.020149946212768555s\n",
      "31/589, train_loss: 235.6627, time taken: 0.02110910415649414s\n",
      "32/589, train_loss: 271.6162, time taken: 0.023113250732421875s\n",
      "33/589, train_loss: 210.1005, time taken: 0.023084640502929688s\n",
      "34/589, train_loss: 312.9311, time taken: 0.023034095764160156s\n",
      "35/589, train_loss: 205.1282, time taken: 0.02205514907836914s\n",
      "36/589, train_loss: 202.9479, time taken: 0.021472692489624023s\n",
      "37/589, train_loss: 249.3194, time taken: 0.02107071876525879s\n",
      "38/589, train_loss: 261.7282, time taken: 0.02122664451599121s\n",
      "39/589, train_loss: 291.8680, time taken: 0.021244287490844727s\n",
      "40/589, train_loss: 314.8089, time taken: 0.020487070083618164s\n",
      "41/589, train_loss: 242.1140, time taken: 0.02105116844177246s\n",
      "42/589, train_loss: 251.6261, time taken: 0.022038936614990234s\n",
      "43/589, train_loss: 201.1631, time taken: 0.023746967315673828s\n",
      "44/589, train_loss: 187.9064, time taken: 0.0228273868560791s\n",
      "45/589, train_loss: 231.5432, time taken: 0.02274036407470703s\n",
      "46/589, train_loss: 218.7198, time taken: 0.021818876266479492s\n",
      "47/589, train_loss: 234.2801, time taken: 0.023664474487304688s\n",
      "48/589, train_loss: 213.4998, time taken: 0.020373821258544922s\n",
      "49/589, train_loss: 230.3717, time taken: 0.020609140396118164s\n",
      "50/589, train_loss: 282.4235, time taken: 0.02307438850402832s\n",
      "51/589, train_loss: 187.2692, time taken: 0.020491361618041992s\n",
      "52/589, train_loss: 250.7580, time taken: 0.022917747497558594s\n",
      "53/589, train_loss: 195.7202, time taken: 0.02322864532470703s\n",
      "54/589, train_loss: 282.0259, time taken: 0.023406505584716797s\n",
      "55/589, train_loss: 245.4669, time taken: 0.022220134735107422s\n",
      "56/589, train_loss: 196.4449, time taken: 0.02328944206237793s\n",
      "57/589, train_loss: 188.4343, time taken: 0.023021936416625977s\n",
      "58/589, train_loss: 227.2661, time taken: 0.023033857345581055s\n",
      "59/589, train_loss: 216.3093, time taken: 0.022578954696655273s\n",
      "60/589, train_loss: 210.1245, time taken: 0.023024559020996094s\n",
      "61/589, train_loss: 289.0088, time taken: 0.020267009735107422s\n",
      "62/589, train_loss: 226.9733, time taken: 0.022411823272705078s\n",
      "63/589, train_loss: 137.7846, time taken: 0.020456552505493164s\n",
      "64/589, train_loss: 251.4790, time taken: 0.020085573196411133s\n",
      "65/589, train_loss: 213.7594, time taken: 0.02292609214782715s\n",
      "66/589, train_loss: 166.4319, time taken: 0.022022485733032227s\n",
      "67/589, train_loss: 189.1735, time taken: 0.022322893142700195s\n",
      "68/589, train_loss: 257.4258, time taken: 0.020875215530395508s\n",
      "69/589, train_loss: 242.9130, time taken: 0.02330613136291504s\n",
      "70/589, train_loss: 199.9566, time taken: 0.023082494735717773s\n",
      "71/589, train_loss: 212.1743, time taken: 0.020522594451904297s\n",
      "72/589, train_loss: 211.5116, time taken: 0.02314305305480957s\n",
      "73/589, train_loss: 255.3915, time taken: 0.0224912166595459s\n",
      "74/589, train_loss: 184.0976, time taken: 0.023250341415405273s\n",
      "75/589, train_loss: 251.0159, time taken: 0.023026704788208008s\n",
      "76/589, train_loss: 166.6595, time taken: 0.020303010940551758s\n",
      "77/589, train_loss: 223.2187, time taken: 0.021000385284423828s\n",
      "78/589, train_loss: 156.7683, time taken: 0.02023911476135254s\n",
      "79/589, train_loss: 199.8551, time taken: 0.021463632583618164s\n",
      "80/589, train_loss: 201.4661, time taken: 0.020543575286865234s\n",
      "81/589, train_loss: 239.1771, time taken: 0.022365570068359375s\n",
      "82/589, train_loss: 213.5574, time taken: 0.021617650985717773s\n",
      "83/589, train_loss: 248.9747, time taken: 0.021511554718017578s\n",
      "84/589, train_loss: 186.5140, time taken: 0.020929336547851562s\n",
      "85/589, train_loss: 268.6993, time taken: 0.023279190063476562s\n",
      "86/589, train_loss: 242.2575, time taken: 0.019948244094848633s\n",
      "87/589, train_loss: 270.0982, time taken: 0.022565841674804688s\n",
      "88/589, train_loss: 226.4534, time taken: 0.02331233024597168s\n",
      "89/589, train_loss: 308.3391, time taken: 0.022918701171875s\n",
      "90/589, train_loss: 255.2032, time taken: 0.020184993743896484s\n",
      "91/589, train_loss: 231.9820, time taken: 0.022516489028930664s\n",
      "92/589, train_loss: 240.6354, time taken: 0.022894620895385742s\n",
      "93/589, train_loss: 278.5368, time taken: 0.020285844802856445s\n",
      "94/589, train_loss: 222.3385, time taken: 0.022017717361450195s\n",
      "95/589, train_loss: 213.5002, time taken: 0.02352619171142578s\n",
      "96/589, train_loss: 256.7773, time taken: 0.025046586990356445s\n",
      "97/589, train_loss: 197.6297, time taken: 0.022513628005981445s\n",
      "98/589, train_loss: 227.3192, time taken: 0.027142047882080078s\n",
      "99/589, train_loss: 209.8049, time taken: 0.020626068115234375s\n",
      "100/589, train_loss: 258.3247, time taken: 0.02188277244567871s\n",
      "101/589, train_loss: 178.5643, time taken: 0.022743940353393555s\n",
      "102/589, train_loss: 272.9577, time taken: 0.0229339599609375s\n",
      "103/589, train_loss: 263.3799, time taken: 0.023033857345581055s\n",
      "104/589, train_loss: 246.5421, time taken: 0.020198345184326172s\n",
      "105/589, train_loss: 237.3849, time taken: 0.021718263626098633s\n",
      "106/589, train_loss: 263.1545, time taken: 0.022942304611206055s\n",
      "107/589, train_loss: 204.2753, time taken: 0.021299362182617188s\n",
      "108/589, train_loss: 228.1269, time taken: 0.020656108856201172s\n",
      "109/589, train_loss: 186.5198, time taken: 0.020551681518554688s\n",
      "110/589, train_loss: 227.7051, time taken: 0.021732807159423828s\n",
      "111/589, train_loss: 161.6552, time taken: 0.022693872451782227s\n",
      "112/589, train_loss: 248.8824, time taken: 0.022083759307861328s\n",
      "113/589, train_loss: 227.7583, time taken: 0.020331621170043945s\n",
      "114/589, train_loss: 278.7466, time taken: 0.022552967071533203s\n",
      "115/589, train_loss: 234.6774, time taken: 0.02332615852355957s\n",
      "116/589, train_loss: 239.7146, time taken: 0.020215988159179688s\n",
      "117/589, train_loss: 249.1067, time taken: 0.021853923797607422s\n",
      "118/589, train_loss: 235.3299, time taken: 0.021197080612182617s\n",
      "119/589, train_loss: 174.2048, time taken: 0.023227453231811523s\n",
      "120/589, train_loss: 261.5885, time taken: 0.023227930068969727s\n",
      "121/589, train_loss: 171.4429, time taken: 0.020208358764648438s\n",
      "122/589, train_loss: 219.2096, time taken: 0.020191192626953125s\n",
      "123/589, train_loss: 250.0651, time taken: 0.02324080467224121s\n",
      "124/589, train_loss: 271.2156, time taken: 0.020236730575561523s\n",
      "125/589, train_loss: 290.2523, time taken: 0.023263931274414062s\n",
      "126/589, train_loss: 292.4270, time taken: 0.02293086051940918s\n",
      "127/589, train_loss: 222.4115, time taken: 0.023244380950927734s\n",
      "128/589, train_loss: 315.0495, time taken: 0.022947311401367188s\n",
      "129/589, train_loss: 222.6618, time taken: 0.02240443229675293s\n",
      "130/589, train_loss: 227.5482, time taken: 0.0201871395111084s\n",
      "131/589, train_loss: 225.9406, time taken: 0.02295088768005371s\n",
      "132/589, train_loss: 205.9373, time taken: 0.022586584091186523s\n",
      "133/589, train_loss: 247.2157, time taken: 0.022815942764282227s\n",
      "134/589, train_loss: 235.6235, time taken: 0.02288985252380371s\n",
      "135/589, train_loss: 250.2721, time taken: 0.020477294921875s\n",
      "136/589, train_loss: 132.5469, time taken: 0.020032644271850586s\n",
      "137/589, train_loss: 274.8851, time taken: 0.02308368682861328s\n",
      "138/589, train_loss: 275.8931, time taken: 0.020230770111083984s\n",
      "139/589, train_loss: 223.1053, time taken: 0.02304983139038086s\n",
      "140/589, train_loss: 236.1418, time taken: 0.023026466369628906s\n",
      "141/589, train_loss: 286.6440, time taken: 0.0228726863861084s\n",
      "142/589, train_loss: 203.9064, time taken: 0.022626161575317383s\n",
      "143/589, train_loss: 233.8983, time taken: 0.022696256637573242s\n",
      "144/589, train_loss: 205.3093, time taken: 0.022553443908691406s\n",
      "145/589, train_loss: 193.0038, time taken: 0.022980690002441406s\n",
      "146/589, train_loss: 241.9856, time taken: 0.02303028106689453s\n",
      "147/589, train_loss: 302.9484, time taken: 0.020164966583251953s\n",
      "148/589, train_loss: 221.1478, time taken: 0.019840240478515625s\n",
      "149/589, train_loss: 221.7005, time taken: 0.020223617553710938s\n",
      "150/589, train_loss: 250.2611, time taken: 0.020249128341674805s\n",
      "151/589, train_loss: 203.6537, time taken: 0.023005962371826172s\n",
      "152/589, train_loss: 269.7607, time taken: 0.020444393157958984s\n",
      "153/589, train_loss: 228.1839, time taken: 0.022255659103393555s\n",
      "154/589, train_loss: 153.7291, time taken: 0.022789955139160156s\n",
      "155/589, train_loss: 294.5822, time taken: 0.022650957107543945s\n",
      "156/589, train_loss: 257.3500, time taken: 0.02273845672607422s\n",
      "157/589, train_loss: 234.2221, time taken: 0.022366762161254883s\n",
      "158/589, train_loss: 268.7592, time taken: 0.022429943084716797s\n",
      "159/589, train_loss: 281.0582, time taken: 0.02246999740600586s\n",
      "160/589, train_loss: 265.7083, time taken: 0.022582054138183594s\n",
      "161/589, train_loss: 216.6974, time taken: 0.022652387619018555s\n",
      "162/589, train_loss: 195.8431, time taken: 0.02027106285095215s\n",
      "163/589, train_loss: 277.2328, time taken: 0.02056598663330078s\n",
      "164/589, train_loss: 251.7186, time taken: 0.023023366928100586s\n",
      "165/589, train_loss: 238.2901, time taken: 0.02313399314880371s\n",
      "166/589, train_loss: 230.9675, time taken: 0.02185797691345215s\n",
      "167/589, train_loss: 228.1172, time taken: 0.022548198699951172s\n",
      "168/589, train_loss: 230.8640, time taken: 0.02198052406311035s\n",
      "169/589, train_loss: 278.3763, time taken: 0.022192716598510742s\n",
      "170/589, train_loss: 214.7862, time taken: 0.02215266227722168s\n",
      "171/589, train_loss: 292.9797, time taken: 0.02293086051940918s\n",
      "172/589, train_loss: 221.1778, time taken: 0.023125886917114258s\n",
      "173/589, train_loss: 242.7889, time taken: 0.022869586944580078s\n",
      "174/589, train_loss: 220.8073, time taken: 0.02270030975341797s\n",
      "175/589, train_loss: 257.6554, time taken: 0.023138046264648438s\n",
      "176/589, train_loss: 232.6879, time taken: 0.02297663688659668s\n",
      "177/589, train_loss: 258.0771, time taken: 0.02282261848449707s\n",
      "178/589, train_loss: 211.9513, time taken: 0.022426366806030273s\n",
      "179/589, train_loss: 209.4485, time taken: 0.022904396057128906s\n",
      "180/589, train_loss: 154.0069, time taken: 0.0228884220123291s\n",
      "181/589, train_loss: 260.1084, time taken: 0.023008346557617188s\n",
      "182/589, train_loss: 220.2729, time taken: 0.02320551872253418s\n",
      "183/589, train_loss: 174.3295, time taken: 0.022935867309570312s\n",
      "184/589, train_loss: 173.5292, time taken: 0.022907495498657227s\n",
      "185/589, train_loss: 202.6552, time taken: 0.022797584533691406s\n",
      "186/589, train_loss: 208.7409, time taken: 0.023041486740112305s\n",
      "187/589, train_loss: 203.4760, time taken: 0.023125648498535156s\n",
      "188/589, train_loss: 280.8581, time taken: 0.021573543548583984s\n",
      "189/589, train_loss: 204.1984, time taken: 0.022549867630004883s\n",
      "190/589, train_loss: 279.9185, time taken: 0.022588491439819336s\n",
      "191/589, train_loss: 232.4766, time taken: 0.020369291305541992s\n",
      "192/589, train_loss: 235.9705, time taken: 0.020519018173217773s\n",
      "193/589, train_loss: 248.8066, time taken: 0.020863771438598633s\n",
      "194/589, train_loss: 265.7753, time taken: 0.02014636993408203s\n",
      "195/589, train_loss: 221.0204, time taken: 0.020357608795166016s\n",
      "196/589, train_loss: 273.6443, time taken: 0.020137786865234375s\n",
      "197/589, train_loss: 165.2480, time taken: 0.02294182777404785s\n",
      "198/589, train_loss: 242.6529, time taken: 0.020261764526367188s\n",
      "199/589, train_loss: 353.4140, time taken: 0.02276921272277832s\n",
      "200/589, train_loss: 212.0292, time taken: 0.020178794860839844s\n",
      "201/589, train_loss: 224.5179, time taken: 0.022677183151245117s\n",
      "202/589, train_loss: 235.9154, time taken: 0.022803068161010742s\n",
      "203/589, train_loss: 169.2219, time taken: 0.023122310638427734s\n",
      "204/589, train_loss: 218.5671, time taken: 0.020198345184326172s\n",
      "205/589, train_loss: 275.9406, time taken: 0.02325272560119629s\n",
      "206/589, train_loss: 290.9940, time taken: 0.020092487335205078s\n",
      "207/589, train_loss: 208.1668, time taken: 0.020170927047729492s\n",
      "208/589, train_loss: 168.4967, time taken: 0.019900798797607422s\n",
      "209/589, train_loss: 254.8046, time taken: 0.022591114044189453s\n",
      "210/589, train_loss: 239.8555, time taken: 0.020216703414916992s\n",
      "211/589, train_loss: 176.9736, time taken: 0.023022174835205078s\n",
      "212/589, train_loss: 185.0574, time taken: 0.022349119186401367s\n",
      "213/589, train_loss: 218.8631, time taken: 0.022459983825683594s\n",
      "214/589, train_loss: 220.0579, time taken: 0.02046942710876465s\n",
      "215/589, train_loss: 214.5557, time taken: 0.020787715911865234s\n",
      "216/589, train_loss: 242.4539, time taken: 0.020650625228881836s\n",
      "217/589, train_loss: 205.9807, time taken: 0.022807598114013672s\n",
      "218/589, train_loss: 190.8094, time taken: 0.022918701171875s\n",
      "219/589, train_loss: 159.2729, time taken: 0.023005962371826172s\n",
      "220/589, train_loss: 230.4877, time taken: 0.02306652069091797s\n",
      "221/589, train_loss: 214.6579, time taken: 0.022914409637451172s\n",
      "222/589, train_loss: 374.2428, time taken: 0.022901058197021484s\n",
      "223/589, train_loss: 201.6747, time taken: 0.02301502227783203s\n",
      "224/589, train_loss: 213.8011, time taken: 0.023040056228637695s\n",
      "225/589, train_loss: 213.1515, time taken: 0.023157835006713867s\n",
      "226/589, train_loss: 264.8624, time taken: 0.023122549057006836s\n",
      "227/589, train_loss: 249.6799, time taken: 0.020403146743774414s\n",
      "228/589, train_loss: 232.6200, time taken: 0.02284383773803711s\n",
      "229/589, train_loss: 217.6124, time taken: 0.020411252975463867s\n",
      "230/589, train_loss: 223.2855, time taken: 0.02037811279296875s\n",
      "231/589, train_loss: 316.1321, time taken: 0.023033618927001953s\n",
      "232/589, train_loss: 229.3181, time taken: 0.020361900329589844s\n",
      "233/589, train_loss: 148.3805, time taken: 0.02088761329650879s\n",
      "234/589, train_loss: 224.9948, time taken: 0.020368337631225586s\n",
      "235/589, train_loss: 274.8658, time taken: 0.02031731605529785s\n",
      "236/589, train_loss: 247.6694, time taken: 0.022682905197143555s\n",
      "237/589, train_loss: 251.6090, time taken: 0.022728919982910156s\n",
      "238/589, train_loss: 199.7339, time taken: 0.022521257400512695s\n",
      "239/589, train_loss: 259.6298, time taken: 0.022684574127197266s\n",
      "240/589, train_loss: 229.2861, time taken: 0.022724390029907227s\n",
      "241/589, train_loss: 281.3674, time taken: 0.022589683532714844s\n",
      "242/589, train_loss: 261.7014, time taken: 0.022230863571166992s\n",
      "243/589, train_loss: 263.3621, time taken: 0.020589590072631836s\n",
      "244/589, train_loss: 188.4922, time taken: 0.022816181182861328s\n",
      "245/589, train_loss: 252.5715, time taken: 0.022779464721679688s\n",
      "246/589, train_loss: 171.2094, time taken: 0.022845983505249023s\n",
      "247/589, train_loss: 215.3554, time taken: 0.02286362648010254s\n",
      "248/589, train_loss: 199.3436, time taken: 0.020119428634643555s\n",
      "249/589, train_loss: 203.0724, time taken: 0.023279666900634766s\n",
      "250/589, train_loss: 238.9119, time taken: 0.021197080612182617s\n",
      "251/589, train_loss: 219.2423, time taken: 0.023204565048217773s\n",
      "252/589, train_loss: 251.4013, time taken: 0.020536184310913086s\n",
      "253/589, train_loss: 213.0425, time taken: 0.020978212356567383s\n",
      "254/589, train_loss: 215.4026, time taken: 0.02122330665588379s\n",
      "255/589, train_loss: 351.8748, time taken: 0.022067785263061523s\n",
      "256/589, train_loss: 178.9541, time taken: 0.02268195152282715s\n",
      "257/589, train_loss: 176.6580, time taken: 0.022945880889892578s\n",
      "258/589, train_loss: 256.1835, time taken: 0.022464752197265625s\n",
      "259/589, train_loss: 247.1550, time taken: 0.0202176570892334s\n",
      "260/589, train_loss: 254.8486, time taken: 0.020196914672851562s\n",
      "261/589, train_loss: 210.1439, time taken: 0.02271246910095215s\n",
      "262/589, train_loss: 205.7234, time taken: 0.022429466247558594s\n",
      "263/589, train_loss: 199.5299, time taken: 0.02059769630432129s\n",
      "264/589, train_loss: 199.2527, time taken: 0.020292997360229492s\n",
      "265/589, train_loss: 243.2130, time taken: 0.020297765731811523s\n",
      "266/589, train_loss: 269.0322, time taken: 0.02068305015563965s\n",
      "267/589, train_loss: 249.7003, time taken: 0.020145416259765625s\n",
      "268/589, train_loss: 228.8056, time taken: 0.020890474319458008s\n",
      "269/589, train_loss: 243.9501, time taken: 0.0225832462310791s\n",
      "270/589, train_loss: 183.1569, time taken: 0.022854089736938477s\n",
      "271/589, train_loss: 240.4901, time taken: 0.020273685455322266s\n",
      "272/589, train_loss: 205.8317, time taken: 0.020200014114379883s\n",
      "273/589, train_loss: 232.3384, time taken: 0.022817134857177734s\n",
      "274/589, train_loss: 263.0377, time taken: 0.022722244262695312s\n",
      "275/589, train_loss: 243.8879, time taken: 0.02286386489868164s\n",
      "276/589, train_loss: 178.7973, time taken: 0.022403478622436523s\n",
      "277/589, train_loss: 237.4228, time taken: 0.023273944854736328s\n",
      "278/589, train_loss: 308.3588, time taken: 0.022878170013427734s\n",
      "279/589, train_loss: 350.9110, time taken: 0.023035526275634766s\n",
      "280/589, train_loss: 209.2533, time taken: 0.022759437561035156s\n",
      "281/589, train_loss: 224.2590, time taken: 0.020298004150390625s\n",
      "282/589, train_loss: 203.9333, time taken: 0.020025968551635742s\n",
      "283/589, train_loss: 224.6847, time taken: 0.020216941833496094s\n",
      "284/589, train_loss: 243.4649, time taken: 0.020155668258666992s\n",
      "285/589, train_loss: 197.5854, time taken: 0.020891904830932617s\n",
      "286/589, train_loss: 206.7753, time taken: 0.022991657257080078s\n",
      "287/589, train_loss: 286.2246, time taken: 0.02302265167236328s\n",
      "288/589, train_loss: 195.7819, time taken: 0.022742033004760742s\n",
      "289/589, train_loss: 222.1623, time taken: 0.02283334732055664s\n",
      "290/589, train_loss: 249.7468, time taken: 0.022964954376220703s\n",
      "291/589, train_loss: 205.0006, time taken: 0.02298879623413086s\n",
      "292/589, train_loss: 225.0505, time taken: 0.02242302894592285s\n",
      "293/589, train_loss: 205.5329, time taken: 0.022900104522705078s\n",
      "294/589, train_loss: 236.5780, time taken: 0.022870779037475586s\n",
      "295/589, train_loss: 270.0480, time taken: 0.02258467674255371s\n",
      "296/589, train_loss: 322.5737, time taken: 0.02029561996459961s\n",
      "297/589, train_loss: 221.9831, time taken: 0.022798776626586914s\n",
      "298/589, train_loss: 214.6979, time taken: 0.022897720336914062s\n",
      "299/589, train_loss: 179.0016, time taken: 0.0229184627532959s\n",
      "300/589, train_loss: 216.4953, time taken: 0.022687911987304688s\n",
      "301/589, train_loss: 234.7195, time taken: 0.02260422706604004s\n",
      "302/589, train_loss: 279.2251, time taken: 0.02269268035888672s\n",
      "303/589, train_loss: 168.9442, time taken: 0.020311832427978516s\n",
      "304/589, train_loss: 218.8344, time taken: 0.020296335220336914s\n",
      "305/589, train_loss: 212.6516, time taken: 0.022611141204833984s\n",
      "306/589, train_loss: 236.6887, time taken: 0.02033400535583496s\n",
      "307/589, train_loss: 200.5827, time taken: 0.020175457000732422s\n",
      "308/589, train_loss: 236.3711, time taken: 0.022693634033203125s\n",
      "309/589, train_loss: 196.7678, time taken: 0.02271103858947754s\n",
      "310/589, train_loss: 208.3958, time taken: 0.022792339324951172s\n",
      "311/589, train_loss: 202.9375, time taken: 0.020567893981933594s\n",
      "312/589, train_loss: 239.0075, time taken: 0.02037835121154785s\n",
      "313/589, train_loss: 189.3347, time taken: 0.020728588104248047s\n",
      "314/589, train_loss: 257.0479, time taken: 0.020473718643188477s\n",
      "315/589, train_loss: 259.8383, time taken: 0.02215290069580078s\n",
      "316/589, train_loss: 234.6990, time taken: 0.020699262619018555s\n",
      "317/589, train_loss: 194.6291, time taken: 0.02027750015258789s\n",
      "318/589, train_loss: 255.2467, time taken: 0.02284073829650879s\n",
      "319/589, train_loss: 270.0147, time taken: 0.0222318172454834s\n",
      "320/589, train_loss: 240.3703, time taken: 0.022376537322998047s\n",
      "321/589, train_loss: 243.7603, time taken: 0.023081541061401367s\n",
      "322/589, train_loss: 195.4940, time taken: 0.02015233039855957s\n",
      "323/589, train_loss: 312.1696, time taken: 0.02007770538330078s\n",
      "324/589, train_loss: 261.4108, time taken: 0.020421743392944336s\n",
      "325/589, train_loss: 216.0964, time taken: 0.021512746810913086s\n",
      "326/589, train_loss: 256.5396, time taken: 0.02041482925415039s\n",
      "327/589, train_loss: 251.0260, time taken: 0.022007226943969727s\n",
      "328/589, train_loss: 192.5193, time taken: 0.021725177764892578s\n",
      "329/589, train_loss: 194.0044, time taken: 0.02277827262878418s\n",
      "330/589, train_loss: 191.2581, time taken: 0.022942304611206055s\n",
      "331/589, train_loss: 236.9042, time taken: 0.022449493408203125s\n",
      "332/589, train_loss: 156.3876, time taken: 0.022829532623291016s\n",
      "333/589, train_loss: 246.7915, time taken: 0.022932052612304688s\n",
      "334/589, train_loss: 186.7387, time taken: 0.02048468589782715s\n",
      "335/589, train_loss: 240.2561, time taken: 0.02290940284729004s\n",
      "336/589, train_loss: 200.9496, time taken: 0.023103713989257812s\n",
      "337/589, train_loss: 246.7094, time taken: 0.02317047119140625s\n",
      "338/589, train_loss: 223.2108, time taken: 0.020411252975463867s\n",
      "339/589, train_loss: 233.5670, time taken: 0.02126312255859375s\n",
      "340/589, train_loss: 305.4573, time taken: 0.02042388916015625s\n",
      "341/589, train_loss: 231.5354, time taken: 0.022997379302978516s\n",
      "342/589, train_loss: 186.2535, time taken: 0.023054122924804688s\n",
      "343/589, train_loss: 271.7393, time taken: 0.022867441177368164s\n",
      "344/589, train_loss: 217.2122, time taken: 0.02256035804748535s\n",
      "345/589, train_loss: 204.9798, time taken: 0.022539377212524414s\n",
      "346/589, train_loss: 206.0737, time taken: 0.02060985565185547s\n",
      "347/589, train_loss: 202.7866, time taken: 0.020483016967773438s\n",
      "348/589, train_loss: 228.2097, time taken: 0.02031874656677246s\n",
      "349/589, train_loss: 241.0098, time taken: 0.02032303810119629s\n",
      "350/589, train_loss: 205.2240, time taken: 0.022742509841918945s\n",
      "351/589, train_loss: 205.5392, time taken: 0.022060394287109375s\n",
      "352/589, train_loss: 197.7187, time taken: 0.020244836807250977s\n",
      "353/589, train_loss: 203.9831, time taken: 0.020097017288208008s\n",
      "354/589, train_loss: 224.9795, time taken: 0.022703886032104492s\n",
      "355/589, train_loss: 221.2076, time taken: 0.022486448287963867s\n",
      "356/589, train_loss: 277.1755, time taken: 0.02292180061340332s\n",
      "357/589, train_loss: 244.8596, time taken: 0.02301812171936035s\n",
      "358/589, train_loss: 217.6381, time taken: 0.02278280258178711s\n",
      "359/589, train_loss: 260.4234, time taken: 0.022960424423217773s\n",
      "360/589, train_loss: 217.8815, time taken: 0.02170109748840332s\n",
      "361/589, train_loss: 199.5172, time taken: 0.02066969871520996s\n",
      "362/589, train_loss: 184.4430, time taken: 0.020243167877197266s\n",
      "363/589, train_loss: 272.2699, time taken: 0.022792816162109375s\n",
      "364/589, train_loss: 249.4080, time taken: 0.022713184356689453s\n",
      "365/589, train_loss: 263.7410, time taken: 0.022687911987304688s\n",
      "366/589, train_loss: 290.1409, time taken: 0.02274942398071289s\n",
      "367/589, train_loss: 213.5619, time taken: 0.020354032516479492s\n",
      "368/589, train_loss: 188.2860, time taken: 0.020040273666381836s\n",
      "369/589, train_loss: 152.9823, time taken: 0.020199298858642578s\n",
      "370/589, train_loss: 207.9910, time taken: 0.02054905891418457s\n",
      "371/589, train_loss: 198.2811, time taken: 0.02369213104248047s\n",
      "372/589, train_loss: 206.2228, time taken: 0.022794246673583984s\n",
      "373/589, train_loss: 206.2698, time taken: 0.020668745040893555s\n",
      "374/589, train_loss: 280.9176, time taken: 0.020613431930541992s\n",
      "375/589, train_loss: 222.6040, time taken: 0.020554542541503906s\n",
      "376/589, train_loss: 278.1666, time taken: 0.022676706314086914s\n",
      "377/589, train_loss: 232.2474, time taken: 0.022607088088989258s\n",
      "378/589, train_loss: 205.7524, time taken: 0.022663593292236328s\n",
      "379/589, train_loss: 291.9116, time taken: 0.02274346351623535s\n",
      "380/589, train_loss: 220.8729, time taken: 0.020311832427978516s\n",
      "381/589, train_loss: 195.5606, time taken: 0.020112276077270508s\n",
      "382/589, train_loss: 174.7558, time taken: 0.02025747299194336s\n",
      "383/589, train_loss: 227.3232, time taken: 0.02238774299621582s\n",
      "384/589, train_loss: 209.0776, time taken: 0.022728681564331055s\n",
      "385/589, train_loss: 235.5562, time taken: 0.02262139320373535s\n",
      "386/589, train_loss: 169.5973, time taken: 0.022621631622314453s\n",
      "387/589, train_loss: 277.5182, time taken: 0.020253896713256836s\n",
      "388/589, train_loss: 200.9716, time taken: 0.02006244659423828s\n",
      "389/589, train_loss: 231.4936, time taken: 0.020183563232421875s\n",
      "390/589, train_loss: 221.0219, time taken: 0.02033710479736328s\n",
      "391/589, train_loss: 239.6556, time taken: 0.022751808166503906s\n",
      "392/589, train_loss: 176.4648, time taken: 0.02223968505859375s\n",
      "393/589, train_loss: 206.7809, time taken: 0.022995710372924805s\n",
      "394/589, train_loss: 248.3123, time taken: 0.022609949111938477s\n",
      "395/589, train_loss: 225.9410, time taken: 0.02153778076171875s\n",
      "396/589, train_loss: 196.5237, time taken: 0.02057814598083496s\n",
      "397/589, train_loss: 209.2417, time taken: 0.020123004913330078s\n",
      "398/589, train_loss: 275.2201, time taken: 0.02021026611328125s\n",
      "399/589, train_loss: 241.8301, time taken: 0.02020263671875s\n",
      "400/589, train_loss: 222.5801, time taken: 0.02028799057006836s\n",
      "401/589, train_loss: 246.4038, time taken: 0.020061254501342773s\n",
      "402/589, train_loss: 275.1055, time taken: 0.0202486515045166s\n",
      "403/589, train_loss: 290.0701, time taken: 0.020123720169067383s\n",
      "404/589, train_loss: 226.5836, time taken: 0.020577669143676758s\n",
      "405/589, train_loss: 267.7134, time taken: 0.022665023803710938s\n",
      "406/589, train_loss: 200.5230, time taken: 0.022946596145629883s\n",
      "407/589, train_loss: 255.1292, time taken: 0.022850990295410156s\n",
      "408/589, train_loss: 261.8282, time taken: 0.02256011962890625s\n",
      "409/589, train_loss: 236.2897, time taken: 0.022732257843017578s\n",
      "410/589, train_loss: 228.0549, time taken: 0.022637128829956055s\n",
      "411/589, train_loss: 236.6978, time taken: 0.022850513458251953s\n",
      "412/589, train_loss: 196.5323, time taken: 0.022751331329345703s\n",
      "413/589, train_loss: 197.1127, time taken: 0.022724628448486328s\n",
      "414/589, train_loss: 275.6421, time taken: 0.022536516189575195s\n",
      "415/589, train_loss: 280.7955, time taken: 0.022678613662719727s\n",
      "416/589, train_loss: 247.1099, time taken: 0.020416736602783203s\n",
      "417/589, train_loss: 209.4001, time taken: 0.020201921463012695s\n",
      "418/589, train_loss: 236.7927, time taken: 0.020399093627929688s\n",
      "419/589, train_loss: 301.0023, time taken: 0.02311086654663086s\n",
      "420/589, train_loss: 323.5134, time taken: 0.020198345184326172s\n",
      "421/589, train_loss: 213.4930, time taken: 0.02295970916748047s\n",
      "422/589, train_loss: 198.9413, time taken: 0.02270340919494629s\n",
      "423/589, train_loss: 225.5080, time taken: 0.022704124450683594s\n",
      "424/589, train_loss: 225.2128, time taken: 0.022921323776245117s\n",
      "425/589, train_loss: 228.7755, time taken: 0.02276611328125s\n",
      "426/589, train_loss: 264.2330, time taken: 0.020636796951293945s\n",
      "427/589, train_loss: 263.8480, time taken: 0.02289581298828125s\n",
      "428/589, train_loss: 278.1931, time taken: 0.022498369216918945s\n",
      "429/589, train_loss: 231.9400, time taken: 0.02297520637512207s\n",
      "430/589, train_loss: 270.9784, time taken: 0.022734642028808594s\n",
      "431/589, train_loss: 173.8338, time taken: 0.02287769317626953s\n",
      "432/589, train_loss: 250.9227, time taken: 0.02276444435119629s\n",
      "433/589, train_loss: 211.2475, time taken: 0.0202023983001709s\n",
      "434/589, train_loss: 169.3350, time taken: 0.020171165466308594s\n",
      "435/589, train_loss: 200.4669, time taken: 0.022388696670532227s\n",
      "436/589, train_loss: 211.1644, time taken: 0.020490407943725586s\n",
      "437/589, train_loss: 179.6336, time taken: 0.022613048553466797s\n",
      "438/589, train_loss: 201.2160, time taken: 0.020377159118652344s\n",
      "439/589, train_loss: 213.5308, time taken: 0.020299196243286133s\n",
      "440/589, train_loss: 261.4343, time taken: 0.022858619689941406s\n",
      "441/589, train_loss: 246.9041, time taken: 0.02261972427368164s\n",
      "442/589, train_loss: 236.6910, time taken: 0.02302861213684082s\n",
      "443/589, train_loss: 271.3129, time taken: 0.022539138793945312s\n",
      "444/589, train_loss: 166.9445, time taken: 0.021862506866455078s\n",
      "445/589, train_loss: 277.1069, time taken: 0.022450923919677734s\n",
      "446/589, train_loss: 196.0841, time taken: 0.022718191146850586s\n",
      "447/589, train_loss: 265.8736, time taken: 0.022377490997314453s\n",
      "448/589, train_loss: 241.1857, time taken: 0.021645784378051758s\n",
      "449/589, train_loss: 238.5012, time taken: 0.022912025451660156s\n",
      "450/589, train_loss: 221.1523, time taken: 0.02254343032836914s\n",
      "451/589, train_loss: 199.1219, time taken: 0.022715091705322266s\n",
      "452/589, train_loss: 175.8295, time taken: 0.02262282371520996s\n",
      "453/589, train_loss: 257.5307, time taken: 0.022556066513061523s\n",
      "454/589, train_loss: 186.3409, time taken: 0.022647380828857422s\n",
      "455/589, train_loss: 265.7571, time taken: 0.022478103637695312s\n",
      "456/589, train_loss: 245.5092, time taken: 0.02316427230834961s\n",
      "457/589, train_loss: 249.4501, time taken: 0.022652626037597656s\n",
      "458/589, train_loss: 216.7360, time taken: 0.020301342010498047s\n",
      "459/589, train_loss: 233.2589, time taken: 0.020728588104248047s\n",
      "460/589, train_loss: 224.7323, time taken: 0.020122766494750977s\n",
      "461/589, train_loss: 280.1987, time taken: 0.020428180694580078s\n",
      "462/589, train_loss: 188.2209, time taken: 0.0230865478515625s\n",
      "463/589, train_loss: 243.4209, time taken: 0.020389318466186523s\n",
      "464/589, train_loss: 171.1996, time taken: 0.020129680633544922s\n",
      "465/589, train_loss: 285.2850, time taken: 0.020566225051879883s\n",
      "466/589, train_loss: 175.2442, time taken: 0.022856712341308594s\n",
      "467/589, train_loss: 237.1311, time taken: 0.02056097984313965s\n",
      "468/589, train_loss: 252.8644, time taken: 0.020223140716552734s\n",
      "469/589, train_loss: 227.7072, time taken: 0.020188093185424805s\n",
      "470/589, train_loss: 184.0265, time taken: 0.022861003875732422s\n",
      "471/589, train_loss: 226.6705, time taken: 0.023096799850463867s\n",
      "472/589, train_loss: 252.0079, time taken: 0.022918701171875s\n",
      "473/589, train_loss: 233.9144, time taken: 0.02026534080505371s\n",
      "474/589, train_loss: 201.6265, time taken: 0.02048349380493164s\n",
      "475/589, train_loss: 228.8718, time taken: 0.020569562911987305s\n",
      "476/589, train_loss: 326.8075, time taken: 0.022675275802612305s\n",
      "477/589, train_loss: 267.4227, time taken: 0.022717714309692383s\n",
      "478/589, train_loss: 207.1906, time taken: 0.02264118194580078s\n",
      "479/589, train_loss: 254.8935, time taken: 0.022971153259277344s\n",
      "480/589, train_loss: 197.1178, time taken: 0.022740602493286133s\n",
      "481/589, train_loss: 187.5489, time taken: 0.022884130477905273s\n",
      "482/589, train_loss: 251.5237, time taken: 0.02300238609313965s\n",
      "483/589, train_loss: 226.8451, time taken: 0.022725820541381836s\n",
      "484/589, train_loss: 234.1001, time taken: 0.02291250228881836s\n",
      "485/589, train_loss: 186.9098, time taken: 0.02280735969543457s\n",
      "486/589, train_loss: 205.1860, time taken: 0.021929502487182617s\n",
      "487/589, train_loss: 187.3901, time taken: 0.022979736328125s\n",
      "488/589, train_loss: 213.7813, time taken: 0.022679805755615234s\n",
      "489/589, train_loss: 170.1078, time taken: 0.022772789001464844s\n",
      "490/589, train_loss: 236.1807, time taken: 0.022905349731445312s\n",
      "491/589, train_loss: 210.4149, time taken: 0.022997140884399414s\n",
      "492/589, train_loss: 235.4433, time taken: 0.022853374481201172s\n",
      "493/589, train_loss: 234.7241, time taken: 0.02269601821899414s\n",
      "494/589, train_loss: 253.3702, time taken: 0.022561073303222656s\n",
      "495/589, train_loss: 167.7800, time taken: 0.02027273178100586s\n",
      "496/589, train_loss: 226.2502, time taken: 0.020417451858520508s\n",
      "497/589, train_loss: 190.3333, time taken: 0.020370006561279297s\n",
      "498/589, train_loss: 248.4552, time taken: 0.020150184631347656s\n",
      "499/589, train_loss: 240.8361, time taken: 0.022891759872436523s\n",
      "500/589, train_loss: 193.3783, time taken: 0.022404193878173828s\n",
      "501/589, train_loss: 199.1964, time taken: 0.02326226234436035s\n",
      "502/589, train_loss: 241.7264, time taken: 0.02031731605529785s\n",
      "503/589, train_loss: 194.0262, time taken: 0.020205020904541016s\n",
      "504/589, train_loss: 267.3771, time taken: 0.0227205753326416s\n",
      "505/589, train_loss: 252.7921, time taken: 0.023120880126953125s\n",
      "506/589, train_loss: 218.9715, time taken: 0.022780418395996094s\n",
      "507/589, train_loss: 169.5082, time taken: 0.02028369903564453s\n",
      "508/589, train_loss: 237.4193, time taken: 0.02000880241394043s\n",
      "509/589, train_loss: 226.1805, time taken: 0.022938013076782227s\n",
      "510/589, train_loss: 229.9791, time taken: 0.022759675979614258s\n",
      "511/589, train_loss: 222.6268, time taken: 0.020596742630004883s\n",
      "512/589, train_loss: 181.0028, time taken: 0.022710084915161133s\n",
      "513/589, train_loss: 230.8900, time taken: 0.02286386489868164s\n",
      "514/589, train_loss: 159.9567, time taken: 0.022795438766479492s\n",
      "515/589, train_loss: 230.2800, time taken: 0.0202481746673584s\n",
      "516/589, train_loss: 233.1653, time taken: 0.020021915435791016s\n",
      "517/589, train_loss: 175.4876, time taken: 0.020364999771118164s\n",
      "518/589, train_loss: 210.3742, time taken: 0.020624399185180664s\n",
      "519/589, train_loss: 145.1443, time taken: 0.020203351974487305s\n",
      "520/589, train_loss: 192.4565, time taken: 0.02021932601928711s\n",
      "521/589, train_loss: 221.6861, time taken: 0.020667314529418945s\n",
      "522/589, train_loss: 226.6058, time taken: 0.022114038467407227s\n",
      "523/589, train_loss: 211.5009, time taken: 0.020773649215698242s\n",
      "524/589, train_loss: 201.4416, time taken: 0.023018836975097656s\n",
      "525/589, train_loss: 204.0267, time taken: 0.0203092098236084s\n",
      "526/589, train_loss: 240.6637, time taken: 0.02222609519958496s\n",
      "527/589, train_loss: 305.8118, time taken: 0.020131826400756836s\n",
      "528/589, train_loss: 201.5344, time taken: 0.02106761932373047s\n",
      "529/589, train_loss: 222.2170, time taken: 0.020537137985229492s\n",
      "530/589, train_loss: 201.5693, time taken: 0.02265000343322754s\n",
      "531/589, train_loss: 226.6249, time taken: 0.02296614646911621s\n",
      "532/589, train_loss: 218.6889, time taken: 0.02015972137451172s\n",
      "533/589, train_loss: 271.6693, time taken: 0.022674083709716797s\n",
      "534/589, train_loss: 262.7794, time taken: 0.021155118942260742s\n",
      "535/589, train_loss: 203.2877, time taken: 0.020061016082763672s\n",
      "536/589, train_loss: 208.9062, time taken: 0.022977590560913086s\n",
      "537/589, train_loss: 223.5786, time taken: 0.020787715911865234s\n",
      "538/589, train_loss: 248.7444, time taken: 0.021979808807373047s\n",
      "539/589, train_loss: 206.8936, time taken: 0.020305156707763672s\n",
      "540/589, train_loss: 157.7757, time taken: 0.020278453826904297s\n",
      "541/589, train_loss: 188.6588, time taken: 0.020354509353637695s\n",
      "542/589, train_loss: 274.7831, time taken: 0.02293252944946289s\n",
      "543/589, train_loss: 190.6768, time taken: 0.022424936294555664s\n",
      "544/589, train_loss: 147.1890, time taken: 0.02292943000793457s\n",
      "545/589, train_loss: 210.4881, time taken: 0.02036118507385254s\n",
      "546/589, train_loss: 168.9138, time taken: 0.02061939239501953s\n",
      "547/589, train_loss: 216.2099, time taken: 0.02024221420288086s\n",
      "548/589, train_loss: 202.6017, time taken: 0.02008533477783203s\n",
      "549/589, train_loss: 262.1019, time taken: 0.023087501525878906s\n",
      "550/589, train_loss: 322.5785, time taken: 0.022556781768798828s\n",
      "551/589, train_loss: 212.1969, time taken: 0.020811080932617188s\n",
      "552/589, train_loss: 322.3467, time taken: 0.02036142349243164s\n",
      "553/589, train_loss: 203.5640, time taken: 0.022986650466918945s\n",
      "554/589, train_loss: 174.6263, time taken: 0.022336721420288086s\n",
      "555/589, train_loss: 199.7297, time taken: 0.02008223533630371s\n",
      "556/589, train_loss: 179.1620, time taken: 0.020229816436767578s\n",
      "557/589, train_loss: 241.7362, time taken: 0.020543575286865234s\n",
      "558/589, train_loss: 241.5941, time taken: 0.020267724990844727s\n",
      "559/589, train_loss: 202.2181, time taken: 0.02287912368774414s\n",
      "560/589, train_loss: 251.9033, time taken: 0.023024797439575195s\n",
      "561/589, train_loss: 240.8916, time taken: 0.02270221710205078s\n",
      "562/589, train_loss: 266.0733, time taken: 0.022806167602539062s\n",
      "563/589, train_loss: 167.3396, time taken: 0.022838354110717773s\n",
      "564/589, train_loss: 250.9203, time taken: 0.02294158935546875s\n",
      "565/589, train_loss: 177.8244, time taken: 0.022952795028686523s\n",
      "566/589, train_loss: 191.5392, time taken: 0.02015066146850586s\n",
      "567/589, train_loss: 180.5768, time taken: 0.020806312561035156s\n",
      "568/589, train_loss: 179.0819, time taken: 0.020582914352416992s\n",
      "569/589, train_loss: 199.8712, time taken: 0.020370006561279297s\n",
      "570/589, train_loss: 258.7140, time taken: 0.019993066787719727s\n",
      "571/589, train_loss: 189.9155, time taken: 0.020364999771118164s\n",
      "572/589, train_loss: 191.2728, time taken: 0.021293163299560547s\n",
      "573/589, train_loss: 185.9241, time taken: 0.020711898803710938s\n",
      "574/589, train_loss: 276.4553, time taken: 0.02274942398071289s\n",
      "575/589, train_loss: 199.0304, time taken: 0.02285599708557129s\n",
      "576/589, train_loss: 201.7347, time taken: 0.02172064781188965s\n",
      "577/589, train_loss: 196.2367, time taken: 0.020482540130615234s\n",
      "578/589, train_loss: 212.5748, time taken: 0.0228273868560791s\n",
      "579/589, train_loss: 206.3724, time taken: 0.02057361602783203s\n",
      "580/589, train_loss: 291.3659, time taken: 0.020244598388671875s\n",
      "581/589, train_loss: 216.4600, time taken: 0.02278733253479004s\n",
      "582/589, train_loss: 203.7069, time taken: 0.02276158332824707s\n",
      "583/589, train_loss: 195.3524, time taken: 0.02298903465270996s\n",
      "584/589, train_loss: 205.2264, time taken: 0.022507190704345703s\n",
      "585/589, train_loss: 216.1960, time taken: 0.021617650985717773s\n",
      "586/589, train_loss: 192.0842, time taken: 0.020116567611694336s\n",
      "587/589, train_loss: 209.7766, time taken: 0.020363807678222656s\n",
      "588/589, train_loss: 202.4740, time taken: 0.01998424530029297s\n",
      "589/589, train_loss: 226.5199, time taken: 0.020178556442260742s\n",
      "590/589, train_loss: 141.4114, time taken: 0.009094953536987305s\n",
      "epoch 3 average loss: 228.2795\n",
      "Entering Validation for epoch: 3\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([32, 1, 64, 64])\n",
      "Input shape: torch.Size([13, 1, 64, 64])\n",
      "epoch 3 Validation avg loss: 86.4952, time taken: 0.002711057662963867s\n",
      "----------\n",
      "epoch 4/4\n",
      "1/589, train_loss: 281.6213, time taken: 0.02680230140686035s\n",
      "2/589, train_loss: 210.3746, time taken: 0.022463560104370117s\n",
      "3/589, train_loss: 209.9586, time taken: 0.0202023983001709s\n",
      "4/589, train_loss: 208.2177, time taken: 0.02268838882446289s\n",
      "5/589, train_loss: 209.4826, time taken: 0.022855281829833984s\n",
      "6/589, train_loss: 188.2664, time taken: 0.022804737091064453s\n",
      "7/589, train_loss: 218.3956, time taken: 0.02028512954711914s\n",
      "8/589, train_loss: 271.2016, time taken: 0.022342681884765625s\n",
      "9/589, train_loss: 160.5836, time taken: 0.021259069442749023s\n",
      "10/589, train_loss: 189.4433, time taken: 0.020476579666137695s\n",
      "11/589, train_loss: 248.6881, time taken: 0.020135164260864258s\n",
      "12/589, train_loss: 269.9659, time taken: 0.02276921272277832s\n",
      "13/589, train_loss: 141.9117, time taken: 0.022870779037475586s\n",
      "14/589, train_loss: 226.8297, time taken: 0.02233290672302246s\n",
      "15/589, train_loss: 250.6884, time taken: 0.022470474243164062s\n",
      "16/589, train_loss: 306.6672, time taken: 0.023465633392333984s\n",
      "17/589, train_loss: 235.7719, time taken: 0.022014856338500977s\n",
      "18/589, train_loss: 202.6370, time taken: 0.021880149841308594s\n",
      "19/589, train_loss: 218.2833, time taken: 0.02077007293701172s\n",
      "20/589, train_loss: 189.1700, time taken: 0.022988080978393555s\n",
      "21/589, train_loss: 273.3483, time taken: 0.021459102630615234s\n",
      "22/589, train_loss: 246.3232, time taken: 0.022727012634277344s\n",
      "23/589, train_loss: 253.6012, time taken: 0.021087646484375s\n",
      "24/589, train_loss: 212.2419, time taken: 0.021104097366333008s\n",
      "25/589, train_loss: 239.8125, time taken: 0.022889137268066406s\n",
      "26/589, train_loss: 218.8795, time taken: 0.022958755493164062s\n",
      "27/589, train_loss: 217.4028, time taken: 0.02268052101135254s\n",
      "28/589, train_loss: 183.6671, time taken: 0.0227658748626709s\n",
      "29/589, train_loss: 283.2943, time taken: 0.021352767944335938s\n",
      "30/589, train_loss: 253.8867, time taken: 0.02036261558532715s\n",
      "31/589, train_loss: 269.7382, time taken: 0.020281314849853516s\n",
      "32/589, train_loss: 194.2188, time taken: 0.02077770233154297s\n",
      "33/589, train_loss: 221.5047, time taken: 0.02033233642578125s\n",
      "34/589, train_loss: 328.4014, time taken: 0.020292997360229492s\n",
      "35/589, train_loss: 163.5065, time taken: 0.0229949951171875s\n",
      "36/589, train_loss: 214.6322, time taken: 0.02322244644165039s\n",
      "37/589, train_loss: 221.4529, time taken: 0.022882699966430664s\n",
      "38/589, train_loss: 252.7021, time taken: 0.02103734016418457s\n",
      "39/589, train_loss: 185.1114, time taken: 0.021651268005371094s\n",
      "40/589, train_loss: 216.2109, time taken: 0.021645545959472656s\n",
      "41/589, train_loss: 211.4101, time taken: 0.02260899543762207s\n",
      "42/589, train_loss: 360.9204, time taken: 0.02124953269958496s\n",
      "43/589, train_loss: 295.0951, time taken: 0.022357463836669922s\n",
      "44/589, train_loss: 253.8929, time taken: 0.023292064666748047s\n",
      "45/589, train_loss: 248.2866, time taken: 0.020227909088134766s\n",
      "46/589, train_loss: 231.5124, time taken: 0.020251035690307617s\n",
      "47/589, train_loss: 215.3964, time taken: 0.0233004093170166s\n",
      "48/589, train_loss: 242.7331, time taken: 0.020129919052124023s\n",
      "49/589, train_loss: 207.2614, time taken: 0.021138906478881836s\n",
      "50/589, train_loss: 191.6798, time taken: 0.022943496704101562s\n",
      "51/589, train_loss: 251.2843, time taken: 0.023833751678466797s\n",
      "52/589, train_loss: 232.5266, time taken: 0.022274255752563477s\n",
      "53/589, train_loss: 172.6276, time taken: 0.022941112518310547s\n",
      "54/589, train_loss: 236.9512, time taken: 0.020394325256347656s\n",
      "55/589, train_loss: 221.4370, time taken: 0.022963762283325195s\n",
      "56/589, train_loss: 185.0607, time taken: 0.021806716918945312s\n",
      "57/589, train_loss: 239.5089, time taken: 0.020348787307739258s\n",
      "58/589, train_loss: 197.7646, time taken: 0.020666837692260742s\n",
      "59/589, train_loss: 295.2763, time taken: 0.021755456924438477s\n",
      "60/589, train_loss: 214.4622, time taken: 0.020383596420288086s\n",
      "61/589, train_loss: 199.6073, time taken: 0.02028512954711914s\n",
      "62/589, train_loss: 261.3007, time taken: 0.02263498306274414s\n",
      "63/589, train_loss: 241.7522, time taken: 0.023020029067993164s\n",
      "64/589, train_loss: 176.7718, time taken: 0.022214651107788086s\n",
      "65/589, train_loss: 161.8233, time taken: 0.022881746292114258s\n",
      "66/589, train_loss: 191.6181, time taken: 0.02300572395324707s\n",
      "67/589, train_loss: 294.2741, time taken: 0.02299332618713379s\n",
      "68/589, train_loss: 229.6519, time taken: 0.0203092098236084s\n",
      "69/589, train_loss: 321.3034, time taken: 0.022830963134765625s\n",
      "70/589, train_loss: 210.3032, time taken: 0.02251911163330078s\n",
      "71/589, train_loss: 239.4763, time taken: 0.026603221893310547s\n",
      "72/589, train_loss: 215.5782, time taken: 0.020191431045532227s\n",
      "73/589, train_loss: 254.0792, time taken: 0.022541284561157227s\n",
      "74/589, train_loss: 258.6472, time taken: 0.022655963897705078s\n",
      "75/589, train_loss: 215.1023, time taken: 0.02299022674560547s\n",
      "76/589, train_loss: 287.3936, time taken: 0.020438194274902344s\n",
      "77/589, train_loss: 222.8918, time taken: 0.022675275802612305s\n",
      "78/589, train_loss: 227.0217, time taken: 0.02263331413269043s\n",
      "79/589, train_loss: 257.2466, time taken: 0.020590543746948242s\n",
      "80/589, train_loss: 298.1245, time taken: 0.021523237228393555s\n",
      "81/589, train_loss: 207.5931, time taken: 0.02033829689025879s\n",
      "82/589, train_loss: 169.4468, time taken: 0.023343324661254883s\n",
      "83/589, train_loss: 260.1016, time taken: 0.02140951156616211s\n",
      "84/589, train_loss: 272.7664, time taken: 0.02263617515563965s\n",
      "85/589, train_loss: 220.8918, time taken: 0.02028036117553711s\n",
      "86/589, train_loss: 256.5502, time taken: 0.022174596786499023s\n",
      "87/589, train_loss: 178.5426, time taken: 0.02475762367248535s\n",
      "88/589, train_loss: 258.4263, time taken: 0.021821022033691406s\n",
      "89/589, train_loss: 224.7781, time taken: 0.022858381271362305s\n",
      "90/589, train_loss: 287.6050, time taken: 0.022984027862548828s\n",
      "91/589, train_loss: 260.3683, time taken: 0.023514747619628906s\n",
      "92/589, train_loss: 236.4809, time taken: 0.021958112716674805s\n",
      "93/589, train_loss: 234.4719, time taken: 0.021867036819458008s\n",
      "94/589, train_loss: 212.9955, time taken: 0.026103734970092773s\n",
      "95/589, train_loss: 190.4517, time taken: 0.020397663116455078s\n",
      "96/589, train_loss: 297.2094, time taken: 0.020140409469604492s\n",
      "97/589, train_loss: 227.9202, time taken: 0.022873401641845703s\n",
      "98/589, train_loss: 186.6310, time taken: 0.020394325256347656s\n",
      "99/589, train_loss: 231.7688, time taken: 0.020419836044311523s\n",
      "100/589, train_loss: 289.9996, time taken: 0.02249884605407715s\n",
      "101/589, train_loss: 271.1338, time taken: 0.020426034927368164s\n",
      "102/589, train_loss: 154.5197, time taken: 0.02018427848815918s\n",
      "103/589, train_loss: 292.6629, time taken: 0.0233156681060791s\n",
      "104/589, train_loss: 228.4205, time taken: 0.020717620849609375s\n",
      "105/589, train_loss: 200.5612, time taken: 0.022606849670410156s\n",
      "106/589, train_loss: 180.8854, time taken: 0.022754430770874023s\n",
      "107/589, train_loss: 228.3812, time taken: 0.020348548889160156s\n",
      "108/589, train_loss: 250.8340, time taken: 0.02107071876525879s\n",
      "109/589, train_loss: 231.9595, time taken: 0.02269601821899414s\n",
      "110/589, train_loss: 200.8718, time taken: 0.020296335220336914s\n",
      "111/589, train_loss: 217.2893, time taken: 0.022729158401489258s\n",
      "112/589, train_loss: 238.0433, time taken: 0.0235598087310791s\n",
      "113/589, train_loss: 214.5908, time taken: 0.02304387092590332s\n",
      "114/589, train_loss: 236.1041, time taken: 0.019930362701416016s\n",
      "115/589, train_loss: 313.7296, time taken: 0.023054122924804688s\n",
      "116/589, train_loss: 223.9400, time taken: 0.02088332176208496s\n",
      "117/589, train_loss: 208.5875, time taken: 0.020234346389770508s\n",
      "118/589, train_loss: 182.3091, time taken: 0.02058577537536621s\n",
      "119/589, train_loss: 214.2687, time taken: 0.023274660110473633s\n",
      "120/589, train_loss: 157.1219, time taken: 0.022176265716552734s\n",
      "121/589, train_loss: 175.5481, time taken: 0.020798444747924805s\n",
      "122/589, train_loss: 228.6834, time taken: 0.020079851150512695s\n",
      "123/589, train_loss: 167.6650, time taken: 0.020606040954589844s\n",
      "124/589, train_loss: 171.4634, time taken: 0.021689176559448242s\n",
      "125/589, train_loss: 226.4343, time taken: 0.023105144500732422s\n",
      "126/589, train_loss: 199.6887, time taken: 0.020754098892211914s\n",
      "127/589, train_loss: 253.9057, time taken: 0.02182459831237793s\n",
      "128/589, train_loss: 175.1130, time taken: 0.02293229103088379s\n",
      "129/589, train_loss: 224.4400, time taken: 0.02313709259033203s\n",
      "130/589, train_loss: 287.1395, time taken: 0.02279973030090332s\n",
      "131/589, train_loss: 217.8860, time taken: 0.021540164947509766s\n",
      "132/589, train_loss: 222.0388, time taken: 0.021611928939819336s\n",
      "133/589, train_loss: 226.9760, time taken: 0.023163795471191406s\n",
      "134/589, train_loss: 198.0530, time taken: 0.022979021072387695s\n",
      "135/589, train_loss: 202.4942, time taken: 0.022967100143432617s\n",
      "136/589, train_loss: 250.4214, time taken: 0.023229598999023438s\n",
      "137/589, train_loss: 258.1762, time taken: 0.02054119110107422s\n",
      "138/589, train_loss: 256.7957, time taken: 0.02337479591369629s\n",
      "139/589, train_loss: 180.4009, time taken: 0.020622968673706055s\n",
      "140/589, train_loss: 213.2433, time taken: 0.020482540130615234s\n",
      "141/589, train_loss: 205.0693, time taken: 0.02088642120361328s\n",
      "142/589, train_loss: 238.3960, time taken: 0.023146867752075195s\n",
      "143/589, train_loss: 234.1326, time taken: 0.02031230926513672s\n",
      "144/589, train_loss: 289.3635, time taken: 0.021506786346435547s\n",
      "145/589, train_loss: 246.0830, time taken: 0.02292466163635254s\n",
      "146/589, train_loss: 160.3311, time taken: 0.023069381713867188s\n",
      "147/589, train_loss: 155.7873, time taken: 0.021727800369262695s\n",
      "148/589, train_loss: 171.1243, time taken: 0.02289891242980957s\n",
      "149/589, train_loss: 323.4384, time taken: 0.021320343017578125s\n",
      "150/589, train_loss: 257.0161, time taken: 0.021530866622924805s\n",
      "151/589, train_loss: 216.6628, time taken: 0.02288079261779785s\n",
      "152/589, train_loss: 132.6755, time taken: 0.02212357521057129s\n",
      "153/589, train_loss: 154.5793, time taken: 0.020552873611450195s\n",
      "154/589, train_loss: 165.5619, time taken: 0.023019075393676758s\n",
      "155/589, train_loss: 290.9801, time taken: 0.021857023239135742s\n",
      "156/589, train_loss: 219.8796, time taken: 0.0211026668548584s\n",
      "157/589, train_loss: 234.2347, time taken: 0.020168066024780273s\n",
      "158/589, train_loss: 189.1534, time taken: 0.020302295684814453s\n",
      "159/589, train_loss: 221.8156, time taken: 0.02026677131652832s\n",
      "160/589, train_loss: 209.8418, time taken: 0.02200913429260254s\n",
      "161/589, train_loss: 269.5442, time taken: 0.020106792449951172s\n",
      "162/589, train_loss: 283.9550, time taken: 0.021353483200073242s\n",
      "163/589, train_loss: 289.2768, time taken: 0.021943330764770508s\n",
      "164/589, train_loss: 189.4137, time taken: 0.022388458251953125s\n",
      "165/589, train_loss: 225.2549, time taken: 0.021877765655517578s\n",
      "166/589, train_loss: 294.0974, time taken: 0.022441864013671875s\n",
      "167/589, train_loss: 309.9573, time taken: 0.023050785064697266s\n",
      "168/589, train_loss: 165.9621, time taken: 0.023236989974975586s\n",
      "169/589, train_loss: 226.9019, time taken: 0.021831750869750977s\n",
      "170/589, train_loss: 296.5239, time taken: 0.02268242835998535s\n",
      "171/589, train_loss: 212.7278, time taken: 0.022091388702392578s\n",
      "172/589, train_loss: 272.1315, time taken: 0.019998788833618164s\n",
      "173/589, train_loss: 233.7028, time taken: 0.02212810516357422s\n",
      "174/589, train_loss: 233.6959, time taken: 0.02221083641052246s\n",
      "175/589, train_loss: 208.1438, time taken: 0.02332472801208496s\n",
      "176/589, train_loss: 153.7611, time taken: 0.02333211898803711s\n",
      "177/589, train_loss: 175.6242, time taken: 0.023290157318115234s\n",
      "178/589, train_loss: 197.6982, time taken: 0.027179479598999023s\n",
      "179/589, train_loss: 182.3094, time taken: 0.021709680557250977s\n",
      "180/589, train_loss: 229.4874, time taken: 0.02199387550354004s\n",
      "181/589, train_loss: 264.6470, time taken: 0.021975040435791016s\n",
      "182/589, train_loss: 221.7752, time taken: 0.023259401321411133s\n",
      "183/589, train_loss: 252.2199, time taken: 0.02026963233947754s\n",
      "184/589, train_loss: 284.1662, time taken: 0.02286815643310547s\n",
      "185/589, train_loss: 259.7670, time taken: 0.02331066131591797s\n",
      "186/589, train_loss: 157.8980, time taken: 0.022418498992919922s\n",
      "187/589, train_loss: 269.8152, time taken: 0.02144932746887207s\n",
      "188/589, train_loss: 241.2976, time taken: 0.023175716400146484s\n",
      "189/589, train_loss: 266.7849, time taken: 0.02074265480041504s\n",
      "190/589, train_loss: 162.4540, time taken: 0.020610332489013672s\n",
      "191/589, train_loss: 201.1687, time taken: 0.022835254669189453s\n",
      "192/589, train_loss: 217.6248, time taken: 0.020127058029174805s\n",
      "193/589, train_loss: 223.6440, time taken: 0.02263188362121582s\n",
      "194/589, train_loss: 198.9099, time taken: 0.022928237915039062s\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(max_epochs):\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_cl_loss = 0\n",
    "    epoch_recon_loss = 0\n",
    "    step = 0\n",
    "\n",
    "    for batch_data in train_loader:\n",
    "        step += 1\n",
    "        start_time = time.time()\n",
    "\n",
    "        inputs, inputs_2, gt_input = (\n",
    "            batch_data[\"image\"].to(device),\n",
    "            batch_data[\"image_2\"].to(device),\n",
    "            batch_data[\"gt_image\"].to(device),\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        outputs_v1 = model(inputs)\n",
    "        outputs_v2 = model(inputs_2)\n",
    "\n",
    "        flat_out_v1 = outputs_v1.flatten(start_dim=1, end_dim=-1)\n",
    "        flat_out_v2 = outputs_v2.flatten(start_dim=1, end_dim=-1)\n",
    "\n",
    "        r_loss = recon_loss(outputs_v1, gt_input)\n",
    "        cl_loss = contrastive_loss(flat_out_v1, flat_out_v2)\n",
    "\n",
    "        # Adjust the CL loss by Recon Loss\n",
    "        total_loss = r_loss + cl_loss * r_loss\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += total_loss.item()\n",
    "        step_loss_values.append(total_loss.item())\n",
    "\n",
    "        # CL & Recon Loss Storage of Value\n",
    "        epoch_cl_loss += cl_loss.item()\n",
    "        epoch_recon_loss += r_loss.item()\n",
    "\n",
    "        end_time = time.time()\n",
    "        print(\n",
    "            f\"{step}/{len(train_ds) // train_loader.batch_size}, \"\n",
    "            f\"train_loss: {total_loss.item():.4f}, \"\n",
    "            f\"time taken: {end_time-start_time}s\"\n",
    "        )\n",
    "\n",
    "    epoch_loss /= step\n",
    "    epoch_cl_loss /= step\n",
    "    epoch_recon_loss /= step\n",
    "\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    epoch_cl_loss_values.append(epoch_cl_loss)\n",
    "    epoch_recon_loss_values.append(epoch_recon_loss)\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    if epoch % val_interval == 0:\n",
    "        print(\"Entering Validation for epoch: {}\".format(epoch + 1))\n",
    "        total_val_loss = 0\n",
    "        val_step = 0\n",
    "        model.eval()\n",
    "        for val_batch in val_loader:\n",
    "            val_step += 1\n",
    "            start_time = time.time()\n",
    "            inputs, gt_input = (\n",
    "                val_batch[\"image\"].to(device),\n",
    "                val_batch[\"gt_image\"].to(device),\n",
    "            )\n",
    "            # print(\"Input shape: {}\".format(inputs.shape))\n",
    "            outputs = model(inputs)\n",
    "            val_loss = recon_loss(outputs, gt_input)\n",
    "            total_val_loss += val_loss.item()\n",
    "            end_time = time.time()\n",
    "\n",
    "        total_val_loss /= val_step\n",
    "        val_loss_values.append(total_val_loss)\n",
    "        print(f\"epoch {epoch + 1} Validation avg loss: {total_val_loss:.4f}, \" f\"time taken: {end_time-start_time}s\")\n",
    "\n",
    "        if total_val_loss < best_val_loss:\n",
    "            print(f\"Saving new model based on validation loss {total_val_loss:.4f}\")\n",
    "            best_val_loss = total_val_loss\n",
    "            checkpoint = {\"epoch\": max_epochs, \"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n",
    "            torch.save(checkpoint, os.path.join(logdir_path, \"best_model.pt\"))\n",
    "\n",
    "        plt.figure(1, figsize=(8, 8))\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(epoch_loss_values)\n",
    "        plt.grid()\n",
    "        plt.title(\"Training Loss\")\n",
    "\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(val_loss_values)\n",
    "        plt.grid()\n",
    "        plt.title(\"Validation Loss\")\n",
    "\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.plot(epoch_cl_loss_values)\n",
    "        plt.grid()\n",
    "        plt.title(\"Training Contrastive Loss\")\n",
    "\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.plot(epoch_recon_loss_values)\n",
    "        plt.grid()\n",
    "        plt.title(\"Training Recon Loss\")\n",
    "\n",
    "        plt.savefig(os.path.join(logdir_path, \"loss_plots.png\"))\n",
    "        plt.close(1)\n",
    "\n",
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
